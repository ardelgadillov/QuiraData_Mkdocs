{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quira Data","text":"Data Analytics               Unlock valuable insights through data analytics, enabling you to make data-driven decisions                       Develop mathematical optimization models to solve complex, constrained problems,  allowing your business to make optimal decisions                       Operations Research               Custom Dashboards               Develop custom dashboards and reports, providing clear insights and uncover data trends","tags":["Data Science","Optimization","Electricity Markets","Machine Learning"]},{"location":"about/","title":"About","text":"<p>StatusCode        : 200 StatusDescription : OK Content           : # Et fuit</p> <pre><code>                ## Fuit umbra quondam exasperat agresti nitor\n\n                Lorem markdownum, in mare annos, salutat cutem occubuisse credi. De quo putant,\n                facibus mea Venus gravidis deformia repetam ad sunt, vix sensi...\n</code></pre> <p>RawContent        : HTTP/1.1 200 OK                     Transfer-Encoding: chunked                     Connection: keep-alive                     access-control-allow-origin: *                     Content-Type: text/plain; charset=utf-8                     Date: Wed, 30 Oct 2024 17:47:32 GMT                     Server: nginx/1.14.2... Forms             : {} Headers           : {[Transfer-Encoding, chunked], [Connection, keep-alive], [access-control-allow-origin, *], [Content-Type, text/plain; charset=utf-8]...} Images            : {} InputFields       : {} Links             : {} ParsedHtml        : mshtml.HTMLDocumentClass RawContentLength  : 2484</p>"},{"location":"blog/","title":"Quira Data Blog","text":"","tags":["Optimization","Data Science"]},{"location":"blog/2024/11/07/deploying-a-dash-application-on-render-a-step-by-step-guide/","title":"Deploying a Plotly Dash Webapp on Render: A Step-by-Step Guide","text":"<p>I like to create web applications in Python, especially using the Plotly Dash framework.  One of my favorite projects is a Sudoku Solver that\u2019s powered by Mixed-Integer Linear Programming  (check it out at sudoku.quiradata.com).  I understand the excitement of bringing a project to life on the web.  In this guide, I\u2019ll walk you through the process I used to deploy my Sudoku app on Render.</p>","tags":["Dash","Render","Webapp","Deployment"]},{"location":"blog/2024/11/07/deploying-a-dash-application-on-render-a-step-by-step-guide/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have the following:</p> <ul> <li>A Render account: Sign up at render.com </li> <li>A GitHub account and repository: This should contain your Python Dash application project</li> </ul>","tags":["Dash","Render","Webapp","Deployment"]},{"location":"blog/2024/11/07/deploying-a-dash-application-on-render-a-step-by-step-guide/#update-python-code-and-create-requirementstxt","title":"Update Python Code and Create requirements.txt","text":"","tags":["Dash","Render","Webapp","Deployment"]},{"location":"blog/2024/11/07/deploying-a-dash-application-on-render-a-step-by-step-guide/#setting-up-intro-point","title":"Setting Up intro point","text":"<ol> <li> <p>Locate the section of your python application file where the <code>app</code> variable is defined.  It should look something like this:</p> <pre><code>app = dash.Dash(__name__)\n</code></pre> </li> <li> <p>Add the following line immediately after the <code>app</code> definition:</p> <pre><code>server = app.server\n</code></pre> </li> </ol> <p>This exposes your Dash app\u2019s Flask server instance, which is necessary for deployment.</p>","tags":["Dash","Render","Webapp","Deployment"]},{"location":"blog/2024/11/07/deploying-a-dash-application-on-render-a-step-by-step-guide/#creatingupdating-requirementstxt","title":"Creating/Updating <code>requirements.txt</code>","text":"<ol> <li> <p>In the root directory of your repository, create a file named <code>requirements.txt</code>.  You can use pipreqs to generate the requirements file based on imports in project. For example:</p> requirements.txt<pre><code>numpy==1.26.4\npandas==2.2.1\ndash==2.17.1\nPyomo==6.6.0\nhighspy~=1.7.1.dev1\n</code></pre> </li> <li> <p>Include gunicorn at the end of the requirements.txt file.  Gunicorn is a Python HTTP server that efficiently handles web requests.</p> requirements.txt<pre><code>numpy==1.26.4\npandas==2.2.1\ndash==2.17.1\nPyomo==6.6.0\nhighspy~=1.7.1.dev1\ngunicorn\n</code></pre> </li> </ol>","tags":["Dash","Render","Webapp","Deployment"]},{"location":"blog/2024/11/07/deploying-a-dash-application-on-render-a-step-by-step-guide/#setting-up-a-new-web-service-on-render","title":"Setting Up a New Web Service on Render","text":"<ol> <li>Log in to Render and navigate to your dashboard.</li> <li>Click +New / Web Service.</li> <li>Provide the URL of your Public GitHub repository containing the Dash app.</li> <li>Configure the service by:</li> <li>Giving your service a unique name.</li> <li>Language: Python 3</li> <li>Branch: The Git branch to build and deploy. In my case master</li> <li>Build Command: Render runs this command to build your app before each deploy      <pre><code>$ pip install -r requirements.txt\n</code></pre></li> <li> <p>Start Command: Render runs this command to start your app with each deploy      <pre><code>$ gunicorn app:server\n</code></pre></p> </li> <li> <p>Hit Deploy Web Service to finalize the setup.</p> </li> </ol>","tags":["Dash","Render","Webapp","Deployment"]},{"location":"blog/2024/11/07/deploying-a-dash-application-on-render-a-step-by-step-guide/#youre-live","title":"You're Live!","text":"<ol> <li>Render will now build and deploy your application. This may take a few minutes.</li> <li>Once deployment is complete, you\u2019ll see a live link to your application.</li> </ol> <p>Your Dash app is now live on Render and accessible from anywhere. </p>","tags":["Dash","Render","Webapp","Deployment"]},{"location":"machine_learning/","title":"Machine Learning","text":"<ul> <li> HTML for content and structure</li> <li> JavaScript for interactivity</li> <li> CSS for text running out of boxes</li> <li> Internet Explorer ... huh?</li> </ul>"},{"location":"machine_learning/Supervised_Learning_Regression/","title":"Linear Regression","text":"In\u00a0[3]: Copied! <pre>import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# import pandas_profiling\nsns.set(color_codes=True)\n%matplotlib inline\n</pre> import warnings warnings.filterwarnings(\"ignore\")  import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # import pandas_profiling sns.set(color_codes=True) %matplotlib inline In\u00a0[5]: Copied! <pre># read data from csv file\ndata = pd.read_csv(r\"C:\\Users\\AndresDelgadillo\\Downloads\\used_cars_data.csv\")\n</pre> # read data from csv file data = pd.read_csv(r\"C:\\Users\\AndresDelgadillo\\Downloads\\used_cars_data.csv\") In\u00a0[6]: Copied! <pre># get columns\ndata.columns\n</pre> # get columns data.columns Out[6]: <pre>Index(['S.No.', 'Name', 'Location', 'Year', 'Kilometers_Driven', 'Fuel_Type',\n       'Transmission', 'Owner_Type', 'Mileage', 'Engine', 'Power', 'Seats',\n       'New_Price', 'Price'],\n      dtype='object')</pre> In\u00a0[7]: Copied! <pre># get size of dataset\ndata.shape\n</pre> # get size of dataset data.shape Out[7]: <pre>(7253, 14)</pre> In\u00a0[8]: Copied! <pre># check dataset information \ndata.info()\n</pre> # check dataset information  data.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7253 entries, 0 to 7252\nData columns (total 14 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   S.No.              7253 non-null   int64  \n 1   Name               7253 non-null   object \n 2   Location           7253 non-null   object \n 3   Year               7253 non-null   int64  \n 4   Kilometers_Driven  7253 non-null   int64  \n 5   Fuel_Type          7253 non-null   object \n 6   Transmission       7253 non-null   object \n 7   Owner_Type         7253 non-null   object \n 8   Mileage            7251 non-null   object \n 9   Engine             7207 non-null   object \n 10  Power              7207 non-null   object \n 11  Seats              7200 non-null   float64\n 12  New_Price          1006 non-null   object \n 13  Price              6019 non-null   float64\ndtypes: float64(2), int64(3), object(9)\nmemory usage: 793.4+ KB\n</pre> In\u00a0[9]: Copied! <pre># check dataset missing values\ntotal = data.isnull().sum().sort_values(ascending=False) # total number of null values\nprint(total)\n</pre> # check dataset missing values total = data.isnull().sum().sort_values(ascending=False) # total number of null values print(total) <pre>New_Price            6247\nPrice                1234\nSeats                  53\nEngine                 46\nPower                  46\nMileage                 2\nS.No.                   0\nName                    0\nLocation                0\nYear                    0\nKilometers_Driven       0\nFuel_Type               0\nTransmission            0\nOwner_Type              0\ndtype: int64\n</pre> <ul> <li>There are 7253 rows and 14 columns.</li> <li>'New_Price' and 'Price' columns have a big number of missing values, and that could affect the results of the analysis. <code>A more deep study is necessary to deal with all missing values</code></li> </ul> In\u00a0[10]: Copied! <pre># check first rows of data\ndata.head()\n</pre> # check first rows of data data.head() Out[10]: S.No. Name Location Year Kilometers_Driven Fuel_Type Transmission Owner_Type Mileage Engine Power Seats New_Price Price 0 0 Maruti Wagon R LXI CNG Mumbai 2010 72000 CNG Manual First 26.6 km/kg 998 CC 58.16 bhp 5.0 NaN 1.75 1 1 Hyundai Creta 1.6 CRDi SX Option Pune 2015 41000 Diesel Manual First 19.67 kmpl 1582 CC 126.2 bhp 5.0 NaN 12.50 2 2 Honda Jazz V Chennai 2011 46000 Petrol Manual First 18.2 kmpl 1199 CC 88.7 bhp 5.0 8.61 Lakh 4.50 3 3 Maruti Ertiga VDI Chennai 2012 87000 Diesel Manual First 20.77 kmpl 1248 CC 88.76 bhp 7.0 NaN 6.00 4 4 Audi A4 New 2.0 TDI Multitronic Coimbatore 2013 40670 Diesel Automatic Second 15.2 kmpl 1968 CC 140.8 bhp 5.0 NaN 17.74 In\u00a0[11]: Copied! <pre>data.tail()\n</pre> data.tail() Out[11]: S.No. Name Location Year Kilometers_Driven Fuel_Type Transmission Owner_Type Mileage Engine Power Seats New_Price Price 7248 7248 Volkswagen Vento Diesel Trendline Hyderabad 2011 89411 Diesel Manual First 20.54 kmpl 1598 CC 103.6 bhp 5.0 NaN NaN 7249 7249 Volkswagen Polo GT TSI Mumbai 2015 59000 Petrol Automatic First 17.21 kmpl 1197 CC 103.6 bhp 5.0 NaN NaN 7250 7250 Nissan Micra Diesel XV Kolkata 2012 28000 Diesel Manual First 23.08 kmpl 1461 CC 63.1 bhp 5.0 NaN NaN 7251 7251 Volkswagen Polo GT TSI Pune 2013 52262 Petrol Automatic Third 17.2 kmpl 1197 CC 103.6 bhp 5.0 NaN NaN 7252 7252 Mercedes-Benz E-Class 2009-2013 E 220 CDI Avan... Kochi 2014 72443 Diesel Automatic First 10.0 kmpl 2148 CC 170 bhp 5.0 NaN NaN In\u00a0[12]: Copied! <pre># get a random sample of data\nnp.random.seed(1) #setting the random seed via np.random.seed to get the same random results every time\ndata.sample(n=5)\n</pre> # get a random sample of data np.random.seed(1) #setting the random seed via np.random.seed to get the same random results every time data.sample(n=5) Out[12]: S.No. Name Location Year Kilometers_Driven Fuel_Type Transmission Owner_Type Mileage Engine Power Seats New_Price Price 2397 2397 Ford EcoSport 1.5 Petrol Trend Kolkata 2016 21460 Petrol Manual First 17.0 kmpl 1497 CC 121.36 bhp 5.0 9.47 Lakh 6.00 3777 3777 Maruti Wagon R VXI 1.2 Kochi 2015 49818 Petrol Manual First 21.5 kmpl 1197 CC 81.80 bhp 5.0 5.44 Lakh 4.11 4425 4425 Ford Endeavour 4x2 XLT Hyderabad 2007 130000 Diesel Manual First 13.1 kmpl 2499 CC 141 bhp 7.0 NaN 6.00 3661 3661 Mercedes-Benz E-Class E250 CDI Avantgrade Coimbatore 2016 39753 Diesel Automatic First 13.0 kmpl 2143 CC 201.1 bhp 5.0 NaN 35.28 4514 4514 Hyundai Xcent 1.2 Kappa AT SX Option Kochi 2016 45560 Petrol Automatic First 16.9 kmpl 1197 CC 82 bhp 5.0 NaN 6.34 <ul> <li>'Name' column could be split in 2 columns. The first column would be the brand and the second column the model of the car</li> <li>'Location', 'Fuel_Type', 'Transmission', and 'Owner_Type' columns could be transformed to 'category'</li> <li>'Mileage', 'Engine', 'Power', and 'New_Price' columns should be numerical values but they appear as 'object'. <code>Processing columns is necessary to convert them to numerical</code></li> <li>'S.No' is the same as the index of the dataset and we can drop the column</li> </ul> In\u00a0[14]: Copied! <pre># Split Mileage column to extract units\ndata[['Mileage','Unit']] = data['Mileage'].str.split(' ',n=2,expand=True)\n</pre> # Split Mileage column to extract units data[['Mileage','Unit']] = data['Mileage'].str.split(' ',n=2,expand=True) In\u00a0[15]: Copied! <pre># Get unique pairs of Fuel_Type and Unit\ndata.groupby(['Fuel_Type','Unit']).size()\n</pre> # Get unique pairs of Fuel_Type and Unit data.groupby(['Fuel_Type','Unit']).size() Out[15]: <pre>Fuel_Type  Unit \nCNG        km/kg      62\nDiesel     kmpl     3852\nLPG        km/kg      12\nPetrol     kmpl     3325\ndtype: int64</pre> <p>There is a clear relation between 'Fuel_Type' and 'Unit'.</p> <ul> <li>Mileage for CNG and LPG are in km/kg</li> <li>Mileage for Diesel and Petrol are in kmpl It is not necessary to convert the units because Fuel_Type column will help to identify this information.</li> </ul> <p>Now, we can convert Mileage to numeric and drop the Unit column</p> In\u00a0[16]: Copied! <pre># drop Name column\ndata.drop(['Unit'], axis=1, inplace=True)\n</pre> # drop Name column data.drop(['Unit'], axis=1, inplace=True) In\u00a0[17]: Copied! <pre># Convert Mileage to Number\ndata['Mileage']=data['Mileage'].astype('float64')\n</pre> # Convert Mileage to Number data['Mileage']=data['Mileage'].astype('float64') In\u00a0[18]: Copied! <pre># check Mileage is number\ndata['Mileage'].head()\n</pre> # check Mileage is number data['Mileage'].head() Out[18]: <pre>0    26.60\n1    19.67\n2    18.20\n3    20.77\n4    15.20\nName: Mileage, dtype: float64</pre> In\u00a0[19]: Copied! <pre>def engine_to_num(engine):\n    \"\"\"This function takes in a string representing the engine and converts it to a number. \n    This function returns the same engine value if the input is already numeric.\"\"\"\n    if isinstance(engine, str):  # checks if engine is a string\n        engine_val = float(engine.replace('CC', '').strip())\n    else:  # this happens when the engine is already number or nan\n        engine_val = engine\n    # return engine as number\n    return engine_val\n\n# apply engine_to_num function to column 'Engine'\ndata['Engine'] = data['Engine'].apply(engine_to_num)\n</pre> def engine_to_num(engine):     \"\"\"This function takes in a string representing the engine and converts it to a number.      This function returns the same engine value if the input is already numeric.\"\"\"     if isinstance(engine, str):  # checks if engine is a string         engine_val = float(engine.replace('CC', '').strip())     else:  # this happens when the engine is already number or nan         engine_val = engine     # return engine as number     return engine_val  # apply engine_to_num function to column 'Engine' data['Engine'] = data['Engine'].apply(engine_to_num) In\u00a0[20]: Copied! <pre># check Engine is number\ndata['Engine'].head()\n</pre> # check Engine is number data['Engine'].head() Out[20]: <pre>0     998.0\n1    1582.0\n2    1199.0\n3    1248.0\n4    1968.0\nName: Engine, dtype: float64</pre> In\u00a0[21]: Copied! <pre>def power_to_num(power):\n    \"\"\"This function takes in a string representing the power and converts it to a number. \n    This function returns the same power value if the input is already numeric.\"\"\"\n    if isinstance(power, str):  # checks if power is a string\n        power_val = power.replace('bhp', '').strip()\n        if power_val != 'null': # check that there is a value \n            power_val = float(power_val)\n        else:\n            power_val = np.nan # returns nan\n    else:  # this happens when the power is already number or nan\n        power_val = power\n    # return power as number\n    return power_val\n\n# apply engine_to_num function to column 'Engine'\ndata['Power'] = data['Power'].apply(power_to_num)\n</pre> def power_to_num(power):     \"\"\"This function takes in a string representing the power and converts it to a number.      This function returns the same power value if the input is already numeric.\"\"\"     if isinstance(power, str):  # checks if power is a string         power_val = power.replace('bhp', '').strip()         if power_val != 'null': # check that there is a value              power_val = float(power_val)         else:             power_val = np.nan # returns nan     else:  # this happens when the power is already number or nan         power_val = power     # return power as number     return power_val  # apply engine_to_num function to column 'Engine' data['Power'] = data['Power'].apply(power_to_num) In\u00a0[22]: Copied! <pre># check Power is number\ndata['Power'].head()\n</pre> # check Power is number data['Power'].head() Out[22]: <pre>0     58.16\n1    126.20\n2     88.70\n3     88.76\n4    140.80\nName: Power, dtype: float64</pre> In\u00a0[23]: Copied! <pre>def price_to_num(price):\n    \"\"\"This function takes in a string representing the price and converts it to a number. \n    This function returns the same price value if the input is already numeric.\"\"\"\n    if isinstance(price, str):  # checks if price is a string\n        # handles Cr and Lakh units\n        if price.endswith('Lakh'):\n            multiplier = 1\n        elif price.endswith('Cr'):\n            multiplier = 100\n        price_val = float(price.replace('Lakh', '').replace('Cr', '').strip()) * multiplier\n    else:  # this happens when the price is already number or nan\n        price_val = price\n    # return price as number\n    return price_val\n\n# apply price_to_num function to column 'New_Price'\ndata['New_Price'] = data['New_Price'].apply(price_to_num)\n</pre> def price_to_num(price):     \"\"\"This function takes in a string representing the price and converts it to a number.      This function returns the same price value if the input is already numeric.\"\"\"     if isinstance(price, str):  # checks if price is a string         # handles Cr and Lakh units         if price.endswith('Lakh'):             multiplier = 1         elif price.endswith('Cr'):             multiplier = 100         price_val = float(price.replace('Lakh', '').replace('Cr', '').strip()) * multiplier     else:  # this happens when the price is already number or nan         price_val = price     # return price as number     return price_val  # apply price_to_num function to column 'New_Price' data['New_Price'] = data['New_Price'].apply(price_to_num) In\u00a0[24]: Copied! <pre># check Price is number\ndata['New_Price'].head()\n</pre> # check Price is number data['New_Price'].head() Out[24]: <pre>0     NaN\n1     NaN\n2    8.61\n3     NaN\n4     NaN\nName: New_Price, dtype: float64</pre> In\u00a0[26]: Copied! <pre>data[['Brand','Model','Specs']] = data['Name'].str.split(' ',n=2,expand=True)\n</pre> data[['Brand','Model','Specs']] = data['Name'].str.split(' ',n=2,expand=True) In\u00a0[27]: Copied! <pre>data[['Name','Brand','Model','Specs']].head()\n</pre> data[['Name','Brand','Model','Specs']].head() Out[27]: Name Brand Model Specs 0 Maruti Wagon R LXI CNG Maruti Wagon R LXI CNG 1 Hyundai Creta 1.6 CRDi SX Option Hyundai Creta 1.6 CRDi SX Option 2 Honda Jazz V Honda Jazz V 3 Maruti Ertiga VDI Maruti Ertiga VDI 4 Audi A4 New 2.0 TDI Multitronic Audi A4 New 2.0 TDI Multitronic <p>Now, we can drop 'Name' column and use 'Brand', 'Model' and 'Specs' columns</p> In\u00a0[28]: Copied! <pre># drop Name column\ndata.drop(['Name'], axis=1, inplace=True)\n</pre> # drop Name column data.drop(['Name'], axis=1, inplace=True) In\u00a0[29]: Copied! <pre>data['Brand']=data['Brand'].astype('category')\ndata['Model']=data['Model'].astype('category')\ndata['Specs']=data['Specs'].astype('category')\ndata['Location']=data['Location'].astype('category')\ndata['Fuel_Type']=data['Fuel_Type'].astype('category')\ndata['Transmission']=data['Transmission'].astype('category')\ndata['Owner_Type']=data['Owner_Type'].astype('category')\n</pre> data['Brand']=data['Brand'].astype('category') data['Model']=data['Model'].astype('category') data['Specs']=data['Specs'].astype('category') data['Location']=data['Location'].astype('category') data['Fuel_Type']=data['Fuel_Type'].astype('category') data['Transmission']=data['Transmission'].astype('category') data['Owner_Type']=data['Owner_Type'].astype('category') In\u00a0[30]: Copied! <pre>data.drop(['S.No.'], axis=1, inplace=True)\n</pre> data.drop(['S.No.'], axis=1, inplace=True) In\u00a0[31]: Copied! <pre># show all rows with duplicates \ndata[data.duplicated(keep=False)]\n</pre> # show all rows with duplicates  data[data.duplicated(keep=False)] Out[31]: Location Year Kilometers_Driven Fuel_Type Transmission Owner_Type Mileage Engine Power Seats New_Price Price Brand Model Specs 6498 Mumbai 2010 52000 Petrol Manual First 17.0 1497.0 118.0 5.0 NaN NaN Honda City 1.5 E MT 6582 Mumbai 2010 52000 Petrol Manual First 17.0 1497.0 118.0 5.0 NaN NaN Honda City 1.5 E MT In\u00a0[32]: Copied! <pre># drop duplicate rows\ndata.drop(data[data.duplicated()].index, axis=0, inplace=True)\n</pre> # drop duplicate rows data.drop(data[data.duplicated()].index, axis=0, inplace=True) In\u00a0[33]: Copied! <pre># Check there are no duplicates \ndata.duplicated().sum()\n</pre> # Check there are no duplicates  data.duplicated().sum() Out[33]: <pre>0</pre> In\u00a0[34]: Copied! <pre>data.info()\n</pre> data.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 7252 entries, 0 to 7252\nData columns (total 15 columns):\n #   Column             Non-Null Count  Dtype   \n---  ------             --------------  -----   \n 0   Location           7252 non-null   category\n 1   Year               7252 non-null   int64   \n 2   Kilometers_Driven  7252 non-null   int64   \n 3   Fuel_Type          7252 non-null   category\n 4   Transmission       7252 non-null   category\n 5   Owner_Type         7252 non-null   category\n 6   Mileage            7250 non-null   float64 \n 7   Engine             7206 non-null   float64 \n 8   Power              7077 non-null   float64 \n 9   Seats              7199 non-null   float64 \n 10  New_Price          1006 non-null   float64 \n 11  Price              6019 non-null   float64 \n 12  Brand              7252 non-null   category\n 13  Model              7252 non-null   category\n 14  Specs              7251 non-null   category\ndtypes: category(7), float64(6), int64(2)\nmemory usage: 665.0 KB\n</pre> In\u00a0[35]: Copied! <pre># check first rows of data\ndata.head()\n</pre> # check first rows of data data.head() Out[35]: Location Year Kilometers_Driven Fuel_Type Transmission Owner_Type Mileage Engine Power Seats New_Price Price Brand Model Specs 0 Mumbai 2010 72000 CNG Manual First 26.60 998.0 58.16 5.0 NaN 1.75 Maruti Wagon R LXI CNG 1 Pune 2015 41000 Diesel Manual First 19.67 1582.0 126.20 5.0 NaN 12.50 Hyundai Creta 1.6 CRDi SX Option 2 Chennai 2011 46000 Petrol Manual First 18.20 1199.0 88.70 5.0 8.61 4.50 Honda Jazz V 3 Chennai 2012 87000 Diesel Manual First 20.77 1248.0 88.76 7.0 NaN 6.00 Maruti Ertiga VDI 4 Coimbatore 2013 40670 Diesel Automatic Second 15.20 1968.0 140.80 5.0 NaN 17.74 Audi A4 New 2.0 TDI Multitronic <p>Data series are the correct Type.</p> In\u00a0[36]: Copied! <pre># get pandas profiling report\n#pandas_profiling.ProfileReport(data)\n</pre> # get pandas profiling report #pandas_profiling.ProfileReport(data) <p>We are going to perform univariate and bivariate analysis to understand the relationship between the columns</p> In\u00a0[37]: Copied! <pre>#sns.pairplot(data, diag_kind='kde');\n</pre> #sns.pairplot(data, diag_kind='kde'); In\u00a0[38]: Copied! <pre># Get stats for numerical columns\ndata.describe()\n</pre> # Get stats for numerical columns data.describe() Out[38]: Year Kilometers_Driven Mileage Engine Power Seats New_Price Price count 7252.000000 7.252000e+03 7250.000000 7206.000000 7077.000000 7199.000000 1006.000000 6019.000000 mean 2013.365830 5.869999e+04 18.141738 1616.590064 112.764474 5.279761 22.779692 9.479468 std 3.254405 8.443351e+04 4.562492 595.324779 53.497297 0.811709 27.759344 11.187917 min 1996.000000 1.710000e+02 0.000000 72.000000 34.200000 0.000000 3.910000 0.440000 25% 2011.000000 3.400000e+04 15.170000 1198.000000 75.000000 5.000000 7.885000 3.500000 50% 2014.000000 5.342900e+04 18.160000 1493.000000 94.000000 5.000000 11.570000 5.640000 75% 2016.000000 7.300000e+04 21.100000 1968.000000 138.100000 5.000000 26.042500 9.950000 max 2019.000000 6.500000e+06 33.540000 5998.000000 616.000000 10.000000 375.000000 160.000000 In\u00a0[40]: Copied! <pre># Get the skewness of numerical columns\ndata.select_dtypes(include=np.number).skew()\n</pre> # Get the skewness of numerical columns data.select_dtypes(include=np.number).skew() Out[40]: <pre>Year                 -0.840219\nKilometers_Driven    61.578378\nMileage              -0.438397\nEngine                1.412244\nPower                 1.961084\nSeats                 1.902039\nNew_Price             4.128300\nPrice                 3.335232\ndtype: float64</pre> In\u00a0[41]: Copied! <pre># creating the 2 subplots\nf2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2\n                                       sharex = True, # x-axis will be shared among all subplots\n                                       gridspec_kw = {\"height_ratios\": (.25, .75)});\nsns.boxplot(data['Kilometers_Driven'], ax=ax_box2, showmeans=True, color='violet'); # boxplot \nsns.distplot(data['Kilometers_Driven'], kde=True, ax=ax_hist2); # histogram\nax_hist2.axvline(np.mean(data['Kilometers_Driven']), color='green', linestyle='--'); # Add mean to the histogram\nax_hist2.axvline(np.median(data['Kilometers_Driven']), color='black', linestyle='-'); # Add median to the histogram\n</pre> # creating the 2 subplots f2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2                                        sharex = True, # x-axis will be shared among all subplots                                        gridspec_kw = {\"height_ratios\": (.25, .75)}); sns.boxplot(data['Kilometers_Driven'], ax=ax_box2, showmeans=True, color='violet'); # boxplot  sns.distplot(data['Kilometers_Driven'], kde=True, ax=ax_hist2); # histogram ax_hist2.axvline(np.mean(data['Kilometers_Driven']), color='green', linestyle='--'); # Add mean to the histogram ax_hist2.axvline(np.median(data['Kilometers_Driven']), color='black', linestyle='-'); # Add median to the histogram In\u00a0[42]: Copied! <pre># Number of rows with mileage equals to 0 \nsum(data['Mileage']==0)\n</pre> # Number of rows with mileage equals to 0  sum(data['Mileage']==0)  Out[42]: <pre>81</pre> In\u00a0[43]: Copied! <pre># creating the 2 subplots\nf2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2\n                                       sharex = True, # x-axis will be shared among all subplots\n                                       gridspec_kw = {\"height_ratios\": (.25, .75)});\nsns.boxplot(data['Engine'], ax=ax_box2, showmeans=True, color='violet'); # boxplot \nsns.distplot(data['Engine'], kde=True, ax=ax_hist2); # histogram\nax_hist2.axvline(np.mean(data['Engine']), color='green', linestyle='--'); # Add mean to the histogram\nax_hist2.axvline(np.median(data['Engine']), color='black', linestyle='-'); # Add median to the histogram\n</pre> # creating the 2 subplots f2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2                                        sharex = True, # x-axis will be shared among all subplots                                        gridspec_kw = {\"height_ratios\": (.25, .75)}); sns.boxplot(data['Engine'], ax=ax_box2, showmeans=True, color='violet'); # boxplot  sns.distplot(data['Engine'], kde=True, ax=ax_hist2); # histogram ax_hist2.axvline(np.mean(data['Engine']), color='green', linestyle='--'); # Add mean to the histogram ax_hist2.axvline(np.median(data['Engine']), color='black', linestyle='-'); # Add median to the histogram <p><code>Engine</code> has several values that are flagged as suspicious by the boxplot. However, those values are consistent with some powerful car models and we cannot considered them as outliers</p> In\u00a0[44]: Copied! <pre># cars with Engine&gt;3000\ndata[data['Engine']&gt;3000]\n</pre> # cars with Engine&gt;3000 data[data['Engine']&gt;3000] Out[44]: Location Year Kilometers_Driven Fuel_Type Transmission Owner_Type Mileage Engine Power Seats New_Price Price Brand Model Specs 70 Mumbai 2008 73000 Petrol Automatic First 8.50 4806.0 500.0 5.0 NaN 14.50 Porsche Cayenne 2009-2014 Turbo 152 Kolkata 2010 35277 Petrol Automatic First 7.81 5461.0 362.9 5.0 NaN 30.00 Mercedes-Benz S Class 2005 2013 S 500 459 Coimbatore 2016 51002 Diesel Automatic First 11.33 4134.0 335.2 7.0 NaN 48.91 Audi Q7 4.2 TDI Quattro Technology 586 Kochi 2014 79926 Diesel Automatic First 11.33 4134.0 335.2 7.0 NaN 29.77 Audi Q7 4.2 TDI Quattro Technology 589 Bangalore 2006 47088 Petrol Automatic Second 10.13 3498.0 364.9 5.0 NaN 19.00 Mercedes-Benz S Class 2005 2013 S 350 L ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 6011 Hyderabad 2009 53000 Petrol Automatic First 0.00 3597.0 262.6 5.0 NaN 4.75 Skoda Superb 3.6 V6 FSI 6186 Mumbai 2008 65000 Petrol Automatic Third 10.13 3498.0 364.9 5.0 NaN NaN Mercedes-Benz S Class 2005 2013 S 350 L 6354 Bangalore 2008 31200 Petrol Automatic Second 10.20 5998.0 616.0 5.0 375.0 NaN Bentley Flying Spur W12 6842 Kolkata 2012 14850 Petrol Automatic First 10.00 3696.0 328.5 2.0 NaN NaN Nissan 370Z AT 7057 Delhi 2009 64000 Petrol Automatic First 7.94 4395.0 450.0 4.0 NaN NaN BMW 6 Series 650i Coupe <p>65 rows \u00d7 15 columns</p> In\u00a0[45]: Copied! <pre># creating the 2 subplots\nf2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2\n                                       sharex = True, # x-axis will be shared among all subplots\n                                       gridspec_kw = {\"height_ratios\": (.25, .75)});\nsns.boxplot(data['Power'], ax=ax_box2, showmeans=True, color='violet'); # boxplot \nsns.distplot(data['Power'], kde=True, ax=ax_hist2); # histogram\nax_hist2.axvline(np.mean(data['Power']), color='green', linestyle='--'); # Add mean to the histogram\nax_hist2.axvline(np.median(data['Power']), color='black', linestyle='-'); # Add median to the histogram\n</pre> # creating the 2 subplots f2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2                                        sharex = True, # x-axis will be shared among all subplots                                        gridspec_kw = {\"height_ratios\": (.25, .75)}); sns.boxplot(data['Power'], ax=ax_box2, showmeans=True, color='violet'); # boxplot  sns.distplot(data['Power'], kde=True, ax=ax_hist2); # histogram ax_hist2.axvline(np.mean(data['Power']), color='green', linestyle='--'); # Add mean to the histogram ax_hist2.axvline(np.median(data['Power']), color='black', linestyle='-'); # Add median to the histogram <p>At the same as Engine. Power has several values that are flagged as suspicious by the boxplot. However, those values are consistent with some powerful car models and we cannot considered them as outliers</p> In\u00a0[46]: Copied! <pre># creating the 2 subplots\nf2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2\n                                       sharex = True, # x-axis will be shared among all subplots\n                                       gridspec_kw = {\"height_ratios\": (.25, .75)});\nsns.boxplot(data['New_Price'], ax=ax_box2, showmeans=True, color='violet'); # boxplot \nsns.distplot(data['New_Price'], kde=True, ax=ax_hist2); # histogram\nax_hist2.axvline(np.mean(data['New_Price']), color='green', linestyle='--'); # Add mean to the histogram\nax_hist2.axvline(np.median(data['New_Price']), color='black', linestyle='-'); # Add median to the histogram\n</pre> # creating the 2 subplots f2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2                                        sharex = True, # x-axis will be shared among all subplots                                        gridspec_kw = {\"height_ratios\": (.25, .75)}); sns.boxplot(data['New_Price'], ax=ax_box2, showmeans=True, color='violet'); # boxplot  sns.distplot(data['New_Price'], kde=True, ax=ax_hist2); # histogram ax_hist2.axvline(np.mean(data['New_Price']), color='green', linestyle='--'); # Add mean to the histogram ax_hist2.axvline(np.median(data['New_Price']), color='black', linestyle='-'); # Add median to the histogram <p>There are several values flagged as suspicious by the boxplot, but they could correspond to luxury cars, and we cannot considered as outliers</p> In\u00a0[47]: Copied! <pre># cars with New_Price&gt;100\ndata[data['New_Price']&gt;100]\n</pre> # cars with New_Price&gt;100 data[data['New_Price']&gt;100] Out[47]: Location Year Kilometers_Driven Fuel_Type Transmission Owner_Type Mileage Engine Power Seats New_Price Price Brand Model Specs 148 Mumbai 2013 23000 Petrol Automatic First 11.05 2894.0 444.00 4.0 128.0 37.00 Audi RS5 Coupe 327 Coimbatore 2017 97430 Diesel Automatic First 14.75 2967.0 245.00 7.0 104.0 62.67 Audi Q7 45 TDI Quattro Technology 1336 Mumbai 2016 20002 Diesel Automatic First 14.75 2967.0 245.00 7.0 104.0 67.00 Audi Q7 45 TDI Quattro Technology 1505 Kochi 2019 26013 Diesel Automatic First 12.65 2993.0 255.00 5.0 139.0 97.07 Land Rover Range Rover Sport SE 1885 Delhi 2018 6000 Diesel Automatic First 11.00 2987.0 258.00 7.0 102.0 79.00 Mercedes-Benz GLS 350d Grand Edition 2056 Kochi 2015 29966 Diesel Automatic Second 16.77 2993.0 261.49 5.0 140.0 43.60 BMW 7 Series 730Ld Eminence 2095 Coimbatore 2019 2526 Petrol Automatic First 19.00 2996.0 362.07 2.0 106.0 83.96 Mercedes-Benz SLC 43 AMG 2178 Mumbai 2017 35000 Diesel Automatic First 18.00 2993.0 255.00 7.0 127.0 41.60 Land Rover Discovery HSE Luxury 3.0 TD6 2528 Delhi 2016 59000 Diesel Automatic First 18.00 2993.0 255.00 7.0 113.0 36.75 Land Rover Discovery SE 3.0 TD6 3132 Kochi 2019 14298 Petrol Automatic First 13.33 2995.0 340.00 5.0 136.0 2.02 Porsche Cayenne Base 3199 Kolkata 2012 41100 Diesel Automatic First 16.77 2993.0 261.49 5.0 166.0 26.50 BMW 7 Series 730Ld Design Pure Excellence CBU 3752 Kochi 2015 38467 Diesel Automatic First 12.65 2993.0 255.00 5.0 160.0 70.66 Land Rover Range Rover Sport HSE 4061 Mumbai 2013 23312 Petrol Automatic First 11.05 2894.0 444.00 4.0 128.0 40.50 Audi RS5 Coupe 4079 Hyderabad 2017 25000 Diesel Automatic First 13.33 2993.0 255.00 5.0 230.0 160.00 Land Rover Range Rover 3.0 Diesel LWB Vogue 4778 Bangalore 2011 47140 Diesel Automatic Second 13.50 2925.0 281.61 5.0 171.0 30.00 Mercedes-Benz S-Class S 350 d 5545 Delhi 2014 47000 Diesel Automatic Second 12.65 2993.0 255.00 5.0 139.0 64.75 Land Rover Range Rover Sport SE 6212 Chennai 2017 16000 Diesel Automatic First 16.77 2993.0 261.49 5.0 158.0 NaN BMW 7 Series 730Ld DPE Signature 6354 Bangalore 2008 31200 Petrol Automatic Second 10.20 5998.0 616.00 5.0 375.0 NaN Bentley Flying Spur W12 6960 Coimbatore 2018 18338 Petrol Automatic First 19.00 2996.0 362.07 2.0 106.0 NaN Mercedes-Benz SLC 43 AMG In\u00a0[48]: Copied! <pre># creating the 2 subplots\nf2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2\n                                       sharex = True, # x-axis will be shared among all subplots\n                                       gridspec_kw = {\"height_ratios\": (.25, .75)});\nsns.boxplot(data['Price'], ax=ax_box2, showmeans=True, color='violet'); # boxplot \nsns.distplot(data['Price'], kde=True, ax=ax_hist2); # histogram\nax_hist2.axvline(np.mean(data['Price']), color='green', linestyle='--'); # Add mean to the histogram\nax_hist2.axvline(np.median(data['Price']), color='black', linestyle='-'); # Add median to the histogram\n</pre> # creating the 2 subplots f2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2                                        sharex = True, # x-axis will be shared among all subplots                                        gridspec_kw = {\"height_ratios\": (.25, .75)}); sns.boxplot(data['Price'], ax=ax_box2, showmeans=True, color='violet'); # boxplot  sns.distplot(data['Price'], kde=True, ax=ax_hist2); # histogram ax_hist2.axvline(np.mean(data['Price']), color='green', linestyle='--'); # Add mean to the histogram ax_hist2.axvline(np.median(data['Price']), color='black', linestyle='-'); # Add median to the histogram <p>Similar than New_Price. There are several values flagged as suspicious by the boxplot, but they could correspond to luxury cars, and we cannot considered as outliers</p> In\u00a0[49]: Copied! <pre># cars with Price&gt;25\ndata[data['Price']&gt;25]\n</pre> # cars with Price&gt;25 data[data['Price']&gt;25] Out[49]: Location Year Kilometers_Driven Fuel_Type Transmission Owner_Type Mileage Engine Power Seats New_Price Price Brand Model Specs 13 Delhi 2014 72000 Diesel Automatic First 12.70 2179.0 187.70 5.0 NaN 27.00 Land Rover Range Rover 2.2L Pure 19 Bangalore 2014 78500 Diesel Automatic First 14.84 2143.0 167.62 5.0 NaN 28.00 Mercedes-Benz New C-Class C 220 CDI BE Avantgare 38 Pune 2013 85000 Diesel Automatic First 11.74 2987.0 254.80 5.0 NaN 28.00 Mercedes-Benz M-Class ML 350 CDI 62 Delhi 2015 58000 Petrol Automatic First 11.74 1796.0 186.00 5.0 NaN 26.70 Mercedes-Benz New C-Class C 200 CGI Avantgarde 67 Coimbatore 2019 15369 Diesel Automatic First 0.00 1950.0 194.00 5.0 49.14 35.67 Mercedes-Benz C-Class Progressive C 220d ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 5927 Coimbatore 2018 29091 Diesel Automatic First 13.22 2967.0 241.40 5.0 NaN 45.52 Audi Q5 3.0 TDI Quattro Technology 5946 Bangalore 2016 16000 Diesel Automatic First 14.69 2993.0 258.00 5.0 NaN 48.00 BMW 5 Series 2013-2017 530d M Sport 5970 Kochi 2018 17773 Petrol Automatic First 13.70 1991.0 183.00 5.0 39.22 26.76 Mercedes-Benz GLA Class 200 Sport 5996 Kochi 2016 31150 Diesel Automatic First 16.36 2179.0 187.70 5.0 NaN 30.54 Jaguar XF 2.2 Litre Luxury 6008 Hyderabad 2013 40000 Diesel Automatic Second 17.85 2967.0 300.00 4.0 NaN 45.00 Porsche Panamera Diesel <p>499 rows \u00d7 15 columns</p> In\u00a0[50]: Copied! <pre>data.describe(include=[\"category\"])\n</pre> data.describe(include=[\"category\"]) Out[50]: Location Fuel_Type Transmission Owner_Type Brand Model Specs count 7252 7252 7252 7252 7252 7252 7251 unique 11 5 2 4 33 219 1893 top Mumbai Diesel Manual First Maruti Swift VDI freq 948 3852 5203 5951 1444 418 88 In\u00a0[51]: Copied! <pre>p = sns.countplot(data['Location'], order=data['Location'].value_counts().index);\nplt.xticks(rotation=45);\n</pre> p = sns.countplot(data['Location'], order=data['Location'].value_counts().index); plt.xticks(rotation=45); <p>There are 11 distinct locations. Mumbai is the most frequent location, and Ahmedabad the least frequent</p> In\u00a0[52]: Copied! <pre>p = sns.countplot(data['Transmission'], order=data['Transmission'].value_counts().index);\nplt.xticks(rotation=45);\n</pre> p = sns.countplot(data['Transmission'], order=data['Transmission'].value_counts().index); plt.xticks(rotation=45); <p>There are 2 distinct Transmission values, Manual and Automatic. Manual corresponds to the 72% of the cars</p> In\u00a0[53]: Copied! <pre>p = sns.countplot(data['Owner_Type'], order=data['Owner_Type'].value_counts().index);\nplt.xticks(rotation=45);\n</pre> p = sns.countplot(data['Owner_Type'], order=data['Owner_Type'].value_counts().index); plt.xticks(rotation=45); <p>There are 4 distinct categories for owner type. First owner corresponds to 82% of the rows</p> In\u00a0[54]: Copied! <pre>p = sns.countplot(data['Fuel_Type'], order=data['Fuel_Type'].value_counts().index);\nplt.xticks(rotation=45);\n</pre> p = sns.countplot(data['Fuel_Type'], order=data['Fuel_Type'].value_counts().index); plt.xticks(rotation=45); <p>There are 5 distinct Fuel Types. Diesel is the most frequent location, and there are only 2 electric cars</p> In\u00a0[55]: Copied! <pre>p = sns.countplot(data['Brand'], order=data['Brand'].value_counts().index);\nplt.xticks(rotation=90);\n</pre> p = sns.countplot(data['Brand'], order=data['Brand'].value_counts().index); plt.xticks(rotation=90); <p>There are 33 distinct Brands. Maruti, Hyundai, Honda and Toyota the most common ones.</p> In\u00a0[57]: Copied! <pre># Get correlation matrix for numeric variables\ndata.select_dtypes(include=np.number).corr()\n</pre> # Get correlation matrix for numeric variables data.select_dtypes(include=np.number).corr() Out[57]: Year Kilometers_Driven Mileage Engine Power Seats New_Price Price Year 1.000000 -0.187884 0.322452 -0.054726 0.013448 0.008166 -0.058798 0.305327 Kilometers_Driven -0.187884 1.000000 -0.069125 0.094816 0.030165 0.090218 -0.008221 -0.011493 Mileage 0.322452 -0.069125 1.000000 -0.593581 -0.531770 -0.310649 -0.378327 -0.306593 Engine -0.054726 0.094816 -0.593581 1.000000 0.859777 0.399256 0.735981 0.658354 Power 0.013448 0.030165 -0.531770 0.859777 1.000000 0.095910 0.877708 0.772566 Seats 0.008166 0.090218 -0.310649 0.399256 0.095910 1.000000 -0.019459 0.052225 New_Price -0.058798 -0.008221 -0.378327 0.735981 0.877708 -0.019459 1.000000 0.871847 Price 0.305327 -0.011493 -0.306593 0.658354 0.772566 0.052225 0.871847 1.000000 In\u00a0[59]: Copied! <pre># Display correlation matrix in a heatmap\nsns.heatmap(data.select_dtypes(include=np.number).corr(), annot=True);\n</pre> # Display correlation matrix in a heatmap sns.heatmap(data.select_dtypes(include=np.number).corr(), annot=True); <ul> <li>Engine has a strong correlation with Power, New_Price and Price</li> <li>Power has a strong correlation with Engine, New_Price and Price</li> <li>New_price has a strong correlation with Engine, Power and Price</li> <li>Price has a strong correlation with Engine, Power and New Price</li> </ul> In\u00a0[60]: Copied! <pre>sns.scatterplot(data=data, x='Power', y='Engine', hue='Price');\n</pre> sns.scatterplot(data=data, x='Power', y='Engine', hue='Price'); <p>There is a strong correlation between Power and Engine. The chart is also showing that more expensive cars tend to have high values for Power and Engine</p> In\u00a0[61]: Copied! <pre>sns.scatterplot(data=data, x='Power', y='Seats', hue='Price');\n</pre> sns.scatterplot(data=data, x='Power', y='Seats', hue='Price'); <p>There is not a clear relationship between Power and Seats. However, cars with 2 seats could have strong power and higher prices.</p> In\u00a0[64]: Copied! <pre>order_by_brand = data.groupby(by=[\"Brand\"])[\"Price\"].median().sort_values().iloc[::-1].index\nplt.figure(figsize=(10,6));\nplt.xticks(rotation=90);\nsns.boxplot(x=data['Brand'], y=data['Price'], order=order_by_brand);\n</pre> order_by_brand = data.groupby(by=[\"Brand\"])[\"Price\"].median().sort_values().iloc[::-1].index plt.figure(figsize=(10,6)); plt.xticks(rotation=90); sns.boxplot(x=data['Brand'], y=data['Price'], order=order_by_brand); <p>This chart shows there are:</p> <ul> <li>Luxury brands that have high prices: BMW, Audi, Mercedes-Benz, Mini, Jaguar, Land, Porsche, Bentley, Lamborghini, Isuzu</li> <li>Brands with medium prices: Ford, Renault, Skoda, Mahindra, Force, Mitsubishi, Toyota, ISUZU, Volvo, Jeep</li> <li>Brands with low prices: Ambassador, Chevrolet, Fiat, Tata, Smart, Datsun, Maruti, Nissan, Hyundai, Volkswagen, Honda</li> </ul> In\u00a0[65]: Copied! <pre>order_by_loc = data.groupby(by=[\"Fuel_Type\"])[\"Price\"].median().sort_values().iloc[::-1].index\nplt.figure(figsize=(15,6));\nplt.xticks(rotation=90);\nsns.boxplot(x=data['Fuel_Type'], y=data['Price'], hue=data['Location'], order=order_by_loc);\n</pre> order_by_loc = data.groupby(by=[\"Fuel_Type\"])[\"Price\"].median().sort_values().iloc[::-1].index plt.figure(figsize=(15,6)); plt.xticks(rotation=90); sns.boxplot(x=data['Fuel_Type'], y=data['Price'], hue=data['Location'], order=order_by_loc); <ul> <li>Electric and Diesel cars have higher Price than Petrol, CNG and LPG.</li> <li>Cars in Bangalore, Coimbatore, Kochi and Mumbai tend to have higher prices than other locations</li> </ul> <p>First we are going to drop column <code>New_Price</code> since it has 6247(86.1%) rows with missing data.</p> In\u00a0[66]: Copied! <pre>data.drop(['New_Price'], axis=1, inplace=True)\n</pre> data.drop(['New_Price'], axis=1, inplace=True) <p>There are 1234 rows with missing Prices. We are going to drop all those rows because Price is the variable we would like to predict and we don't want to create artificial information in the model</p> In\u00a0[67]: Copied! <pre>data.drop(data[data['Price'].isna()].index, axis=0, inplace=True)\n</pre> data.drop(data[data['Price'].isna()].index, axis=0, inplace=True) <p>Let's check new data set</p> In\u00a0[68]: Copied! <pre>data.info()\n</pre> data.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 6019 entries, 0 to 6018\nData columns (total 14 columns):\n #   Column             Non-Null Count  Dtype   \n---  ------             --------------  -----   \n 0   Location           6019 non-null   category\n 1   Year               6019 non-null   int64   \n 2   Kilometers_Driven  6019 non-null   int64   \n 3   Fuel_Type          6019 non-null   category\n 4   Transmission       6019 non-null   category\n 5   Owner_Type         6019 non-null   category\n 6   Mileage            6017 non-null   float64 \n 7   Engine             5983 non-null   float64 \n 8   Power              5876 non-null   float64 \n 9   Seats              5977 non-null   float64 \n 10  Price              6019 non-null   float64 \n 11  Brand              6019 non-null   category\n 12  Model              6019 non-null   category\n 13  Specs              6019 non-null   category\ndtypes: category(7), float64(5), int64(2)\nmemory usage: 520.4 KB\n</pre> In\u00a0[69]: Copied! <pre># counting the number of missing values per row\nnum_missing = data.isnull().sum(axis=1)\nnum_missing.value_counts()\n</pre> # counting the number of missing values per row num_missing = data.isnull().sum(axis=1) num_missing.value_counts() Out[69]: <pre>0    5872\n1     107\n3      36\n2       4\nName: count, dtype: int64</pre> <p>We are going to analyze if there is a pattern for the 36 rows with 3 missing values.</p> In\u00a0[70]: Copied! <pre>data[num_missing == 3]\n</pre> data[num_missing == 3] Out[70]: Location Year Kilometers_Driven Fuel_Type Transmission Owner_Type Mileage Engine Power Seats Price Brand Model Specs 194 Ahmedabad 2007 60006 Petrol Manual First 0.00 NaN NaN NaN 2.95 Honda City 1.5 GXI 208 Kolkata 2010 42001 Petrol Manual First 16.10 NaN NaN NaN 2.11 Maruti Swift 1.3 VXi 733 Chennai 2006 97800 Petrol Manual Third 16.10 NaN NaN NaN 1.75 Maruti Swift 1.3 VXi 749 Mumbai 2008 55001 Diesel Automatic Second 0.00 NaN NaN NaN 26.50 Land Rover Range Rover 3.0 D 1294 Delhi 2009 55005 Petrol Manual First 12.80 NaN NaN NaN 3.20 Honda City 1.3 DX 1327 Hyderabad 2015 50295 Petrol Manual First 16.10 NaN NaN NaN 5.80 Maruti Swift 1.3 ZXI 1385 Pune 2004 115000 Petrol Manual Second 0.00 NaN NaN NaN 1.50 Honda City 1.5 GXI 1460 Coimbatore 2008 69078 Petrol Manual First 0.00 NaN NaN NaN 40.88 Land Rover Range Rover Sport 2005 2012 Sport 2074 Pune 2011 24255 Petrol Manual First 16.10 NaN NaN NaN 3.15 Maruti Swift 1.3 LXI 2096 Coimbatore 2004 52146 Petrol Manual First 0.00 NaN NaN NaN 1.93 Hyundai Santro LP zipPlus 2264 Pune 2012 24500 Petrol Manual Second 18.30 NaN NaN NaN 2.95 Toyota Etios Liva V 2325 Pune 2015 67000 Petrol Manual First 16.10 NaN NaN NaN 4.70 Maruti Swift 1.3 VXI ABS 2335 Mumbai 2007 55000 Petrol Manual Second 16.10 NaN NaN NaN 1.75 Maruti Swift 1.3 VXi 2530 Kochi 2014 64158 Diesel Automatic First 18.48 NaN NaN NaN 17.89 BMW 5 Series 520d Sedan 2542 Bangalore 2011 65000 Petrol Manual Second 0.00 NaN NaN NaN 3.15 Hyundai Santro GLS II - Euro II 2623 Pune 2012 95000 Diesel Automatic Second 18.48 NaN NaN NaN 18.00 BMW 5 Series 520d Sedan 2668 Kolkata 2014 32986 Petrol Manual First 16.10 NaN NaN NaN 4.24 Maruti Swift 1.3 VXi 2737 Jaipur 2001 200000 Petrol Manual First 12.00 NaN NaN NaN 0.70 Maruti Wagon R Vx 2780 Pune 2009 100000 Petrol Manual First 0.00 NaN NaN NaN 1.60 Hyundai Santro GLS II - Euro II 2842 Bangalore 2012 43000 Petrol Manual First 0.00 NaN NaN NaN 3.25 Hyundai Santro GLS II - Euro II 3272 Mumbai 2008 81000 Diesel Automatic Second 18.48 NaN NaN NaN 10.50 BMW 5 Series 520d Sedan 3404 Jaipur 2006 125000 Petrol Manual Fourth &amp; Above 16.10 NaN NaN NaN 2.35 Maruti Swift 1.3 VXi 3520 Delhi 2012 90000 Diesel Automatic First 18.48 NaN NaN NaN 14.50 BMW 5 Series 520d Sedan 3522 Kochi 2012 66400 Petrol Manual First 0.00 NaN NaN NaN 2.66 Hyundai Santro GLS II - Euro II 3810 Kolkata 2013 27000 Petrol Automatic First 14.00 NaN NaN NaN 11.99 Honda CR-V AT With Sun Roof 4011 Pune 2011 45271 Diesel Manual First 20.30 NaN NaN NaN 2.60 Fiat Punto 1.3 Emotion 4152 Mumbai 2003 75000 Diesel Automatic Second 0.00 NaN NaN NaN 16.11 Land Rover Range Rover 3.0 D 4229 Bangalore 2005 79000 Petrol Manual Second 17.00 NaN NaN NaN 1.65 Hyundai Santro Xing XG 4577 Delhi 2012 72000 Diesel Automatic Third 18.48 NaN NaN NaN 13.85 BMW 5 Series 520d Sedan 4604 Pune 2011 98000 Petrol Manual First 16.70 NaN NaN NaN 3.15 Honda Jazz Select Edition 4697 Kochi 2017 17941 Petrol Manual First 15.70 NaN NaN NaN 3.93 Fiat Punto 1.2 Dynamic 4712 Pune 2003 80000 Petrol Manual Second 17.00 NaN NaN NaN 0.90 Hyundai Santro Xing XG 4952 Kolkata 2010 47000 Petrol Manual First 14.60 NaN NaN NaN 1.49 Fiat Punto 1.4 Emotion 5015 Delhi 2006 63000 Petrol Manual First 16.10 NaN NaN NaN 1.60 Maruti Swift 1.3 VXi 5185 Delhi 2012 52000 Petrol Manual First 16.10 NaN NaN NaN 3.65 Maruti Swift 1.3 LXI 5270 Bangalore 2002 53000 Petrol Manual Second 0.00 NaN NaN NaN 1.85 Honda City 1.5 GXI <p>Now, we are going to get the columns with missing values</p> In\u00a0[71]: Copied! <pre>for n in num_missing.value_counts().sort_index().index:\n    if n &gt; 0:\n        print(f'Rows with exactly {n} missing values, NAs are found in:')\n        n_miss_per_col = data[num_missing == n].isnull().sum()\n        print(n_miss_per_col[n_miss_per_col &gt; 0])\n        print('\\n')\n</pre> for n in num_missing.value_counts().sort_index().index:     if n &gt; 0:         print(f'Rows with exactly {n} missing values, NAs are found in:')         n_miss_per_col = data[num_missing == n].isnull().sum()         print(n_miss_per_col[n_miss_per_col &gt; 0])         print('\\n')          <pre>Rows with exactly 1 missing values, NAs are found in:\nMileage      2\nPower      103\nSeats        2\ndtype: int64\n\n\nRows with exactly 2 missing values, NAs are found in:\nPower    4\nSeats    4\ndtype: int64\n\n\nRows with exactly 3 missing values, NAs are found in:\nEngine    36\nPower     36\nSeats     36\ndtype: int64\n\n\n</pre> <p>Now, let's calculate the percentage of missing values per column</p> In\u00a0[72]: Copied! <pre># percentage of missing values\ndata.isnull().sum(axis=0)\n</pre> # percentage of missing values data.isnull().sum(axis=0) Out[72]: <pre>Location               0\nYear                   0\nKilometers_Driven      0\nFuel_Type              0\nTransmission           0\nOwner_Type             0\nMileage                2\nEngine                36\nPower                143\nSeats                 42\nPrice                  0\nBrand                  0\nModel                  0\nSpecs                  0\ndtype: int64</pre> <ul> <li><code>Engine</code>, <code>Power</code>, <code>Seats</code> and <code>Mileage</code> columns have missing values.</li> <li><code>Power</code> column has 143 rows (2.5% of rows) with missing values.</li> <li>Since the percentage of missing values is lower than 3% for all columns, we are going to impute missing values with the k-Nearest Neighbors using KKNImputer.</li> <li>We select the k-Nearest Neighbors instead of the mean to avoid the influence of outliers in those columns</li> </ul> In\u00a0[74]: Copied! <pre># load KNNImputer \nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer()\n</pre> # load KNNImputer  from sklearn.impute import KNNImputer imputer = KNNImputer() In\u00a0[75]: Copied! <pre># create data set with only numeric columns\ndata_n = data.select_dtypes(include=np.number)\ndata_n_cols = data_n.columns.tolist()\ndata_n.info()\n</pre> # create data set with only numeric columns data_n = data.select_dtypes(include=np.number) data_n_cols = data_n.columns.tolist() data_n.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 6019 entries, 0 to 6018\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Year               6019 non-null   int64  \n 1   Kilometers_Driven  6019 non-null   int64  \n 2   Mileage            6017 non-null   float64\n 3   Engine             5983 non-null   float64\n 4   Power              5876 non-null   float64\n 5   Seats              5977 non-null   float64\n 6   Price              6019 non-null   float64\ndtypes: float64(5), int64(2)\nmemory usage: 376.2 KB\n</pre> In\u00a0[76]: Copied! <pre># input values with KNNImputer\ndata_n = pd.DataFrame(imputer.fit_transform(data_n), columns=data_n_cols)\ndata_n.info()\n</pre> # input values with KNNImputer data_n = pd.DataFrame(imputer.fit_transform(data_n), columns=data_n_cols) data_n.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6019 entries, 0 to 6018\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Year               6019 non-null   float64\n 1   Kilometers_Driven  6019 non-null   float64\n 2   Mileage            6019 non-null   float64\n 3   Engine             6019 non-null   float64\n 4   Power              6019 non-null   float64\n 5   Seats              6019 non-null   float64\n 6   Price              6019 non-null   float64\ndtypes: float64(7)\nmemory usage: 329.3 KB\n</pre> In\u00a0[77]: Copied! <pre># replace columns with new imputed columns\ndata['Power'] = data_n['Power']\ndata['Mileage'] = data_n['Mileage']\ndata['Engine'] = data_n['Engine']\ndata['Seats'] = data_n['Seats']\n#check there are not missing values\ndata.info()\n</pre> # replace columns with new imputed columns data['Power'] = data_n['Power'] data['Mileage'] = data_n['Mileage'] data['Engine'] = data_n['Engine'] data['Seats'] = data_n['Seats'] #check there are not missing values data.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 6019 entries, 0 to 6018\nData columns (total 14 columns):\n #   Column             Non-Null Count  Dtype   \n---  ------             --------------  -----   \n 0   Location           6019 non-null   category\n 1   Year               6019 non-null   int64   \n 2   Kilometers_Driven  6019 non-null   int64   \n 3   Fuel_Type          6019 non-null   category\n 4   Transmission       6019 non-null   category\n 5   Owner_Type         6019 non-null   category\n 6   Mileage            6019 non-null   float64 \n 7   Engine             6019 non-null   float64 \n 8   Power              6019 non-null   float64 \n 9   Seats              6019 non-null   float64 \n 10  Price              6019 non-null   float64 \n 11  Brand              6019 non-null   category\n 12  Model              6019 non-null   category\n 13  Specs              6019 non-null   category\ndtypes: category(7), float64(5), int64(2)\nmemory usage: 520.4 KB\n</pre> <p>There are no data missing and we can continue with the analysis</p> In\u00a0[78]: Copied! <pre>sns.histplot(data['Kilometers_Driven']);\n</pre> sns.histplot(data['Kilometers_Driven']); In\u00a0[79]: Copied! <pre># distribution of the log transformation\nsns.histplot(np.log(data['Kilometers_Driven']));\n</pre> # distribution of the log transformation sns.histplot(np.log(data['Kilometers_Driven'])); <p>We can see a very good improvement in the distribution. Now, we are going to create a new column with the log of Kilometers_Driven and drop the Kilometers_Driven column</p> In\u00a0[80]: Copied! <pre>data['Kilometers_Driven_log'] = np.log(data['Kilometers_Driven'])\ndata.drop('Kilometers_Driven', axis=1, inplace=True)\n</pre> data['Kilometers_Driven_log'] = np.log(data['Kilometers_Driven']) data.drop('Kilometers_Driven', axis=1, inplace=True) In\u00a0[81]: Copied! <pre># stats for new Kilometers_Driven_log column\ndata['Kilometers_Driven_log'].describe()\n</pre> # stats for new Kilometers_Driven_log column data['Kilometers_Driven_log'].describe() Out[81]: <pre>count    6019.000000\nmean       10.758780\nstd         0.715788\nmin         5.141664\n25%        10.434116\n50%        10.878047\n75%        11.198215\nmax        15.687313\nName: Kilometers_Driven_log, dtype: float64</pre> In\u00a0[82]: Copied! <pre>data['Kilometers_Driven_log'].skew()\n</pre> data['Kilometers_Driven_log'].skew() Out[82]: <pre>-1.29076524053299</pre> In\u00a0[83]: Copied! <pre># creating the 2 subplots\nf2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2\n                                       sharex = True, # x-axis will be shared among all subplots\n                                       gridspec_kw = {\"height_ratios\": (.25, .75)});\nsns.boxplot(data['Kilometers_Driven_log'], ax=ax_box2, showmeans=True, color='violet'); # boxplot \nsns.distplot(data['Kilometers_Driven_log'], kde=True, ax=ax_hist2); # histogram\nax_hist2.axvline(np.mean(data['Kilometers_Driven_log']), color='green', linestyle='--'); # Add mean to the histogram\nax_hist2.axvline(np.median(data['Kilometers_Driven_log']), color='black', linestyle='-');\n</pre> # creating the 2 subplots f2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2                                        sharex = True, # x-axis will be shared among all subplots                                        gridspec_kw = {\"height_ratios\": (.25, .75)}); sns.boxplot(data['Kilometers_Driven_log'], ax=ax_box2, showmeans=True, color='violet'); # boxplot  sns.distplot(data['Kilometers_Driven_log'], kde=True, ax=ax_hist2); # histogram ax_hist2.axvline(np.mean(data['Kilometers_Driven_log']), color='green', linestyle='--'); # Add mean to the histogram ax_hist2.axvline(np.median(data['Kilometers_Driven_log']), color='black', linestyle='-'); <p>There are several values flagged as suspicious by the boxplot for the Kilometers_Driven_log column. There are some outliers above 14, but the rest of the points aren't inconsistent with the overall distribution of the data.</p> In\u00a0[84]: Copied! <pre>sns.histplot(data['Power']);\n</pre> sns.histplot(data['Power']); In\u00a0[85]: Copied! <pre># distribution of the log transformation\nsns.histplot(np.log(data['Power']));\n</pre> # distribution of the log transformation sns.histplot(np.log(data['Power'])); <p>We can see an improvement in the distribution. Now, we are going to create a new column with the log of Power and drop the Power column</p> In\u00a0[86]: Copied! <pre>data['Power_log'] = np.log(data['Power'])\ndata.drop('Power', axis=1, inplace=True)\n</pre> data['Power_log'] = np.log(data['Power']) data.drop('Power', axis=1, inplace=True) In\u00a0[87]: Copied! <pre># stats for new Kilometers_Driven_log column\ndata['Power_log'].describe()\n</pre> # stats for new Kilometers_Driven_log column data['Power_log'].describe() Out[87]: <pre>count    6019.000000\nmean        4.635187\nstd         0.414201\nmin         3.532226\n25%         4.317488\n50%         4.543295\n75%         4.927978\nmax         6.327937\nName: Power_log, dtype: float64</pre> In\u00a0[88]: Copied! <pre>data['Power_log'].skew()\n</pre> data['Power_log'].skew() Out[88]: <pre>0.46088996911606844</pre> In\u00a0[89]: Copied! <pre># creating the 2 subplots\nf2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2\n                                       sharex = True, # x-axis will be shared among all subplots\n                                       gridspec_kw = {\"height_ratios\": (.25, .75)});\nsns.boxplot(data['Power_log'], ax=ax_box2, showmeans=True, color='violet'); # boxplot \nsns.distplot(data['Power_log'], kde=True, ax=ax_hist2); # histogram\nax_hist2.axvline(np.mean(data['Power_log']), color='green', linestyle='--'); # Add mean to the histogram\nax_hist2.axvline(np.median(data['Power_log']), color='black', linestyle='-');\n</pre> # creating the 2 subplots f2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2                                        sharex = True, # x-axis will be shared among all subplots                                        gridspec_kw = {\"height_ratios\": (.25, .75)}); sns.boxplot(data['Power_log'], ax=ax_box2, showmeans=True, color='violet'); # boxplot  sns.distplot(data['Power_log'], kde=True, ax=ax_hist2); # histogram ax_hist2.axvline(np.mean(data['Power_log']), color='green', linestyle='--'); # Add mean to the histogram ax_hist2.axvline(np.median(data['Power_log']), color='black', linestyle='-'); <p>There are several values flagged as suspicious by the boxplot for the Power_log column. However, those points aren't inconsistent with the overall distribution of the data.</p> In\u00a0[90]: Copied! <pre>sns.histplot(data['Engine']);\n</pre> sns.histplot(data['Engine']); In\u00a0[91]: Copied! <pre># distribution of the log transformation\nsns.histplot(np.log(data['Engine']));\n</pre> # distribution of the log transformation sns.histplot(np.log(data['Engine'])); <p>We do not see an improvement in the distribution, and we are going to keep the original column</p> In\u00a0[92]: Copied! <pre>sns.histplot(data['Price']);\n</pre> sns.histplot(data['Price']); In\u00a0[93]: Copied! <pre># distribution of the log transformation\nsns.histplot(np.log(data['Price']));\n</pre> # distribution of the log transformation sns.histplot(np.log(data['Price'])); <p>We can see an improvement in the distribution. Now, we are going to create a new column with the log of Price and drop the Price column</p> In\u00a0[94]: Copied! <pre>data['Price_log'] = np.log(data['Price'])\ndata.drop('Price', axis=1, inplace=True)\n</pre> data['Price_log'] = np.log(data['Price']) data.drop('Price', axis=1, inplace=True) In\u00a0[95]: Copied! <pre># stats for new Kilometers_Driven_log column\ndata['Price_log'].describe()\n</pre> # stats for new Kilometers_Driven_log column data['Price_log'].describe() Out[95]: <pre>count    6019.000000\nmean        1.825095\nstd         0.874059\nmin        -0.820981\n25%         1.252763\n50%         1.729884\n75%         2.297573\nmax         5.075174\nName: Price_log, dtype: float64</pre> In\u00a0[96]: Copied! <pre>data['Price_log'].skew()\n</pre> data['Price_log'].skew() Out[96]: <pre>0.4173906918413524</pre> In\u00a0[97]: Copied! <pre># creating the 2 subplots\nf2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2\n                                       sharex = True, # x-axis will be shared among all subplots\n                                       gridspec_kw = {\"height_ratios\": (.25, .75)});\nsns.boxplot(data['Price_log'], ax=ax_box2, showmeans=True, color='violet'); # boxplot \nsns.distplot(data['Price_log'], kde=True, ax=ax_hist2); # histogram\nax_hist2.axvline(np.mean(data['Price_log']), color='green', linestyle='--'); # Add mean to the histogram\nax_hist2.axvline(np.median(data['Price_log']), color='black', linestyle='-');\n</pre> # creating the 2 subplots f2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2                                        sharex = True, # x-axis will be shared among all subplots                                        gridspec_kw = {\"height_ratios\": (.25, .75)}); sns.boxplot(data['Price_log'], ax=ax_box2, showmeans=True, color='violet'); # boxplot  sns.distplot(data['Price_log'], kde=True, ax=ax_hist2); # histogram ax_hist2.axvline(np.mean(data['Price_log']), color='green', linestyle='--'); # Add mean to the histogram ax_hist2.axvline(np.median(data['Price_log']), color='black', linestyle='-'); There are several values flagged as suspicious by the boxplot for the Price_log column. However, those points aren't inconsistent with the overall distribution of the data.  <p>Kilometers_Driven_log have some outliers above 14. We are going to replace those values with the median</p> In\u00a0[98]: Copied! <pre># replacing zeros with mean\ndata.loc[data['Kilometers_Driven_log']&gt;14,'Kilometers_Driven_log'] = data['Kilometers_Driven_log'].mean()\n</pre> # replacing zeros with mean data.loc[data['Kilometers_Driven_log']&gt;14,'Kilometers_Driven_log'] = data['Kilometers_Driven_log'].mean() <p>Mileage column have several rows with value equals zero. We are going to replace those values with the median</p> In\u00a0[99]: Copied! <pre># replacing zeros with mean\ndata.loc[data['Mileage']==0,'Mileage'] = data['Mileage'].mean()\n</pre> # replacing zeros with mean data.loc[data['Mileage']==0,'Mileage'] = data['Mileage'].mean() In\u00a0[100]: Copied! <pre># check new distribution\nsns.histplot(data['Mileage']);\n</pre> # check new distribution sns.histplot(data['Mileage']); In\u00a0[101]: Copied! <pre>data['Mileage'].describe()\n</pre> data['Mileage'].describe() Out[101]: <pre>count    6019.000000\nmean       18.340122\nstd         4.151511\nmin         6.400000\n25%        15.400000\n50%        18.150000\n75%        21.100000\nmax        33.540000\nName: Mileage, dtype: float64</pre> In\u00a0[102]: Copied! <pre>data[data['Seats']==0]\n</pre> data[data['Seats']==0] Out[102]: Location Year Fuel_Type Transmission Owner_Type Mileage Engine Seats Brand Model Specs Kilometers_Driven_log Power_log Price_log 3999 Hyderabad 2012 Petrol Automatic First 10.5 3197.0 0.0 Audi A4 3.2 FSI Tiptronic Quattro 11.736069 5.084134 2.890372 In\u00a0[103]: Copied! <pre># replacing zeros with mean\ndata.loc[data['Seats']==0,'Seats'] = data['Seats'].mean()\n</pre> # replacing zeros with mean data.loc[data['Seats']==0,'Seats'] = data['Seats'].mean() <p>First, we are going to drop column Specs because it has high cardinality (1893 distinct values)</p> In\u00a0[104]: Copied! <pre>data.drop(['Specs'], axis=1, inplace=True)\n</pre> data.drop(['Specs'], axis=1, inplace=True) In\u00a0[105]: Copied! <pre># check there are not missing values and columns are the correcy type\ndata.info()\n</pre> # check there are not missing values and columns are the correcy type data.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 6019 entries, 0 to 6018\nData columns (total 13 columns):\n #   Column                 Non-Null Count  Dtype   \n---  ------                 --------------  -----   \n 0   Location               6019 non-null   category\n 1   Year                   6019 non-null   int64   \n 2   Fuel_Type              6019 non-null   category\n 3   Transmission           6019 non-null   category\n 4   Owner_Type             6019 non-null   category\n 5   Mileage                6019 non-null   float64 \n 6   Engine                 6019 non-null   float64 \n 7   Seats                  6019 non-null   float64 \n 8   Brand                  6019 non-null   category\n 9   Model                  6019 non-null   category\n 10  Kilometers_Driven_log  6019 non-null   float64 \n 11  Power_log              6019 non-null   float64 \n 12  Price_log              6019 non-null   float64 \ndtypes: category(6), float64(6), int64(1)\nmemory usage: 429.4 KB\n</pre> In\u00a0[106]: Copied! <pre>ind_vars = data.drop([\"Price_log\"], axis=1)\ndep_var = data[[\"Price_log\"]]\n</pre> ind_vars = data.drop([\"Price_log\"], axis=1) dep_var = data[[\"Price_log\"]] In\u00a0[107]: Copied! <pre>def encode_cat_vars(x):\n    x = pd.get_dummies(\n        x,\n        columns=x.select_dtypes(include=[\"object\", \"category\"]).columns.tolist(),\n        drop_first=True,\n    )\n    return x\n\n\nind_vars_num = encode_cat_vars(ind_vars)\nind_vars_num.head()\n</pre> def encode_cat_vars(x):     x = pd.get_dummies(         x,         columns=x.select_dtypes(include=[\"object\", \"category\"]).columns.tolist(),         drop_first=True,     )     return x   ind_vars_num = encode_cat_vars(ind_vars) ind_vars_num.head()  Out[107]: Year Mileage Engine Seats Kilometers_Driven_log Power_log Location_Bangalore Location_Chennai Location_Coimbatore Location_Delhi ... Model_Xcent Model_Xenon Model_Xylo Model_Yeti Model_Z4 Model_Zen Model_Zest Model_i10 Model_i20 Model_redi-GO 0 2010 26.60 998.0 5.0 11.184421 4.063198 False False False False ... False False False False False False False False False False 1 2015 19.67 1582.0 5.0 10.621327 4.837868 False False False False ... False False False False False False False False False False 2 2011 18.20 1199.0 5.0 10.736397 4.485260 False True False False ... False False False False False False False False False False 3 2012 20.77 1248.0 7.0 11.373663 4.485936 False True False False ... False False False False False False False False False False 4 2013 15.20 1968.0 5.0 10.613246 4.947340 False False True False ... False False False False False False False False False False <p>5 rows \u00d7 274 columns</p> In\u00a0[108]: Copied! <pre>ind_vars_num.shape\n</pre> ind_vars_num.shape Out[108]: <pre>(6019, 274)</pre> <p>The independent set has 6019 rows and 274 columns</p> In\u00a0[109]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Create train and test data sets\nx_train, x_test, y_train, y_test = train_test_split(\n    ind_vars_num, dep_var, test_size=0.3, random_state=1\n)\n\n\n# Create train and test data sets\nx_train3, x_test3, y_train3, y_test3 = train_test_split(\n    ind_vars_num, dep_var, test_size=0.2, random_state=10\n)\n</pre> from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression  # Create train and test data sets x_train, x_test, y_train, y_test = train_test_split(     ind_vars_num, dep_var, test_size=0.3, random_state=1 )   # Create train and test data sets x_train3, x_test3, y_train3, y_test3 = train_test_split(     ind_vars_num, dep_var, test_size=0.2, random_state=10 ) In\u00a0[110]: Copied! <pre>print(\"Number of rows in train data =\", x_train.shape[0])\nprint(\"Number of rows in train data =\", x_test.shape[0])\n</pre> print(\"Number of rows in train data =\", x_train.shape[0]) print(\"Number of rows in train data =\", x_test.shape[0]) <pre>Number of rows in train data = 4213\nNumber of rows in train data = 1806\n</pre> In\u00a0[111]: Copied! <pre># Run Linear Regression\nlin_reg_model = LinearRegression()\nlin_reg_model.fit(x_train, y_train)\n</pre> # Run Linear Regression lin_reg_model = LinearRegression() lin_reg_model.fit(x_train, y_train) Out[111]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0LinearRegression?Documentation for LinearRegressioniFitted<pre>LinearRegression()</pre> In\u00a0[112]: Copied! <pre># R^2 train set\nlin_reg_model.score(x_train, y_train)\n</pre> # R^2 train set lin_reg_model.score(x_train, y_train) Out[112]: <pre>0.9587840089626443</pre> In\u00a0[113]: Copied! <pre># R^2 test set\nlin_reg_model.score(x_test, y_test)\n</pre> # R^2 test set lin_reg_model.score(x_test, y_test) Out[113]: <pre>0.959104503710836</pre> In\u00a0[114]: Copied! <pre>def r2(y,y_predict):\n    e = y-y_predict\n    ym = np.mean(y)\n    v = y-ym\n    e2 = np.sum(e*e)\n    v2 = np.sum(v*v)\n    return 1-(e2/v2)\n</pre> def r2(y,y_predict):     e = y-y_predict     ym = np.mean(y)     v = y-ym     e2 = np.sum(e*e)     v2 = np.sum(v*v)     return 1-(e2/v2) In\u00a0[115]: Copied! <pre>r2(y_train,lin_reg_model.predict(x_train))\n</pre> r2(y_train,lin_reg_model.predict(x_train)) Out[115]: <pre>Price_log    0.958784\ndtype: float64</pre> In\u00a0[116]: Copied! <pre>r2(y_test,lin_reg_model.predict(x_test))\n</pre> r2(y_test,lin_reg_model.predict(x_test)) Out[116]: <pre>Price_log    0.959105\ndtype: float64</pre> In\u00a0[117]: Copied! <pre>r2(np.exp(y_train),np.exp(lin_reg_model.predict(x_train)))\n</pre> r2(np.exp(y_train),np.exp(lin_reg_model.predict(x_train))) Out[117]: <pre>Price_log    0.922877\ndtype: float64</pre> In\u00a0[118]: Copied! <pre>r2(np.exp(y_test),np.exp(lin_reg_model.predict(x_test)))\n</pre> r2(np.exp(y_test),np.exp(lin_reg_model.predict(x_test))) Out[118]: <pre>Price_log    0.908336\ndtype: float64</pre> <p>The $R^2$ for the train set is 0.958 and for the test set is 0.958. Both values are comparable and very similar. Therefore, the model is not overfitting and the performance is very good</p> In\u00a0[119]: Copied! <pre># To check model performance\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n# Adjusted R^2\ndef adj_r2(ind_vars, targets, predictions):\n    r2 = r2_score(targets, predictions)\n    n = ind_vars.shape[0]\n    k = ind_vars.shape[1]\n    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n\n\n# Model performance check\ndef model_perf(model, inp, out):\n\n    y_pred = model.predict(inp)\n    y_act = out.values\n    \n    #Dictionary with metrics\n    metrics = {\"RMSE\": np.sqrt(mean_squared_error(y_act, y_pred)),\n               \"MAE\": mean_absolute_error(y_act, y_pred),\n               \"R^2\": r2_score(y_act, y_pred),\n               \"Adjusted R^2\": adj_r2(inp, y_act, y_pred)}\n    return metrics\n</pre> # To check model performance from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error  # Adjusted R^2 def adj_r2(ind_vars, targets, predictions):     r2 = r2_score(targets, predictions)     n = ind_vars.shape[0]     k = ind_vars.shape[1]     return 1 - ((1 - r2) * (n - 1) / (n - k - 1))   # Model performance check def model_perf(model, inp, out):      y_pred = model.predict(inp)     y_act = out.values          #Dictionary with metrics     metrics = {\"RMSE\": np.sqrt(mean_squared_error(y_act, y_pred)),                \"MAE\": mean_absolute_error(y_act, y_pred),                \"R^2\": r2_score(y_act, y_pred),                \"Adjusted R^2\": adj_r2(inp, y_act, y_pred)}     return metrics In\u00a0[120]: Copied! <pre># Model performance on train set\nmodel_perf(lin_reg_model, x_train, y_train)\n</pre> # Model performance on train set model_perf(lin_reg_model, x_train, y_train) Out[120]: <pre>{'RMSE': 0.1770842726656583,\n 'MAE': 0.12405596763815441,\n 'R^2': 0.9587840089626443,\n 'Adjusted R^2': 0.9559162635222594}</pre> In\u00a0[121]: Copied! <pre># Model performance on test set\nmodel_perf(lin_reg_model, x_test, y_test)\n</pre> # Model performance on test set model_perf(lin_reg_model, x_test, y_test) Out[121]: <pre>{'RMSE': 0.17745658563106884,\n 'MAE': 0.12829130869663247,\n 'R^2': 0.959104503710836,\n 'Adjusted R^2': 0.9517855187446499}</pre> <p>We can conclude that the model is not overfitting since all metrics are comparable in both train and test sets. The model is able to predict Prices with a mean error of 0.129 on the test set</p> <p>Now, we are going to analyze the distribution of the residuals</p> In\u00a0[122]: Copied! <pre># train set residuals distribution\nresiduals_train = lin_reg_model.predict(x_train) - y_train\nhplot = sns.histplot(residuals_train, kde=True);\nhplot.set_xlim(-1,1);\n</pre> # train set residuals distribution residuals_train = lin_reg_model.predict(x_train) - y_train hplot = sns.histplot(residuals_train, kde=True); hplot.set_xlim(-1,1); In\u00a0[123]: Copied! <pre># scatterplot between residuals and predicted variables\ny_train_predict = pd.DataFrame(lin_reg_model.predict(x_train), columns=['y_predict'])\nsns.scatterplot(x=y_train_predict['y_predict'], y=residuals_train['Price_log']);\n</pre> # scatterplot between residuals and predicted variables y_train_predict = pd.DataFrame(lin_reg_model.predict(x_train), columns=['y_predict']) sns.scatterplot(x=y_train_predict['y_predict'], y=residuals_train['Price_log']); <p>The scatter plot is random and therefore the model does not violate the assumption of Homoscedasticity</p> In\u00a0[124]: Copied! <pre>residuals_test = lin_reg_model.predict(x_test) - y_test\nhplot = sns.histplot(residuals_test, kde=True);\nhplot.set_xlim(-1,1);\n</pre> residuals_test = lin_reg_model.predict(x_test) - y_test hplot = sns.histplot(residuals_test, kde=True); hplot.set_xlim(-1,1); In\u00a0[125]: Copied! <pre># scatterplot between residuals and predicted variables\ny_test_predict = pd.DataFrame(lin_reg_model.predict(x_test), columns=['y_predict'])\nsns.scatterplot(x=y_test_predict['y_predict'], y=residuals_test['Price_log']);\n</pre> # scatterplot between residuals and predicted variables y_test_predict = pd.DataFrame(lin_reg_model.predict(x_test), columns=['y_predict']) sns.scatterplot(x=y_test_predict['y_predict'], y=residuals_test['Price_log']); <p>The scatter plot is random and therefore the model does not violate the assumption of Homoscedasticity</p> In\u00a0[126]: Copied! <pre># Create data frame with coefficients\ncoef_df = pd.DataFrame(\n    np.append(lin_reg_model.coef_.flatten(), lin_reg_model.intercept_),\n    index=x_train.columns.tolist() + [\"Intercept\"],\n    columns=[\"Coefficients\"],\n)\n# Display all coefficients\npd.set_option('display.max_rows', coef_df.shape[0]+1)\ncoef_df\n</pre> # Create data frame with coefficients coef_df = pd.DataFrame(     np.append(lin_reg_model.coef_.flatten(), lin_reg_model.intercept_),     index=x_train.columns.tolist() + [\"Intercept\"],     columns=[\"Coefficients\"], ) # Display all coefficients pd.set_option('display.max_rows', coef_df.shape[0]+1) coef_df Out[126]: Coefficients Year 1.061775e-01 Mileage 1.316338e-03 Engine -4.557528e-05 Seats -1.807394e-04 Kilometers_Driven_log -7.715894e-02 Power_log 3.782705e-01 Location_Bangalore 1.767534e-01 Location_Chennai 5.678747e-02 Location_Coimbatore 1.474313e-01 Location_Delhi -8.008404e-02 Location_Hyderabad 1.564025e-01 Location_Jaipur -1.610073e-02 Location_Kochi -1.353527e-02 Location_Kolkata -2.176047e-01 Location_Mumbai -5.679099e-02 Location_Pune -2.156988e-02 Fuel_Type_Diesel 4.966604e-02 Fuel_Type_Electric 3.062401e-01 Fuel_Type_LPG -1.187979e-02 Fuel_Type_Petrol -6.045009e-02 Transmission_Manual -1.121924e-01 Owner_Type_Fourth &amp; Above -1.194041e-01 Owner_Type_Second -5.739302e-02 Owner_Type_Third -1.684767e-01 Brand_Audi 5.774404e-01 Brand_BMW 1.069879e-01 Brand_Bentley 9.889603e-01 Brand_Chevrolet -7.607063e-01 Brand_Datsun -1.033328e+00 Brand_Fiat -9.095028e-01 Brand_Force -4.784993e-02 Brand_Ford -7.306453e-01 Brand_Hindustan 3.052961e-12 Brand_Honda -5.494955e-01 Brand_Hyundai -1.140057e+00 Brand_ISUZU -3.463250e-01 Brand_Isuzu 8.891776e-13 Brand_Jaguar 8.123010e-01 Brand_Jeep -1.848095e-02 Brand_Lamborghini 1.184292e+00 Brand_Land 4.192147e-01 Brand_Mahindra -5.890545e-01 Brand_Maruti -6.364073e-01 Brand_Mercedes-Benz 6.110158e-01 Brand_Mini 3.724754e-01 Brand_Mitsubishi -6.988398e-02 Brand_Nissan -4.932404e-01 Brand_OpelCorsa -6.200596e-13 Brand_Porsche 7.997953e-01 Brand_Renault -6.041900e-01 Brand_Skoda -3.930683e-01 Brand_Smart -2.371821e-01 Brand_Tata -5.277481e-01 Brand_Toyota 1.267729e-02 Brand_Volkswagen -4.056313e-01 Brand_Volvo 2.399334e-01 Model_1.4Gsi -1.327410e-14 Model_1000 2.902123e-13 Model_3 1.064970e-01 Model_370Z -1.439404e-13 Model_5 4.035985e-01 Model_6 1.121772e+00 Model_7 8.506502e-01 Model_800 -6.486312e-01 Model_A -4.333749e-01 Model_A-Star -2.464424e-01 Model_A3 -3.709362e-01 Model_A4 -2.706989e-01 Model_A6 -1.468467e-01 Model_A7 4.505197e-01 Model_A8 2.214500e-01 Model_Abarth -5.806466e-14 Model_Accent 7.664687e-02 Model_Accord 1.786603e-01 Model_Alto -4.737990e-01 Model_Amaze -2.578199e-01 Model_Ameo -3.923683e-01 Model_Aspire 1.004053e-01 Model_Aveo -3.457447e-01 Model_Avventura 1.180748e-01 Model_B -4.471035e-01 Model_BR-V -6.853318e-02 Model_BRV 2.825778e-02 Model_Baleno -1.021670e-01 Model_Beat -3.622543e-01 Model_Beetle -2.145506e-14 Model_Bolero 1.954131e-01 Model_Bolt -6.321259e-01 Model_Boxster 4.052314e-15 Model_Brio -3.555369e-01 Model_C-Class -3.104868e-01 Model_CLA -2.939341e-01 Model_CLS-Class 3.338558e-01 Model_CR-V 4.936109e-01 Model_Camry 2.003381e-01 Model_Captiva 1.795402e-01 Model_Captur 1.181568e-01 Model_Cayenne -4.493680e-01 Model_Cayman 5.211655e-01 Model_Cedia -4.966320e-01 Model_Celerio -3.286449e-01 Model_Ciaz 1.128385e-01 Model_City 1.452770e-02 Model_Civic -8.820436e-02 Model_Classic -3.442637e-01 Model_Clubman 2.810297e-01 Model_Compass -1.848095e-02 Model_Continental 9.889603e-01 Model_Cooper 1.988535e-01 Model_Corolla -3.308798e-01 Model_Countryman -1.074077e-01 Model_Creta 8.796960e-01 Model_CrossPolo -3.654663e-01 Model_Cruze 8.068924e-02 Model_D-MAX -3.463250e-01 Model_Duster 1.221770e-01 Model_Dzire -4.513748e-02 Model_E 6.661338e-16 Model_E-Class -1.518833e-01 Model_EON 3.452707e-02 Model_EcoSport 1.588931e-01 Model_Ecosport 2.573158e-01 Model_Eeco -4.662428e-01 Model_Elantra 8.812794e-01 Model_Elite 5.396578e-01 Model_Endeavour 7.964040e-01 Model_Enjoy -1.127670e-01 Model_Ertiga 2.135297e-01 Model_Esteem -6.439154e-01 Model_Estilo -3.688273e-01 Model_Etios -7.384430e-01 Model_Evalia -4.024599e-01 Model_F 5.949628e-01 Model_Fabia -5.627410e-01 Model_Fiesta -7.866301e-02 Model_Figo -1.827555e-01 Model_Fluence -6.255634e-02 Model_Flying -1.110223e-16 Model_Fortuner 3.763759e-01 Model_Fortwo -2.371821e-01 Model_Freestyle 4.682399e-01 Model_Fusion 1.819050e-01 Model_GL-Class 4.952753e-01 Model_GLA -2.212246e-01 Model_GLC 1.174502e-01 Model_GLE 3.154723e-01 Model_GLS 4.387667e-01 Model_GO -2.105149e-01 Model_Gallardo 1.184292e+00 Model_Getz 1.771284e-01 Model_Grand 2.571492e-01 Model_Grande -1.834601e-01 Model_Hexa 3.285831e-01 Model_Ignis -3.791245e-01 Model_Ikon -2.971519e-01 Model_Indica -8.287199e-01 Model_Indigo -7.456837e-01 Model_Innova 8.701657e-02 Model_Jazz -1.622787e-01 Model_Jeep 1.181196e-01 Model_Jetta 1.227673e-01 Model_KUV -4.220742e-01 Model_KWID -6.321051e-01 Model_Koleos 3.891056e-01 Model_Lancer -1.172340e-01 Model_Land -2.775558e-16 Model_Laura -1.106812e-01 Model_Linea 6.946507e-03 Model_Lodgy 1.736719e-03 Model_Logan -5.156677e-01 Model_M-Class 1.294610e-01 Model_MU 4.163336e-17 Model_MUX 2.706169e-16 Model_Manza -6.278130e-01 Model_Micra -3.797234e-01 Model_Mobilio -1.055585e-01 Model_Montero 3.367374e-01 Model_Motors 9.714451e-17 Model_Mustang 1.576728e+00 Model_Nano -1.066021e+00 Model_New -2.606214e-01 Model_Nexon -3.983023e-02 Model_NuvoSport -3.193562e-01 Model_Octavia 1.391406e-01 Model_Omni -4.721723e-01 Model_One -4.784993e-02 Model_Optra -1.671785e-01 Model_Outlander -6.370752e-02 Model_Pajero 2.709521e-01 Model_Panamera 7.279978e-01 Model_Passat 5.778024e-02 Model_Petra -2.967823e-01 Model_Platinum 8.326673e-17 Model_Polo -3.349437e-01 Model_Prius 3.062401e-01 Model_Pulse -2.651567e-01 Model_Punto -2.652687e-01 Model_Q3 -2.577470e-01 Model_Q5 4.141793e-02 Model_Q7 2.518171e-01 Model_Qualis 1.120295e-01 Model_Quanto -3.687409e-01 Model_R-Class 6.386756e-02 Model_RS5 3.304247e-01 Model_Rapid -2.682940e-01 Model_Redi -4.869040e-01 Model_Renault -2.098534e-01 Model_Ritz -2.182836e-01 Model_Rover 4.192147e-01 Model_S 1.629848e-01 Model_S-Class 2.594994e-01 Model_S-Cross 1.441084e-01 Model_S60 -1.927748e-02 Model_S80 -2.712951e-01 Model_SL-Class 7.085461e-01 Model_SLC 2.621350e-01 Model_SLK-Class 4.748246e-01 Model_SX4 -9.882907e-02 Model_Safari 2.992587e-02 Model_Sail -1.856540e-01 Model_Santa 1.029070e+00 Model_Santro 1.564984e-01 Model_Scala -2.755480e-01 Model_Scorpio 2.628888e-01 Model_Siena -2.890131e-01 Model_Sonata 8.564400e-01 Model_Spark -4.065682e-01 Model_Ssangyong 4.359764e-01 Model_Sumo -9.638676e-02 Model_Sunny -2.104897e-01 Model_Superb 2.042283e-01 Model_Swift -4.020150e-02 Model_TT 3.280398e-01 Model_TUV -8.666424e-02 Model_Tavera 5.592310e-01 Model_Teana 6.672175e-02 Model_Terrano -1.826751e-02 Model_Thar 6.232812e-02 Model_Tiago -6.192760e-01 Model_Tigor -4.091198e-01 Model_Tiguan 7.408846e-01 Model_Tucson 8.083781e-01 Model_V40 6.752698e-02 Model_Vento -2.342850e-01 Model_Venture -4.374467e-01 Model_Verito -2.849063e-01 Model_Verna 5.399973e-01 Model_Versa -1.978553e-02 Model_Vitara 1.270174e-01 Model_WR-V -1.783798e-01 Model_WRV -4.824091e-02 Model_Wagon -2.844928e-01 Model_X-Trail 4.509783e-01 Model_X1 1.142903e-01 Model_X3 4.470527e-01 Model_X5 7.394769e-01 Model_X6 9.909875e-01 Model_XC60 5.142907e-02 Model_XC90 4.115498e-01 Model_XE 0.000000e+00 Model_XF -2.061667e-01 Model_XJ 4.235049e-01 Model_XUV300 3.627486e-01 Model_XUV500 3.739087e-01 Model_Xcent 2.875080e-01 Model_Xenon -4.504703e-01 Model_Xylo -1.931750e-01 Model_Yeti 2.052791e-01 Model_Z4 9.327412e-01 Model_Zen -3.685581e-01 Model_Zest -4.196436e-01 Model_i10 2.949555e-01 Model_i20 4.661499e-01 Model_redi-GO -3.359091e-01 Intercept -2.122822e+02 In\u00a0[127]: Copied! <pre>coef_df[coef_df['Coefficients']&gt;0].sort_values(by='Coefficients', ascending=False)\n</pre> coef_df[coef_df['Coefficients']&gt;0].sort_values(by='Coefficients', ascending=False) Out[127]: Coefficients Model_Mustang 1.576728e+00 Brand_Lamborghini 1.184292e+00 Model_Gallardo 1.184292e+00 Model_6 1.121772e+00 Model_Santa 1.029070e+00 Model_X6 9.909875e-01 Brand_Bentley 9.889603e-01 Model_Continental 9.889603e-01 Model_Z4 9.327412e-01 Model_Elantra 8.812794e-01 Model_Creta 8.796960e-01 Model_Sonata 8.564400e-01 Model_7 8.506502e-01 Brand_Jaguar 8.123010e-01 Model_Tucson 8.083781e-01 Brand_Porsche 7.997953e-01 Model_Endeavour 7.964040e-01 Model_Tiguan 7.408846e-01 Model_X5 7.394769e-01 Model_Panamera 7.279978e-01 Model_SL-Class 7.085461e-01 Brand_Mercedes-Benz 6.110158e-01 Model_F 5.949628e-01 Brand_Audi 5.774404e-01 Model_Tavera 5.592310e-01 Model_Verna 5.399973e-01 Model_Elite 5.396578e-01 Model_Cayman 5.211655e-01 Model_GL-Class 4.952753e-01 Model_CR-V 4.936109e-01 Model_SLK-Class 4.748246e-01 Model_Freestyle 4.682399e-01 Model_i20 4.661499e-01 Model_X-Trail 4.509783e-01 Model_A7 4.505197e-01 Model_X3 4.470527e-01 Model_GLS 4.387667e-01 Model_Ssangyong 4.359764e-01 Model_XJ 4.235049e-01 Brand_Land 4.192147e-01 Model_Rover 4.192147e-01 Model_XC90 4.115498e-01 Model_5 4.035985e-01 Model_Koleos 3.891056e-01 Power_log 3.782705e-01 Model_Fortuner 3.763759e-01 Model_XUV500 3.739087e-01 Brand_Mini 3.724754e-01 Model_XUV300 3.627486e-01 Model_Montero 3.367374e-01 Model_CLS-Class 3.338558e-01 Model_RS5 3.304247e-01 Model_Hexa 3.285831e-01 Model_TT 3.280398e-01 Model_GLE 3.154723e-01 Fuel_Type_Electric 3.062401e-01 Model_Prius 3.062401e-01 Model_i10 2.949555e-01 Model_Xcent 2.875080e-01 Model_Clubman 2.810297e-01 Model_Pajero 2.709521e-01 Model_Scorpio 2.628888e-01 Model_SLC 2.621350e-01 Model_S-Class 2.594994e-01 Model_Ecosport 2.573158e-01 Model_Grand 2.571492e-01 Model_Q7 2.518171e-01 Brand_Volvo 2.399334e-01 Model_A8 2.214500e-01 Model_Ertiga 2.135297e-01 Model_Yeti 2.052791e-01 Model_Superb 2.042283e-01 Model_Camry 2.003381e-01 Model_Cooper 1.988535e-01 Model_Bolero 1.954131e-01 Model_Fusion 1.819050e-01 Model_Captiva 1.795402e-01 Model_Accord 1.786603e-01 Model_Getz 1.771284e-01 Location_Bangalore 1.767534e-01 Model_S 1.629848e-01 Model_EcoSport 1.588931e-01 Model_Santro 1.564984e-01 Location_Hyderabad 1.564025e-01 Location_Coimbatore 1.474313e-01 Model_S-Cross 1.441084e-01 Model_Octavia 1.391406e-01 Model_M-Class 1.294610e-01 Model_Vitara 1.270174e-01 Model_Jetta 1.227673e-01 Model_Duster 1.221770e-01 Model_Captur 1.181568e-01 Model_Jeep 1.181196e-01 Model_Avventura 1.180748e-01 Model_GLC 1.174502e-01 Model_X1 1.142903e-01 Model_Ciaz 1.128385e-01 Model_Qualis 1.120295e-01 Brand_BMW 1.069879e-01 Model_3 1.064970e-01 Year 1.061775e-01 Model_Aspire 1.004053e-01 Model_Innova 8.701657e-02 Model_Cruze 8.068924e-02 Model_Accent 7.664687e-02 Model_V40 6.752698e-02 Model_Teana 6.672175e-02 Model_R-Class 6.386756e-02 Model_Thar 6.232812e-02 Model_Passat 5.778024e-02 Location_Chennai 5.678747e-02 Model_XC60 5.142907e-02 Fuel_Type_Diesel 4.966604e-02 Model_Q5 4.141793e-02 Model_EON 3.452707e-02 Model_Safari 2.992587e-02 Model_BRV 2.825778e-02 Model_City 1.452770e-02 Brand_Toyota 1.267729e-02 Model_Linea 6.946507e-03 Model_Lodgy 1.736719e-03 Mileage 1.316338e-03 Brand_Hindustan 3.052961e-12 Brand_Isuzu 8.891776e-13 Model_1000 2.902123e-13 Model_Boxster 4.052314e-15 Model_E 6.661338e-16 Model_MUX 2.706169e-16 Model_Motors 9.714451e-17 Model_Platinum 8.326673e-17 Model_MU 4.163336e-17 In\u00a0[128]: Copied! <pre>coef_df[coef_df['Coefficients']&lt;0].sort_values(by='Coefficients')\n</pre> coef_df[coef_df['Coefficients']&lt;0].sort_values(by='Coefficients') Out[128]: Coefficients Intercept -2.122822e+02 Brand_Hyundai -1.140057e+00 Model_Nano -1.066021e+00 Brand_Datsun -1.033328e+00 Brand_Fiat -9.095028e-01 Model_Indica -8.287199e-01 Brand_Chevrolet -7.607063e-01 Model_Indigo -7.456837e-01 Model_Etios -7.384430e-01 Brand_Ford -7.306453e-01 Model_800 -6.486312e-01 Model_Esteem -6.439154e-01 Brand_Maruti -6.364073e-01 Model_Bolt -6.321259e-01 Model_KWID -6.321051e-01 Model_Manza -6.278130e-01 Model_Tiago -6.192760e-01 Brand_Renault -6.041900e-01 Brand_Mahindra -5.890545e-01 Model_Fabia -5.627410e-01 Brand_Honda -5.494955e-01 Brand_Tata -5.277481e-01 Model_Logan -5.156677e-01 Model_Cedia -4.966320e-01 Brand_Nissan -4.932404e-01 Model_Redi -4.869040e-01 Model_Alto -4.737990e-01 Model_Omni -4.721723e-01 Model_Eeco -4.662428e-01 Model_Xenon -4.504703e-01 Model_Cayenne -4.493680e-01 Model_B -4.471035e-01 Model_Venture -4.374467e-01 Model_A -4.333749e-01 Model_KUV -4.220742e-01 Model_Zest -4.196436e-01 Model_Tigor -4.091198e-01 Model_Spark -4.065682e-01 Brand_Volkswagen -4.056313e-01 Model_Evalia -4.024599e-01 Brand_Skoda -3.930683e-01 Model_Ameo -3.923683e-01 Model_Micra -3.797234e-01 Model_Ignis -3.791245e-01 Model_A3 -3.709362e-01 Model_Estilo -3.688273e-01 Model_Quanto -3.687409e-01 Model_Zen -3.685581e-01 Model_CrossPolo -3.654663e-01 Model_Beat -3.622543e-01 Model_Brio -3.555369e-01 Model_D-MAX -3.463250e-01 Brand_ISUZU -3.463250e-01 Model_Aveo -3.457447e-01 Model_Classic -3.442637e-01 Model_redi-GO -3.359091e-01 Model_Polo -3.349437e-01 Model_Corolla -3.308798e-01 Model_Celerio -3.286449e-01 Model_NuvoSport -3.193562e-01 Model_C-Class -3.104868e-01 Model_Ikon -2.971519e-01 Model_Petra -2.967823e-01 Model_CLA -2.939341e-01 Model_Siena -2.890131e-01 Model_Verito -2.849063e-01 Model_Wagon -2.844928e-01 Model_Scala -2.755480e-01 Model_S80 -2.712951e-01 Model_A4 -2.706989e-01 Model_Rapid -2.682940e-01 Model_Punto -2.652687e-01 Model_Pulse -2.651567e-01 Model_New -2.606214e-01 Model_Amaze -2.578199e-01 Model_Q3 -2.577470e-01 Model_A-Star -2.464424e-01 Brand_Smart -2.371821e-01 Model_Fortwo -2.371821e-01 Model_Vento -2.342850e-01 Model_GLA -2.212246e-01 Model_Ritz -2.182836e-01 Location_Kolkata -2.176047e-01 Model_GO -2.105149e-01 Model_Sunny -2.104897e-01 Model_Renault -2.098534e-01 Model_XF -2.061667e-01 Model_Xylo -1.931750e-01 Model_Sail -1.856540e-01 Model_Grande -1.834601e-01 Model_Figo -1.827555e-01 Model_WR-V -1.783798e-01 Owner_Type_Third -1.684767e-01 Model_Optra -1.671785e-01 Model_Jazz -1.622787e-01 Model_E-Class -1.518833e-01 Model_A6 -1.468467e-01 Owner_Type_Fourth &amp; Above -1.194041e-01 Model_Lancer -1.172340e-01 Model_Enjoy -1.127670e-01 Transmission_Manual -1.121924e-01 Model_Laura -1.106812e-01 Model_Countryman -1.074077e-01 Model_Mobilio -1.055585e-01 Model_Baleno -1.021670e-01 Model_SX4 -9.882907e-02 Model_Sumo -9.638676e-02 Model_Civic -8.820436e-02 Model_TUV -8.666424e-02 Location_Delhi -8.008404e-02 Model_Fiesta -7.866301e-02 Kilometers_Driven_log -7.715894e-02 Brand_Mitsubishi -6.988398e-02 Model_BR-V -6.853318e-02 Model_Outlander -6.370752e-02 Model_Fluence -6.255634e-02 Fuel_Type_Petrol -6.045009e-02 Owner_Type_Second -5.739302e-02 Location_Mumbai -5.679099e-02 Model_WRV -4.824091e-02 Brand_Force -4.784993e-02 Model_One -4.784993e-02 Model_Dzire -4.513748e-02 Model_Swift -4.020150e-02 Model_Nexon -3.983023e-02 Location_Pune -2.156988e-02 Model_Versa -1.978553e-02 Model_S60 -1.927748e-02 Model_Compass -1.848095e-02 Brand_Jeep -1.848095e-02 Model_Terrano -1.826751e-02 Location_Jaipur -1.610073e-02 Location_Kochi -1.353527e-02 Fuel_Type_LPG -1.187979e-02 Seats -1.807394e-04 Engine -4.557528e-05 Brand_OpelCorsa -6.200596e-13 Model_370Z -1.439404e-13 Model_Abarth -5.806466e-14 Model_Beetle -2.145506e-14 Model_1.4Gsi -1.327410e-14 Model_Land -2.775558e-16 Model_Flying -1.110223e-16 <ul> <li>There are 274 independent variables, and it is difficult to identify all key variables with strong relationship with Price. Therefore, we will select a subset of important features with forward feature selection using <code>SequentialFeatureSelector</code></li> </ul> In\u00a0[130]: Copied! <pre>from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n</pre> from mlxtend.feature_selection import SequentialFeatureSelector as SFS In\u00a0[133]: Copied! <pre>reg = LinearRegression()\n\n# Build step forward feature selection\nsfs = SFS(\n    reg,\n    k_features=x_train.shape[1],\n    forward=True,  # k_features denotes \"Number of features to select\"\n    floating=False,\n    scoring=\"r2\",\n    n_jobs=-1,\n    verbose=2,\n    cv=5,\n)\n\n# Perform SFFS\nsfs = sfs.fit(x_train, y_train)\n</pre> reg = LinearRegression()  # Build step forward feature selection sfs = SFS(     reg,     k_features=x_train.shape[1],     forward=True,  # k_features denotes \"Number of features to select\"     floating=False,     scoring=\"r2\",     n_jobs=-1,     verbose=2,     cv=5, )  # Perform SFFS sfs = sfs.fit(x_train, y_train) <pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.8s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.5s\n[Parallel(n_jobs=-1)]: Done 274 out of 274 | elapsed:   12.2s finished\n\n[2024-10-30 15:22:54] Features: 1/274 -- score: 0.6008605735778773[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 228 tasks      | elapsed:    4.8s\n[Parallel(n_jobs=-1)]: Done 242 out of 273 | elapsed:    5.0s remaining:    0.6s\n[Parallel(n_jobs=-1)]: Done 273 out of 273 | elapsed:    5.9s finished\n\n[2024-10-30 15:23:00] Features: 2/274 -- score: 0.8258189338417044[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 272 out of 272 | elapsed:    5.9s finished\n\n[2024-10-30 15:23:06] Features: 3/274 -- score: 0.843163289844291[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 240 out of 271 | elapsed:    5.0s remaining:    0.6s\n[Parallel(n_jobs=-1)]: Done 271 out of 271 | elapsed:    5.8s finished\n\n[2024-10-30 15:23:12] Features: 4/274 -- score: 0.8607242290453196[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed:    5.7s finished\n\n[2024-10-30 15:23:17] Features: 5/274 -- score: 0.8673527352506463[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 238 out of 269 | elapsed:    5.0s remaining:    0.6s\n[Parallel(n_jobs=-1)]: Done 269 out of 269 | elapsed:    5.8s finished\n\n[2024-10-30 15:23:23] Features: 6/274 -- score: 0.8733406729210295[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 268 out of 268 | elapsed:    5.8s finished\n\n[2024-10-30 15:23:29] Features: 7/274 -- score: 0.8779019854352988[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 236 out of 267 | elapsed:    5.3s remaining:    0.6s\n[Parallel(n_jobs=-1)]: Done 267 out of 267 | elapsed:    6.0s finished\n\n[2024-10-30 15:23:35] Features: 8/274 -- score: 0.8821076936222507[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 266 out of 266 | elapsed:    5.9s finished\n\n[2024-10-30 15:23:41] Features: 9/274 -- score: 0.8865803385052942[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 234 out of 265 | elapsed:    5.4s remaining:    0.6s\n[Parallel(n_jobs=-1)]: Done 265 out of 265 | elapsed:    6.1s finished\n\n[2024-10-30 15:23:48] Features: 10/274 -- score: 0.892136862625631[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 264 out of 264 | elapsed:    5.9s finished\n\n[2024-10-30 15:23:53] Features: 11/274 -- score: 0.8997588358866588[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 232 out of 263 | elapsed:    5.6s remaining:    0.7s\n[Parallel(n_jobs=-1)]: Done 263 out of 263 | elapsed:    6.3s finished\n\n[2024-10-30 15:24:00] Features: 12/274 -- score: 0.9045279013258785[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 262 out of 262 | elapsed:    5.8s finished\n\n[2024-10-30 15:24:06] Features: 13/274 -- score: 0.9077032400393492[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 230 out of 261 | elapsed:    5.4s remaining:    0.6s\n[Parallel(n_jobs=-1)]: Done 261 out of 261 | elapsed:    6.1s finished\n\n[2024-10-30 15:24:12] Features: 14/274 -- score: 0.9113184768449554[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 260 out of 260 | elapsed:    5.9s finished\n\n[2024-10-30 15:24:18] Features: 15/274 -- score: 0.9144300582982374[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 228 out of 259 | elapsed:    6.2s remaining:    0.8s\n[Parallel(n_jobs=-1)]: Done 259 out of 259 | elapsed:    6.9s finished\n\n[2024-10-30 15:24:25] Features: 16/274 -- score: 0.9165568997201585[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    6.6s\n[Parallel(n_jobs=-1)]: Done 258 out of 258 | elapsed:   10.4s finished\n\n[2024-10-30 15:24:36] Features: 17/274 -- score: 0.9187094547000717[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 226 out of 257 | elapsed:    5.5s remaining:    0.7s\n[Parallel(n_jobs=-1)]: Done 257 out of 257 | elapsed:    6.1s finished\n\n[2024-10-30 15:24:42] Features: 18/274 -- score: 0.9210414399636491[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 256 out of 256 | elapsed:    7.3s finished\n\n[2024-10-30 15:24:49] Features: 19/274 -- score: 0.9231785068400782[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 255 out of 255 | elapsed:    7.5s finished\n\n[2024-10-30 15:24:57] Features: 20/274 -- score: 0.9244145086624963[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 223 out of 254 | elapsed:    5.9s remaining:    0.7s\n[Parallel(n_jobs=-1)]: Done 254 out of 254 | elapsed:    7.0s finished\n\n[2024-10-30 15:25:04] Features: 21/274 -- score: 0.9254483170392269[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 222 out of 253 | elapsed:    5.8s remaining:    0.7s\n[Parallel(n_jobs=-1)]: Done 253 out of 253 | elapsed:    7.1s finished\n\n[2024-10-30 15:25:11] Features: 22/274 -- score: 0.9264841941808273[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 252 out of 252 | elapsed:    7.6s finished\n\n[2024-10-30 15:25:19] Features: 23/274 -- score: 0.9274758369192984[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 220 out of 251 | elapsed:    6.0s remaining:    0.8s\n[Parallel(n_jobs=-1)]: Done 251 out of 251 | elapsed:    7.1s finished\n\n[2024-10-30 15:25:26] Features: 24/274 -- score: 0.92844212796468[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:    6.7s finished\n\n[2024-10-30 15:25:33] Features: 25/274 -- score: 0.9292203842611386[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 218 out of 249 | elapsed:    5.9s remaining:    0.8s\n[Parallel(n_jobs=-1)]: Done 249 out of 249 | elapsed:    7.0s finished\n\n[2024-10-30 15:25:40] Features: 26/274 -- score: 0.9298510047495391[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 248 out of 248 | elapsed:    7.3s finished\n\n[2024-10-30 15:25:47] Features: 27/274 -- score: 0.9304019840531833[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 216 out of 247 | elapsed:    6.1s remaining:    0.8s\n[Parallel(n_jobs=-1)]: Done 247 out of 247 | elapsed:    7.1s finished\n\n[2024-10-30 15:25:55] Features: 28/274 -- score: 0.9309305347004898[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 246 out of 246 | elapsed:    7.0s finished\n\n[2024-10-30 15:26:02] Features: 29/274 -- score: 0.931458673523718[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    5.6s\n[Parallel(n_jobs=-1)]: Done 245 out of 245 | elapsed:   10.5s finished\n\n[2024-10-30 15:26:12] Features: 30/274 -- score: 0.9319344324038337[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.7s\n[Parallel(n_jobs=-1)]: Done 244 out of 244 | elapsed:   11.4s finished\n\n[2024-10-30 15:26:24] Features: 31/274 -- score: 0.9324067034952849[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    5.6s\n[Parallel(n_jobs=-1)]: Done 243 out of 243 | elapsed:   10.8s finished\n\n[2024-10-30 15:26:35] Features: 32/274 -- score: 0.9328337130107986[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 242 out of 242 | elapsed:    7.5s finished\n\n[2024-10-30 15:26:42] Features: 33/274 -- score: 0.9333355895459066[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 210 out of 241 | elapsed:    6.4s remaining:    0.9s\n[Parallel(n_jobs=-1)]: Done 241 out of 241 | elapsed:    7.5s finished\n\n[2024-10-30 15:26:50] Features: 34/274 -- score: 0.9338466167427188[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:    7.3s finished\n\n[2024-10-30 15:26:57] Features: 35/274 -- score: 0.93427063728439[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    5.8s\n[Parallel(n_jobs=-1)]: Done 239 out of 239 | elapsed:   10.7s finished\n\n[2024-10-30 15:27:08] Features: 36/274 -- score: 0.9346969157079771[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.4s\n[Parallel(n_jobs=-1)]: Done 238 out of 238 | elapsed:   11.5s finished\n\n[2024-10-30 15:27:20] Features: 37/274 -- score: 0.9351182568635021[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.1s\n[Parallel(n_jobs=-1)]: Done 237 out of 237 | elapsed:   10.7s finished\n\n[2024-10-30 15:27:31] Features: 38/274 -- score: 0.9355593302089608[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.3s\n[Parallel(n_jobs=-1)]: Done 236 out of 236 | elapsed:   11.5s finished\n\n[2024-10-30 15:27:42] Features: 39/274 -- score: 0.9359783078777568[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    5.5s\n[Parallel(n_jobs=-1)]: Done 235 out of 235 | elapsed:    9.7s finished\n\n[2024-10-30 15:27:52] Features: 40/274 -- score: 0.9363600521469563[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 234 out of 234 | elapsed:    6.7s finished\n\n[2024-10-30 15:27:59] Features: 41/274 -- score: 0.9367483734642255[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 202 out of 233 | elapsed:    5.9s remaining:    0.8s\n[Parallel(n_jobs=-1)]: Done 233 out of 233 | elapsed:    6.7s finished\n\n[2024-10-30 15:28:06] Features: 42/274 -- score: 0.9370884676559399[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    5.5s\n[Parallel(n_jobs=-1)]: Done 232 out of 232 | elapsed:    9.7s finished\n\n[2024-10-30 15:28:15] Features: 43/274 -- score: 0.9374125821578387[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=-1)]: Done 200 out of 231 | elapsed:    5.9s remaining:    0.8s\n[Parallel(n_jobs=-1)]: Done 231 out of 231 | elapsed:    6.7s finished\n\n[2024-10-30 15:28:22] Features: 44/274 -- score: 0.9377590488212355[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    5.6s\n[Parallel(n_jobs=-1)]: Done 230 out of 230 | elapsed:    9.7s finished\n\n[2024-10-30 15:28:32] Features: 45/274 -- score: 0.9380620809895246[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 198 out of 229 | elapsed:    6.1s remaining:    0.9s\n[Parallel(n_jobs=-1)]: Done 229 out of 229 | elapsed:    6.8s finished\n\n[2024-10-30 15:28:39] Features: 46/274 -- score: 0.9383606770154213[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    5.7s\n[Parallel(n_jobs=-1)]: Done 228 out of 228 | elapsed:    9.8s finished\n\n[2024-10-30 15:28:49] Features: 47/274 -- score: 0.9386434676769945[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 196 out of 227 | elapsed:    6.2s remaining:    0.9s\n[Parallel(n_jobs=-1)]: Done 227 out of 227 | elapsed:    6.8s finished\n\n[2024-10-30 15:28:56] Features: 48/274 -- score: 0.938917220224309[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    5.7s\n[Parallel(n_jobs=-1)]: Done 226 out of 226 | elapsed:    9.7s finished\n\n[2024-10-30 15:29:06] Features: 49/274 -- score: 0.9391992842156454[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    5.9s\n[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:   10.6s finished\n\n[2024-10-30 15:29:16] Features: 50/274 -- score: 0.9394813780691168[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.8s\n[Parallel(n_jobs=-1)]: Done 224 out of 224 | elapsed:   11.7s finished\n\n[2024-10-30 15:29:28] Features: 51/274 -- score: 0.9397591483128049[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.6s\n[Parallel(n_jobs=-1)]: Done 223 out of 223 | elapsed:   11.0s finished\n\n[2024-10-30 15:29:39] Features: 52/274 -- score: 0.9400251925243157[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.7s\n[Parallel(n_jobs=-1)]: Done 222 out of 222 | elapsed:   11.1s finished\n\n[2024-10-30 15:29:50] Features: 53/274 -- score: 0.9403015910400038[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.1s\n[Parallel(n_jobs=-1)]: Done 221 out of 221 | elapsed:   10.3s finished\n\n[2024-10-30 15:30:01] Features: 54/274 -- score: 0.9405794121248693[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.4s\n[Parallel(n_jobs=-1)]: Done 220 out of 220 | elapsed:   10.9s finished\n\n[2024-10-30 15:30:12] Features: 55/274 -- score: 0.9408523283296407[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    7.3s\n[Parallel(n_jobs=-1)]: Done 219 out of 219 | elapsed:    9.1s finished\n\n[2024-10-30 15:30:21] Features: 56/274 -- score: 0.9411047750951868[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 218 out of 218 | elapsed:    8.3s finished\n\n[2024-10-30 15:30:30] Features: 57/274 -- score: 0.9413510911204437[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.1s\n[Parallel(n_jobs=-1)]: Done 217 out of 217 | elapsed:   10.3s finished\n\n[2024-10-30 15:30:40] Features: 58/274 -- score: 0.9415890736708146[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.3s\n[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed:   10.5s finished\n\n[2024-10-30 15:30:51] Features: 59/274 -- score: 0.9418027576675003[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.7s\n[Parallel(n_jobs=-1)]: Done 215 out of 215 | elapsed:   10.9s finished\n\n[2024-10-30 15:31:02] Features: 60/274 -- score: 0.9419907820263911[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.7s\n[Parallel(n_jobs=-1)]: Done 214 out of 214 | elapsed:   10.8s finished\n\n[2024-10-30 15:31:12] Features: 61/274 -- score: 0.942173275773842[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.7s\n[Parallel(n_jobs=-1)]: Done 213 out of 213 | elapsed:   10.9s finished\n\n[2024-10-30 15:31:23] Features: 62/274 -- score: 0.942356160410377[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    8.1s\n[Parallel(n_jobs=-1)]: Done 212 out of 212 | elapsed:   12.8s finished\n\n[2024-10-30 15:31:36] Features: 63/274 -- score: 0.9425128317572566[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.9s\n[Parallel(n_jobs=-1)]: Done 211 out of 211 | elapsed:   11.4s finished\n\n[2024-10-30 15:31:48] Features: 64/274 -- score: 0.94266687641947[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.8s\n[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed:   11.0s finished\n\n[2024-10-30 15:31:59] Features: 65/274 -- score: 0.9428146211526883[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.5s\n[Parallel(n_jobs=-1)]: Done 209 out of 209 | elapsed:   11.6s finished\n\n[2024-10-30 15:32:11] Features: 66/274 -- score: 0.9429493707252343[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.1s\n[Parallel(n_jobs=-1)]: Done 208 out of 208 | elapsed:   11.0s finished\n\n[2024-10-30 15:32:22] Features: 67/274 -- score: 0.9430837362751194[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.9s\n[Parallel(n_jobs=-1)]: Done 207 out of 207 | elapsed:   10.6s finished\n\n[2024-10-30 15:32:33] Features: 68/274 -- score: 0.9432136432224116[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.8s\n[Parallel(n_jobs=-1)]: Done 206 out of 206 | elapsed:   10.5s finished\n\n[2024-10-30 15:32:43] Features: 69/274 -- score: 0.943344306484075[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.5s\n[Parallel(n_jobs=-1)]: Done 205 out of 205 | elapsed:   10.2s finished\n\n[2024-10-30 15:32:53] Features: 70/274 -- score: 0.9434634052333731[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.1s\n[Parallel(n_jobs=-1)]: Done 204 out of 204 | elapsed:   11.2s finished\n\n[2024-10-30 15:33:05] Features: 71/274 -- score: 0.943577067507883[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.3s\n[Parallel(n_jobs=-1)]: Done 203 out of 203 | elapsed:   11.1s finished\n\n[2024-10-30 15:33:16] Features: 72/274 -- score: 0.9436891888008686[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.7s\n[Parallel(n_jobs=-1)]: Done 202 out of 202 | elapsed:   10.3s finished\n\n[2024-10-30 15:33:26] Features: 73/274 -- score: 0.9437981539239481[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.6s\n[Parallel(n_jobs=-1)]: Done 201 out of 201 | elapsed:   10.1s finished\n\n[2024-10-30 15:33:37] Features: 74/274 -- score: 0.9439100954531178[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.8s\n[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   10.2s finished\n\n[2024-10-30 15:33:47] Features: 75/274 -- score: 0.9440163805264301[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.7s\n[Parallel(n_jobs=-1)]: Done 199 out of 199 | elapsed:   10.6s finished\n\n[2024-10-30 15:33:58] Features: 76/274 -- score: 0.9441303003262262[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.9s\n[Parallel(n_jobs=-1)]: Done 198 out of 198 | elapsed:   10.3s finished\n\n[2024-10-30 15:34:08] Features: 77/274 -- score: 0.9442572101220648[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.9s\n[Parallel(n_jobs=-1)]: Done 197 out of 197 | elapsed:   10.3s finished\n\n[2024-10-30 15:34:18] Features: 78/274 -- score: 0.9443594920438331[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.9s\n[Parallel(n_jobs=-1)]: Done 196 out of 196 | elapsed:   10.3s finished\n\n[2024-10-30 15:34:29] Features: 79/274 -- score: 0.9444757561496895[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.9s\n[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:   10.3s finished\n\n[2024-10-30 15:34:39] Features: 80/274 -- score: 0.9445896136937307[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.1s\n[Parallel(n_jobs=-1)]: Done 194 out of 194 | elapsed:   10.4s finished\n\n[2024-10-30 15:34:50] Features: 81/274 -- score: 0.9447056871041397[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.2s\n[Parallel(n_jobs=-1)]: Done 193 out of 193 | elapsed:   10.4s finished\n\n[2024-10-30 15:35:00] Features: 82/274 -- score: 0.9448220672453754[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.1s\n[Parallel(n_jobs=-1)]: Done 192 out of 192 | elapsed:   10.4s finished\n\n[2024-10-30 15:35:11] Features: 83/274 -- score: 0.94492005052404[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.3s\n[Parallel(n_jobs=-1)]: Done 191 out of 191 | elapsed:   10.5s finished\n\n[2024-10-30 15:35:21] Features: 84/274 -- score: 0.9450094008269196[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.2s\n[Parallel(n_jobs=-1)]: Done 190 out of 190 | elapsed:   10.3s finished\n\n[2024-10-30 15:35:32] Features: 85/274 -- score: 0.9450977522742992[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.8s\n[Parallel(n_jobs=-1)]: Done 189 out of 189 | elapsed:   10.9s finished\n\n[2024-10-30 15:35:43] Features: 86/274 -- score: 0.945183112223553[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.3s\n[Parallel(n_jobs=-1)]: Done 188 out of 188 | elapsed:   10.4s finished\n\n[2024-10-30 15:35:53] Features: 87/274 -- score: 0.9452683784764198[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.2s\n[Parallel(n_jobs=-1)]: Done 187 out of 187 | elapsed:   10.1s finished\n\n[2024-10-30 15:36:03] Features: 88/274 -- score: 0.9453481246814794[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.5s\n[Parallel(n_jobs=-1)]: Done 186 out of 186 | elapsed:   10.5s finished\n\n[2024-10-30 15:36:14] Features: 89/274 -- score: 0.9454213386716412[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.5s\n[Parallel(n_jobs=-1)]: Done 185 out of 185 | elapsed:   10.4s finished\n\n[2024-10-30 15:36:24] Features: 90/274 -- score: 0.9454975389541538[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.4s\n[Parallel(n_jobs=-1)]: Done 184 out of 184 | elapsed:   10.3s finished\n\n[2024-10-30 15:36:35] Features: 91/274 -- score: 0.9455714413525443[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.6s\n[Parallel(n_jobs=-1)]: Done 183 out of 183 | elapsed:   10.4s finished\n\n[2024-10-30 15:36:45] Features: 92/274 -- score: 0.9456308638597681[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.6s\n[Parallel(n_jobs=-1)]: Done 182 out of 182 | elapsed:   10.7s finished\n\n[2024-10-30 15:36:56] Features: 93/274 -- score: 0.9456890609297093[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.6s\n[Parallel(n_jobs=-1)]: Done 181 out of 181 | elapsed:   10.3s finished\n\n[2024-10-30 15:37:07] Features: 94/274 -- score: 0.9457530047073327[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.6s\n[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:   10.4s finished\n\n[2024-10-30 15:37:17] Features: 95/274 -- score: 0.9458156688654359[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.8s\n[Parallel(n_jobs=-1)]: Done 179 out of 179 | elapsed:   10.5s finished\n\n[2024-10-30 15:37:28] Features: 96/274 -- score: 0.9458783878178758[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.6s\n[Parallel(n_jobs=-1)]: Done 178 out of 178 | elapsed:   10.3s finished\n\n[2024-10-30 15:37:38] Features: 97/274 -- score: 0.9459372840659993[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    8.3s\n[Parallel(n_jobs=-1)]: Done 177 out of 177 | elapsed:   11.1s finished\n\n[2024-10-30 15:37:49] Features: 98/274 -- score: 0.9459940673854778[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.5s\n[Parallel(n_jobs=-1)]: Done 176 out of 176 | elapsed:    9.9s finished\n\n[2024-10-30 15:37:59] Features: 99/274 -- score: 0.9460492720394387[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.4s\n[Parallel(n_jobs=-1)]: Done 175 out of 175 | elapsed:    9.8s finished\n\n[2024-10-30 15:38:09] Features: 100/274 -- score: 0.9461033266642728[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.5s\n[Parallel(n_jobs=-1)]: Done 174 out of 174 | elapsed:    9.8s finished\n\n[2024-10-30 15:38:19] Features: 101/274 -- score: 0.9461515907857343[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.8s\n[Parallel(n_jobs=-1)]: Done 173 out of 173 | elapsed:   10.0s finished\n\n[2024-10-30 15:38:29] Features: 102/274 -- score: 0.9462131431251473[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.5s\n[Parallel(n_jobs=-1)]: Done 172 out of 172 | elapsed:    9.8s finished\n\n[2024-10-30 15:38:39] Features: 103/274 -- score: 0.9462549214668187[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.6s\n[Parallel(n_jobs=-1)]: Done 171 out of 171 | elapsed:    9.8s finished\n\n[2024-10-30 15:38:49] Features: 104/274 -- score: 0.9462972708555017[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.9s\n[Parallel(n_jobs=-1)]: Done 170 out of 170 | elapsed:   10.0s finished\n\n[2024-10-30 15:38:59] Features: 105/274 -- score: 0.9463400739021506[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.8s\n[Parallel(n_jobs=-1)]: Done 169 out of 169 | elapsed:    9.9s finished\n\n[2024-10-30 15:39:09] Features: 106/274 -- score: 0.9463834060610402[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.8s\n[Parallel(n_jobs=-1)]: Done 168 out of 168 | elapsed:    9.9s finished\n\n[2024-10-30 15:39:19] Features: 107/274 -- score: 0.9464307667944067[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.7s\n[Parallel(n_jobs=-1)]: Done 167 out of 167 | elapsed:    9.7s finished\n\n[2024-10-30 15:39:29] Features: 108/274 -- score: 0.9464850186244597[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.8s\n[Parallel(n_jobs=-1)]: Done 166 out of 166 | elapsed:    9.8s finished\n\n[2024-10-30 15:39:39] Features: 109/274 -- score: 0.9465277954018362[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    8.0s\n[Parallel(n_jobs=-1)]: Done 165 out of 165 | elapsed:    9.9s finished\n\n[2024-10-30 15:39:49] Features: 110/274 -- score: 0.9465667535032496[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    8.2s\n[Parallel(n_jobs=-1)]: Done 164 out of 164 | elapsed:   10.2s finished\n\n[2024-10-30 15:39:59] Features: 111/274 -- score: 0.9466030402172381[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.9s\n[Parallel(n_jobs=-1)]: Done 163 out of 163 | elapsed:    9.8s finished\n\n[2024-10-30 15:40:09] Features: 112/274 -- score: 0.9466365074034908[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    8.2s\n[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed:   10.1s finished\n\n[2024-10-30 15:40:19] Features: 113/274 -- score: 0.9466699567782252[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    8.3s\n[Parallel(n_jobs=-1)]: Done 161 out of 161 | elapsed:   10.0s finished\n\n[2024-10-30 15:40:29] Features: 114/274 -- score: 0.9467020565119888[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:    9.9s finished\n\n[2024-10-30 15:40:39] Features: 115/274 -- score: 0.9467339018748081[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 159 out of 159 | elapsed:    9.6s finished\n\n[2024-10-30 15:40:49] Features: 116/274 -- score: 0.9467653933027028[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 158 out of 158 | elapsed:   10.0s finished\n\n[2024-10-30 15:40:59] Features: 117/274 -- score: 0.9467921828397865[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:    9.8s finished\n\n[2024-10-30 15:41:09] Features: 118/274 -- score: 0.9468260457095262[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=-1)]: Done 156 out of 156 | elapsed:   10.0s finished\n\n[2024-10-30 15:41:19] Features: 119/274 -- score: 0.94685019984123[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=-1)]: Done 155 out of 155 | elapsed:    9.6s finished\n\n[2024-10-30 15:41:29] Features: 120/274 -- score: 0.9468726221227485[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 154 out of 154 | elapsed:    9.8s finished\n\n[2024-10-30 15:41:39] Features: 121/274 -- score: 0.9468937940854705[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=-1)]: Done 153 out of 153 | elapsed:    9.8s finished\n\n[2024-10-30 15:41:49] Features: 122/274 -- score: 0.946912792717584[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=-1)]: Done 152 out of 152 | elapsed:   10.0s finished\n\n[2024-10-30 15:41:59] Features: 123/274 -- score: 0.9469287907503101[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=-1)]: Done 151 out of 151 | elapsed:    9.7s finished\n\n[2024-10-30 15:42:09] Features: 124/274 -- score: 0.9469436039366472[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:   11.1s finished\n\n[2024-10-30 15:42:20] Features: 125/274 -- score: 0.9469570319894081[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 149 out of 149 | elapsed:   11.1s finished\n\n[2024-10-30 15:42:31] Features: 126/274 -- score: 0.9469679475395798[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=-1)]: Done 148 out of 148 | elapsed:   10.8s finished\n\n[2024-10-30 15:42:42] Features: 127/274 -- score: 0.946978064208601[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 147 out of 147 | elapsed:   10.7s finished\n\n[2024-10-30 15:42:53] Features: 128/274 -- score: 0.9469880479693635[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 146 out of 146 | elapsed:   10.9s finished\n\n[2024-10-30 15:43:04] Features: 129/274 -- score: 0.9469982149421234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 145 out of 145 | elapsed:   10.6s finished\n\n[2024-10-30 15:43:15] Features: 130/274 -- score: 0.9470063220552042[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed:   10.5s finished\n\n[2024-10-30 15:43:25] Features: 131/274 -- score: 0.9470185501560527[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done 143 out of 143 | elapsed:   10.7s finished\n\n[2024-10-30 15:43:36] Features: 132/274 -- score: 0.9470565490628251[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 142 out of 142 | elapsed:   10.4s finished\n\n[2024-10-30 15:43:46] Features: 133/274 -- score: 0.9470944712657312[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=-1)]: Done 141 out of 141 | elapsed:   10.0s finished\n\n[2024-10-30 15:43:57] Features: 134/274 -- score: 0.947173384136591[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:   10.1s finished\n\n[2024-10-30 15:44:07] Features: 135/274 -- score: 0.9472745279064239[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=-1)]: Done 139 out of 139 | elapsed:    9.8s finished\n\n[2024-10-30 15:44:17] Features: 136/274 -- score: 0.9473021761095183[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=-1)]: Done 138 out of 138 | elapsed:    9.8s finished\n\n[2024-10-30 15:44:27] Features: 137/274 -- score: 0.947330640994581[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=-1)]: Done 137 out of 137 | elapsed:    9.8s finished\n\n[2024-10-30 15:44:36] Features: 138/274 -- score: 0.9473590626880469[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 136 out of 136 | elapsed:    9.8s finished\n\n[2024-10-30 15:44:46] Features: 139/274 -- score: 0.94737424598568[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=-1)]: Done 135 out of 135 | elapsed:    9.6s finished\n\n[2024-10-30 15:44:56] Features: 140/274 -- score: 0.9473837932381318[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 134 out of 134 | elapsed:    9.9s finished\n\n[2024-10-30 15:45:06] Features: 141/274 -- score: 0.9473929396082703[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 133 out of 133 | elapsed:    9.6s finished\n\n[2024-10-30 15:45:16] Features: 142/274 -- score: 0.9474012770826576[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 132 out of 132 | elapsed:    9.7s finished\n\n[2024-10-30 15:45:25] Features: 143/274 -- score: 0.9474061962769336[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=-1)]: Done 131 out of 131 | elapsed:    9.8s finished\n\n[2024-10-30 15:45:35] Features: 144/274 -- score: 0.9474113416791115[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    9.5s finished\n\n[2024-10-30 15:45:45] Features: 145/274 -- score: 0.9474160621882547[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=-1)]: Done 129 out of 129 | elapsed:    9.7s finished\n\n[2024-10-30 15:45:55] Features: 146/274 -- score: 0.9474199201898766[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=-1)]: Done 128 out of 128 | elapsed:    9.6s finished\n\n[2024-10-30 15:46:05] Features: 147/274 -- score: 0.9474233272155509[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 127 out of 127 | elapsed:    9.4s finished\n\n[2024-10-30 15:46:14] Features: 148/274 -- score: 0.9474296351823636[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=-1)]: Done 126 out of 126 | elapsed:    9.5s finished\n\n[2024-10-30 15:46:24] Features: 149/274 -- score: 0.9474395540206642[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:    9.5s finished\n\n[2024-10-30 15:46:33] Features: 150/274 -- score: 0.9474429828093704[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=-1)]: Done 124 out of 124 | elapsed:    9.4s finished\n\n[2024-10-30 15:46:43] Features: 151/274 -- score: 0.9474465016426885[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=-1)]: Done 123 out of 123 | elapsed:    9.5s finished\n\n[2024-10-30 15:46:52] Features: 152/274 -- score: 0.9474495032412944[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n[Parallel(n_jobs=-1)]: Done 122 out of 122 | elapsed:   10.6s finished\n\n[2024-10-30 15:47:03] Features: 153/274 -- score: 0.9474519156543872[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done 121 out of 121 | elapsed:    9.7s finished\n\n[2024-10-30 15:47:13] Features: 154/274 -- score: 0.9474660378832593[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    9.6s finished\n\n[2024-10-30 15:47:23] Features: 155/274 -- score: 0.9474905332865735[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done 119 out of 119 | elapsed:    9.3s finished\n\n[2024-10-30 15:47:32] Features: 156/274 -- score: 0.9475096523448945[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done 118 out of 118 | elapsed:    9.3s finished\n\n[2024-10-30 15:47:41] Features: 157/274 -- score: 0.947511307748069[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=-1)]: Done 117 out of 117 | elapsed:    9.4s finished\n\n[2024-10-30 15:47:51] Features: 158/274 -- score: 0.9475128007899316[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done 116 out of 116 | elapsed:    9.4s finished\n\n[2024-10-30 15:48:00] Features: 159/274 -- score: 0.947514045113268[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=-1)]: Done 115 out of 115 | elapsed:    9.1s finished\n\n[2024-10-30 15:48:10] Features: 160/274 -- score: 0.9475152239346908[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done 114 out of 114 | elapsed:    9.4s finished\n\n[2024-10-30 15:48:19] Features: 161/274 -- score: 0.9475163807873658[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done 113 out of 113 | elapsed:    9.8s finished\n\n[2024-10-30 15:48:29] Features: 162/274 -- score: 0.9475173605240507[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:   10.1s finished\n\n[2024-10-30 15:48:39] Features: 163/274 -- score: 0.9475182819483899[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=-1)]: Done 111 out of 111 | elapsed:    9.1s finished\n\n[2024-10-30 15:48:48] Features: 164/274 -- score: 0.9475196186641346[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 110 out of 110 | elapsed:    8.9s finished\n\n[2024-10-30 15:48:57] Features: 165/274 -- score: 0.947559673168465[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=-1)]: Done 109 out of 109 | elapsed:    8.8s finished\n\n[2024-10-30 15:49:06] Features: 166/274 -- score: 0.947613687662578[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:    9.0s finished\n\n[2024-10-30 15:49:15] Features: 167/274 -- score: 0.9476635282128644[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=-1)]: Done 107 out of 107 | elapsed:    8.4s finished\n\n[2024-10-30 15:49:24] Features: 168/274 -- score: 0.947702904101134[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=-1)]: Done 106 out of 106 | elapsed:    8.8s finished\n\n[2024-10-30 15:49:33] Features: 169/274 -- score: 0.9477199291545982[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done 105 out of 105 | elapsed:    8.8s finished\n\n[2024-10-30 15:49:42] Features: 170/274 -- score: 0.9477278669523537[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n[Parallel(n_jobs=-1)]: Done 104 out of 104 | elapsed:    8.9s finished\n\n[2024-10-30 15:49:51] Features: 171/274 -- score: 0.9477322562290738[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=-1)]: Done 103 out of 103 | elapsed:    8.6s finished\n\n[2024-10-30 15:49:59] Features: 172/274 -- score: 0.9477346568857261[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=-1)]: Done 102 out of 102 | elapsed:    8.4s finished\n\n[2024-10-30 15:50:08] Features: 173/274 -- score: 0.9477348473305426[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=-1)]: Done 101 out of 101 | elapsed:    8.8s finished\n\n[2024-10-30 15:50:17] Features: 174/274 -- score: 0.9477348473305568[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    8.7s finished\n\n[2024-10-30 15:50:26] Features: 175/274 -- score: 0.9477348473305618[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n[Parallel(n_jobs=-1)]: Done  99 out of  99 | elapsed:    8.8s finished\n\n[2024-10-30 15:50:35] Features: 176/274 -- score: 0.9477348473305675[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n[Parallel(n_jobs=-1)]: Done  98 out of  98 | elapsed:    8.9s finished\n\n[2024-10-30 15:50:44] Features: 177/274 -- score: 0.9477348473305586[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done  97 out of  97 | elapsed:    8.5s finished\n\n[2024-10-30 15:50:52] Features: 178/274 -- score: 0.9477348473305609[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed:    8.5s finished\n\n[2024-10-30 15:51:01] Features: 179/274 -- score: 0.9477348473305595[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n[Parallel(n_jobs=-1)]: Done  95 out of  95 | elapsed:    8.2s finished\n\n[2024-10-30 15:51:09] Features: 180/274 -- score: 0.9477348473305647[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done  94 out of  94 | elapsed:    8.3s finished\n\n[2024-10-30 15:51:17] Features: 181/274 -- score: 0.9477348473305642[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n[Parallel(n_jobs=-1)]: Done  93 out of  93 | elapsed:    8.3s finished\n\n[2024-10-30 15:51:26] Features: 182/274 -- score: 0.9477348473305577[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n[Parallel(n_jobs=-1)]: Done  92 out of  92 | elapsed:    8.1s finished\n\n[2024-10-30 15:51:34] Features: 183/274 -- score: 0.9477348473305609[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done  91 out of  91 | elapsed:    7.8s finished\n\n[2024-10-30 15:51:42] Features: 184/274 -- score: 0.9479327591304928[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    8.2s finished\n\n[2024-10-30 15:51:50] Features: 185/274 -- score: 0.9477348473305778[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n[Parallel(n_jobs=-1)]: Done  89 out of  89 | elapsed:    7.9s finished\n\n[2024-10-30 15:51:58] Features: 186/274 -- score: 0.9477348473305677[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n[Parallel(n_jobs=-1)]: Done  88 out of  88 | elapsed:    7.9s finished\n\n[2024-10-30 15:52:06] Features: 187/274 -- score: 0.9477348473305719[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=-1)]: Done  87 out of  87 | elapsed:    7.8s finished\n\n[2024-10-30 15:52:14] Features: 188/274 -- score: 0.9477348473305671[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n[Parallel(n_jobs=-1)]: Done  86 out of  86 | elapsed:    8.0s finished\n\n[2024-10-30 15:52:22] Features: 189/274 -- score: 0.94773484733056[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n[Parallel(n_jobs=-1)]: Done  85 out of  85 | elapsed:    8.3s finished\n\n[2024-10-30 15:52:31] Features: 190/274 -- score: 0.9477348473305632[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n[Parallel(n_jobs=-1)]: Done  84 out of  84 | elapsed:    7.9s finished\n\n[2024-10-30 15:52:39] Features: 191/274 -- score: 0.9477348473305647[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n[Parallel(n_jobs=-1)]: Done  83 out of  83 | elapsed:    7.7s finished\n\n[2024-10-30 15:52:46] Features: 192/274 -- score: 0.947734847330557[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n[Parallel(n_jobs=-1)]: Done  82 out of  82 | elapsed:    8.1s finished\n\n[2024-10-30 15:52:55] Features: 193/274 -- score: 0.9477348473305544[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.6s\n[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:    8.3s finished\n\n[2024-10-30 15:53:03] Features: 194/274 -- score: 0.9479431431644982[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.5s\n[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    8.0s finished\n\n[2024-10-30 15:53:11] Features: 195/274 -- score: 0.9477174961107611[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n[Parallel(n_jobs=-1)]: Done  79 out of  79 | elapsed:    7.5s finished\n\n[2024-10-30 15:53:19] Features: 196/274 -- score: 0.9477220057713558[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.5s\n[Parallel(n_jobs=-1)]: Done  78 out of  78 | elapsed:    7.5s finished\n\n[2024-10-30 15:53:26] Features: 197/274 -- score: 0.9478021679359317[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n[Parallel(n_jobs=-1)]: Done  77 out of  77 | elapsed:    7.9s finished\n\n[2024-10-30 15:53:34] Features: 198/274 -- score: 0.9476871492756201[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.5s\n[Parallel(n_jobs=-1)]: Done  76 out of  76 | elapsed:    7.7s finished\n\n[2024-10-30 15:53:42] Features: 199/274 -- score: 0.9476871492756416[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    7.4s finished\n\n[2024-10-30 15:53:50] Features: 200/274 -- score: 0.947793040632469[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n[Parallel(n_jobs=-1)]: Done  74 out of  74 | elapsed:    7.7s finished\n\n[2024-10-30 15:53:58] Features: 201/274 -- score: 0.9477213973343659[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.6s\n[Parallel(n_jobs=-1)]: Done  73 out of  73 | elapsed:    7.5s finished\n\n[2024-10-30 15:54:05] Features: 202/274 -- score: 0.9476673271270835[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:    7.4s finished\n\n[2024-10-30 15:54:13] Features: 203/274 -- score: 0.9476626655969114[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.5s\n[Parallel(n_jobs=-1)]: Done  71 out of  71 | elapsed:    7.1s finished\n\n[2024-10-30 15:54:20] Features: 204/274 -- score: 0.9476300273488697[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.6s\n[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:    7.2s finished\n\n[2024-10-30 15:54:27] Features: 205/274 -- score: 0.9474341605681742[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.5s\n[Parallel(n_jobs=-1)]: Done  69 out of  69 | elapsed:    7.1s finished\n\n[2024-10-30 15:54:34] Features: 206/274 -- score: 0.9474283068356218[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.5s\n[Parallel(n_jobs=-1)]: Done  68 out of  68 | elapsed:    7.1s finished\n\n[2024-10-30 15:54:42] Features: 207/274 -- score: 0.9474175553568045[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.5s\n[Parallel(n_jobs=-1)]: Done  67 out of  67 | elapsed:    7.0s finished\n\n[2024-10-30 15:54:49] Features: 208/274 -- score: 0.9474659427760701[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.6s\n[Parallel(n_jobs=-1)]: Done  66 out of  66 | elapsed:    7.0s finished\n\n[2024-10-30 15:54:56] Features: 209/274 -- score: 0.9474492362265832[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.5s\n[Parallel(n_jobs=-1)]: Done  65 out of  65 | elapsed:    6.9s finished\n\n[2024-10-30 15:55:03] Features: 210/274 -- score: 0.9472195020915131[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.5s\n[Parallel(n_jobs=-1)]: Done  64 out of  64 | elapsed:    6.9s finished\n\n[2024-10-30 15:55:10] Features: 211/274 -- score: 0.9472840750057999[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n[Parallel(n_jobs=-1)]: Done  63 out of  63 | elapsed:    6.8s finished\n\n[2024-10-30 15:55:17] Features: 212/274 -- score: 0.9470945712133542[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.7s\n[Parallel(n_jobs=-1)]: Done  62 out of  62 | elapsed:    6.8s finished\n\n[2024-10-30 15:55:24] Features: 213/274 -- score: 0.9471612135864695[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.6s\n[Parallel(n_jobs=-1)]: Done  61 out of  61 | elapsed:    6.6s finished\n\n[2024-10-30 15:55:30] Features: 214/274 -- score: 0.9469856521781331[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.6s\n[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    6.6s finished\n\n[2024-10-30 15:55:37] Features: 215/274 -- score: 0.9469172110948165[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.6s\n[Parallel(n_jobs=-1)]: Done  59 out of  59 | elapsed:    6.5s finished\n\n[2024-10-30 15:55:44] Features: 216/274 -- score: 0.9468774297337254[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.6s\n[Parallel(n_jobs=-1)]: Done  58 out of  58 | elapsed:    6.5s finished\n\n[2024-10-30 15:55:50] Features: 217/274 -- score: 0.9468858969393554[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.9s\n[Parallel(n_jobs=-1)]: Done  55 out of  57 | elapsed:    6.6s remaining:    0.1s\n[Parallel(n_jobs=-1)]: Done  57 out of  57 | elapsed:    6.6s finished\n\n[2024-10-30 15:55:57] Features: 218/274 -- score: 0.9468858969393794[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.8s\n[Parallel(n_jobs=-1)]: Done  54 out of  56 | elapsed:    6.5s remaining:    0.1s\n[Parallel(n_jobs=-1)]: Done  56 out of  56 | elapsed:    6.5s finished\n\n[2024-10-30 15:56:04] Features: 219/274 -- score: 0.9468858969393843[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.8s\n[Parallel(n_jobs=-1)]: Done  52 out of  55 | elapsed:    6.3s remaining:    0.3s\n[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed:    6.4s finished\n\n[2024-10-30 15:56:10] Features: 220/274 -- score: 0.9468966252072881[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.8s\n[Parallel(n_jobs=-1)]: Done  51 out of  54 | elapsed:    6.0s remaining:    0.3s\n[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:    6.1s finished\n\n[2024-10-30 15:56:16] Features: 221/274 -- score: 0.9468963452865928[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.7s\n[Parallel(n_jobs=-1)]: Done  49 out of  53 | elapsed:    5.9s remaining:    0.4s\n[Parallel(n_jobs=-1)]: Done  53 out of  53 | elapsed:    6.1s finished\n\n[2024-10-30 15:56:22] Features: 222/274 -- score: 0.9468913460980689[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.8s\n[Parallel(n_jobs=-1)]: Done  48 out of  52 | elapsed:    5.7s remaining:    0.4s\n[Parallel(n_jobs=-1)]: Done  52 out of  52 | elapsed:    5.9s finished\n\n[2024-10-30 15:56:28] Features: 223/274 -- score: 0.9469002261643519[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.7s\n[Parallel(n_jobs=-1)]: Done  46 out of  51 | elapsed:    5.6s remaining:    0.5s\n[Parallel(n_jobs=-1)]: Done  51 out of  51 | elapsed:    5.7s finished\n\n[2024-10-30 15:56:34] Features: 224/274 -- score: 0.9468903226124956[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.8s\n[Parallel(n_jobs=-1)]: Done  45 out of  50 | elapsed:    5.5s remaining:    0.5s\n[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    5.7s finished\n\n[2024-10-30 15:56:40] Features: 225/274 -- score: 0.9468797661389831[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.7s\n[Parallel(n_jobs=-1)]: Done  43 out of  49 | elapsed:    5.4s remaining:    0.7s\n[Parallel(n_jobs=-1)]: Done  49 out of  49 | elapsed:    5.6s finished\n\n[2024-10-30 15:56:46] Features: 226/274 -- score: 0.9468713588961108[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.9s\n[Parallel(n_jobs=-1)]: Done  42 out of  48 | elapsed:    5.3s remaining:    0.7s\n[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:    5.5s finished\n\n[2024-10-30 15:56:51] Features: 227/274 -- score: 0.9468383593915652[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.8s\n[Parallel(n_jobs=-1)]: Done  40 out of  47 | elapsed:    5.0s remaining:    0.8s\n[Parallel(n_jobs=-1)]: Done  47 out of  47 | elapsed:    5.3s finished\n\n[2024-10-30 15:56:57] Features: 228/274 -- score: 0.9467858747397688[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.9s\n[Parallel(n_jobs=-1)]: Done  39 out of  46 | elapsed:    5.0s remaining:    0.8s\n[Parallel(n_jobs=-1)]: Done  46 out of  46 | elapsed:    5.3s finished\n\n[2024-10-30 15:57:02] Features: 229/274 -- score: 0.9467710676386449[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.9s\n[Parallel(n_jobs=-1)]: Done  37 out of  45 | elapsed:    5.0s remaining:    1.0s\n[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    5.3s finished\n\n[2024-10-30 15:57:08] Features: 230/274 -- score: 0.9467550149626927[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.9s\n[Parallel(n_jobs=-1)]: Done  36 out of  44 | elapsed:    5.0s remaining:    1.0s\n[Parallel(n_jobs=-1)]: Done  44 out of  44 | elapsed:    5.4s finished\n\n[2024-10-30 15:57:13] Features: 231/274 -- score: 0.9467383676403364[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.9s\n[Parallel(n_jobs=-1)]: Done  34 out of  43 | elapsed:    4.6s remaining:    1.1s\n[Parallel(n_jobs=-1)]: Done  43 out of  43 | elapsed:    5.0s finished\n\n[2024-10-30 15:57:18] Features: 232/274 -- score: 0.9467131783637202[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.8s\n[Parallel(n_jobs=-1)]: Done  33 out of  42 | elapsed:    4.6s remaining:    1.2s\n[Parallel(n_jobs=-1)]: Done  42 out of  42 | elapsed:    5.0s finished\n\n[2024-10-30 15:57:23] Features: 233/274 -- score: 0.9466859943937971[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.9s\n[Parallel(n_jobs=-1)]: Done  31 out of  41 | elapsed:    4.3s remaining:    1.3s\n[Parallel(n_jobs=-1)]: Done  41 out of  41 | elapsed:    4.9s finished\n\n[2024-10-30 15:57:28] Features: 234/274 -- score: 0.9466508831445383[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.8s\n[Parallel(n_jobs=-1)]: Done  30 out of  40 | elapsed:    4.4s remaining:    1.4s\n[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    4.9s finished\n\n[2024-10-30 15:57:34] Features: 235/274 -- score: 0.9466138704337347[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  28 out of  39 | elapsed:    4.2s remaining:    1.6s\n[Parallel(n_jobs=-1)]: Done  39 out of  39 | elapsed:    4.7s finished\n\n[2024-10-30 15:57:38] Features: 236/274 -- score: 0.9465750648065324[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  27 out of  38 | elapsed:    4.0s remaining:    1.6s\n[Parallel(n_jobs=-1)]: Done  38 out of  38 | elapsed:    4.6s finished\n\n[2024-10-30 15:57:43] Features: 237/274 -- score: 0.946533477078739[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  25 out of  37 | elapsed:    3.7s remaining:    1.7s\n[Parallel(n_jobs=-1)]: Done  37 out of  37 | elapsed:    4.6s finished\n\n[2024-10-30 15:57:48] Features: 238/274 -- score: 0.946486463996916[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  24 out of  36 | elapsed:    3.7s remaining:    1.8s\n[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    4.5s finished\n\n[2024-10-30 15:57:52] Features: 239/274 -- score: 0.9464394609940602[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  22 out of  35 | elapsed:    3.5s remaining:    2.0s\n[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:    4.3s finished\n\n[2024-10-30 15:57:57] Features: 240/274 -- score: 0.9467273803937399[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  21 out of  34 | elapsed:    3.5s remaining:    2.1s\n[Parallel(n_jobs=-1)]: Done  34 out of  34 | elapsed:    4.2s finished\n\n[2024-10-30 15:58:01] Features: 241/274 -- score: 0.9467599072842546[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  19 out of  33 | elapsed:    3.3s remaining:    2.4s\n[Parallel(n_jobs=-1)]: Done  33 out of  33 | elapsed:    4.2s finished\n\n[2024-10-30 15:58:05] Features: 242/274 -- score: 0.9467735822418181[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  18 out of  32 | elapsed:    3.0s remaining:    2.3s\n[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:    4.0s finished\n\n[2024-10-30 15:58:09] Features: 243/274 -- score: 0.94672377591958[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  16 out of  31 | elapsed:    2.7s remaining:    2.5s\n[Parallel(n_jobs=-1)]: Done  31 out of  31 | elapsed:    3.8s finished\n\n[2024-10-30 15:58:13] Features: 244/274 -- score: 0.9466509249761966[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  15 out of  30 | elapsed:    2.8s remaining:    2.8s\n[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    3.9s finished\n\n[2024-10-30 15:58:17] Features: 245/274 -- score: 0.9466867379399406[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  13 out of  29 | elapsed:    2.6s remaining:    3.2s\n[Parallel(n_jobs=-1)]: Done  29 out of  29 | elapsed:    3.8s finished\n\n[2024-10-30 15:58:21] Features: 246/274 -- score: 0.9466071795787135[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  12 out of  28 | elapsed:    2.4s remaining:    3.3s\n[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:    3.6s finished\n\n[2024-10-30 15:58:25] Features: 247/274 -- score: 0.9465238896311116[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  10 out of  27 | elapsed:    2.2s remaining:    3.7s\n[Parallel(n_jobs=-1)]: Done  24 out of  27 | elapsed:    3.3s remaining:    0.3s\n[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:    3.4s finished\n\n[2024-10-30 15:58:28] Features: 248/274 -- score: 0.946521579716731[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 out of  26 | elapsed:    2.1s remaining:    4.0s\n[Parallel(n_jobs=-1)]: Done  23 out of  26 | elapsed:    3.3s remaining:    0.3s\n[Parallel(n_jobs=-1)]: Done  26 out of  26 | elapsed:    3.4s finished\n\n[2024-10-30 15:58:32] Features: 249/274 -- score: 0.9464362915278152[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   7 out of  25 | elapsed:    1.8s remaining:    4.7s\n[Parallel(n_jobs=-1)]: Done  20 out of  25 | elapsed:    3.1s remaining:    0.7s\n[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    3.3s finished\n\n[2024-10-30 15:58:35] Features: 250/274 -- score: 0.9463261799186548[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:    1.7s remaining:    5.4s\n[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:    3.0s remaining:    0.7s\n[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    3.1s finished\n\n[2024-10-30 15:58:39] Features: 251/274 -- score: 0.9462690802707947[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   4 out of  23 | elapsed:    1.3s remaining:    6.5s\n[Parallel(n_jobs=-1)]: Done  16 out of  23 | elapsed:    2.7s remaining:    1.1s\n[Parallel(n_jobs=-1)]: Done  23 out of  23 | elapsed:    2.9s finished\n\n[2024-10-30 15:58:42] Features: 252/274 -- score: 0.9461447525929906[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   3 out of  22 | elapsed:    1.3s remaining:    8.9s\n[Parallel(n_jobs=-1)]: Done  15 out of  22 | elapsed:    2.6s remaining:    1.1s\n[Parallel(n_jobs=-1)]: Done  22 out of  22 | elapsed:    2.9s finished\n\n[2024-10-30 15:58:45] Features: 253/274 -- score: 0.9459954927439093[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  12 out of  21 | elapsed:    2.3s remaining:    1.7s\n[Parallel(n_jobs=-1)]: Done  21 out of  21 | elapsed:    2.8s finished\n\n[2024-10-30 15:58:47] Features: 254/274 -- score: 0.9458383532301985[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:    2.2s remaining:    1.8s\n[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    2.7s finished\n\n[2024-10-30 15:58:50] Features: 255/274 -- score: 0.945679156290041[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   8 out of  19 | elapsed:    1.9s remaining:    2.6s\n[Parallel(n_jobs=-1)]: Done  19 out of  19 | elapsed:    2.6s finished\n\n[2024-10-30 15:58:53] Features: 256/274 -- score: 0.9455899760485096[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   7 out of  18 | elapsed:    1.8s remaining:    2.9s\n[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    2.5s finished\n\n[2024-10-30 15:58:56] Features: 257/274 -- score: 0.9454303045137135[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   4 out of  17 | elapsed:    1.4s remaining:    4.7s\n[Parallel(n_jobs=-1)]: Done  13 out of  17 | elapsed:    2.1s remaining:    0.6s\n[Parallel(n_jobs=-1)]: Done  17 out of  17 | elapsed:    2.3s finished\n\n[2024-10-30 15:58:58] Features: 258/274 -- score: 0.9458661657907452[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   3 out of  16 | elapsed:    1.2s remaining:    5.5s\n[Parallel(n_jobs=-1)]: Done  12 out of  16 | elapsed:    2.0s remaining:    0.6s\n[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:    2.1s finished\n\n[2024-10-30 15:59:00] Features: 259/274 -- score: 0.9461832380497425[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:    1.8s remaining:    1.5s\n[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    2.2s finished\n\n[2024-10-30 15:59:03] Features: 260/274 -- score: 0.9460372836244307[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   7 out of  14 | elapsed:    1.6s remaining:    1.6s\n[Parallel(n_jobs=-1)]: Done  14 out of  14 | elapsed:    1.9s finished\n\n[2024-10-30 15:59:04] Features: 261/274 -- score: 0.9458763464886486[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   3 out of  13 | elapsed:    1.2s remaining:    4.2s\n[Parallel(n_jobs=-1)]: Done  10 out of  13 | elapsed:    1.7s remaining:    0.4s\n[Parallel(n_jobs=-1)]: Done  13 out of  13 | elapsed:    1.8s finished\n\n[2024-10-30 15:59:06] Features: 262/274 -- score: 0.9458467262666927[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    1.1s remaining:    6.0s\n[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    1.6s remaining:    0.5s\n[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    1.7s finished\n\n[2024-10-30 15:59:08] Features: 263/274 -- score: 0.9456388669986378[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   4 out of  11 | elapsed:    1.1s remaining:    2.1s\n[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    1.5s finished\n\n[2024-10-30 15:59:10] Features: 264/274 -- score: 0.9452713138924475[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    1.1s remaining:    2.6s\n[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.4s finished\n\n[2024-10-30 15:59:11] Features: 265/274 -- score: 0.945161446961162[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:    1.1s remaining:    2.3s\n[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    1.3s finished\n\n[2024-10-30 15:59:13] Features: 266/274 -- score: 0.944754144553993[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:    0.9s remaining:    2.8s\n[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    1.1s finished\n\n[2024-10-30 15:59:14] Features: 267/274 -- score: 0.9446487424045044[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   4 out of   7 | elapsed:    0.9s remaining:    0.6s\n[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed:    1.0s finished\n\n[2024-10-30 15:59:15] Features: 268/274 -- score: 0.9441455030203605[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    0.8s remaining:    0.8s\n[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    0.9s finished\n\n[2024-10-30 15:59:16] Features: 269/274 -- score: 0.9441455030203807[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.8s finished\n\n[2024-10-30 15:59:17] Features: 270/274 -- score: 0.9434739244637808[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.6s finished\n\n[2024-10-30 15:59:18] Features: 271/274 -- score: 0.9434233272407218[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.5s finished\n\n[2024-10-30 15:59:18] Features: 272/274 -- score: 0.9424326394992495[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    0.4s finished\n\n[2024-10-30 15:59:19] Features: 273/274 -- score: -30180713684991.74[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n\n[2024-10-30 15:59:19] Features: 274/274 -- score: -2977506930194.4707</pre> <p>Now, we are going to plot the score vs number of features</p> In\u00a0[134]: Copied! <pre># score results\nsfs_dict = sfs.get_metric_dict()\nx = [i for i in sfs_dict]\ny = [sfs_dict[i]['avg_score'] for i in sfs_dict]\n# slice list to avoid last 2 extreme scores\nx2 = x[0:272]\ny2 = y[0:272]\nsns.lineplot(x=x2, y=y2);\n</pre> # score results sfs_dict = sfs.get_metric_dict() x = [i for i in sfs_dict] y = [sfs_dict[i]['avg_score'] for i in sfs_dict] # slice list to avoid last 2 extreme scores x2 = x[0:272] y2 = y[0:272] sns.lineplot(x=x2, y=y2); <p>With 50 features the score is around 0.93, and it does not improve significantly with additional features. Actually, it decreases after 265 features.</p> In\u00a0[135]: Copied! <pre>reg = LinearRegression()\n\n# Build step forward feature selection with 50 features\nsfs = SFS(\n    reg,\n    k_features=20,\n    forward=True,\n    floating=False,\n    scoring=\"r2\",\n    n_jobs=-1,\n    verbose=2,\n    cv=5,\n)\n\n# Perform SFFS\nsfs = sfs.fit(x_train, y_train)\n</pre> reg = LinearRegression()  # Build step forward feature selection with 50 features sfs = SFS(     reg,     k_features=20,     forward=True,     floating=False,     scoring=\"r2\",     n_jobs=-1,     verbose=2,     cv=5, )  # Perform SFFS sfs = sfs.fit(x_train, y_train) <pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.5s\n[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    8.1s\n[Parallel(n_jobs=-1)]: Done 274 out of 274 | elapsed:   12.2s finished\n\n[2024-10-30 16:18:10] Features: 1/20 -- score: 0.6008605735778773[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n[Parallel(n_jobs=-1)]: Done 228 tasks      | elapsed:    4.0s\n[Parallel(n_jobs=-1)]: Done 242 out of 273 | elapsed:    4.2s remaining:    0.5s\n[Parallel(n_jobs=-1)]: Done 273 out of 273 | elapsed:    4.9s finished\n\n[2024-10-30 16:18:15] Features: 2/20 -- score: 0.8258189338417044[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 272 out of 272 | elapsed:    4.8s finished\n\n[2024-10-30 16:18:20] Features: 3/20 -- score: 0.843163289844291[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 240 out of 271 | elapsed:    4.3s remaining:    0.5s\n[Parallel(n_jobs=-1)]: Done 271 out of 271 | elapsed:    4.9s finished\n\n[2024-10-30 16:18:25] Features: 4/20 -- score: 0.8607242290453196[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed:    4.8s finished\n\n[2024-10-30 16:18:30] Features: 5/20 -- score: 0.8673527352506463[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 238 out of 269 | elapsed:    4.2s remaining:    0.5s\n[Parallel(n_jobs=-1)]: Done 269 out of 269 | elapsed:    4.8s finished\n\n[2024-10-30 16:18:35] Features: 6/20 -- score: 0.8733406729210295[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 268 out of 268 | elapsed:    4.8s finished\n\n[2024-10-30 16:18:39] Features: 7/20 -- score: 0.8779019854352988[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 236 out of 267 | elapsed:    4.4s remaining:    0.5s\n[Parallel(n_jobs=-1)]: Done 267 out of 267 | elapsed:    5.1s finished\n\n[2024-10-30 16:18:45] Features: 8/20 -- score: 0.8821076936222507[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 266 out of 266 | elapsed:    5.1s finished\n\n[2024-10-30 16:18:50] Features: 9/20 -- score: 0.8865803385052942[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 234 out of 265 | elapsed:    4.5s remaining:    0.5s\n[Parallel(n_jobs=-1)]: Done 265 out of 265 | elapsed:    5.1s finished\n\n[2024-10-30 16:18:55] Features: 10/20 -- score: 0.892136862625631[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 264 out of 264 | elapsed:    5.1s finished\n\n[2024-10-30 16:19:00] Features: 11/20 -- score: 0.8997588358866588[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 232 out of 263 | elapsed:    4.5s remaining:    0.5s\n[Parallel(n_jobs=-1)]: Done 263 out of 263 | elapsed:    5.1s finished\n\n[2024-10-30 16:19:06] Features: 12/20 -- score: 0.9045279013258785[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 262 out of 262 | elapsed:    5.1s finished\n\n[2024-10-30 16:19:11] Features: 13/20 -- score: 0.9077032400393492[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 230 out of 261 | elapsed:    4.5s remaining:    0.5s\n[Parallel(n_jobs=-1)]: Done 261 out of 261 | elapsed:    5.1s finished\n\n[2024-10-30 16:19:16] Features: 14/20 -- score: 0.9113184768449554[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 260 out of 260 | elapsed:    5.1s finished\n\n[2024-10-30 16:19:21] Features: 15/20 -- score: 0.9144300582982374[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 228 out of 259 | elapsed:    4.6s remaining:    0.5s\n[Parallel(n_jobs=-1)]: Done 259 out of 259 | elapsed:    5.2s finished\n\n[2024-10-30 16:19:26] Features: 16/20 -- score: 0.9165568997201585[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 258 out of 258 | elapsed:    5.2s finished\n\n[2024-10-30 16:19:32] Features: 17/20 -- score: 0.9187094547000717[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 226 out of 257 | elapsed:    5.0s remaining:    0.6s\n[Parallel(n_jobs=-1)]: Done 257 out of 257 | elapsed:    5.5s finished\n\n[2024-10-30 16:19:37] Features: 18/20 -- score: 0.9210414399636491[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 256 out of 256 | elapsed:    5.4s finished\n\n[2024-10-30 16:19:43] Features: 19/20 -- score: 0.9231785068400782[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 255 out of 255 | elapsed:    5.7s finished\n\n[2024-10-30 16:19:49] Features: 20/20 -- score: 0.9244145086624963</pre> In\u00a0[136]: Copied! <pre># Most important features\nfeat_cols = list(sfs.k_feature_idx_)\nprint(feat_cols)\n</pre> # Most important features feat_cols = list(sfs.k_feature_idx_) print(feat_cols) <pre>[0, 2, 4, 5, 6, 8, 10, 13, 19, 20, 24, 25, 27, 37, 40, 43, 44, 52, 53, 266]\n</pre> In\u00a0[137]: Copied! <pre>x_train.columns[feat_cols]\n</pre> x_train.columns[feat_cols] Out[137]: <pre>Index(['Year', 'Engine', 'Kilometers_Driven_log', 'Power_log',\n       'Location_Bangalore', 'Location_Coimbatore', 'Location_Hyderabad',\n       'Location_Kolkata', 'Fuel_Type_Petrol', 'Transmission_Manual',\n       'Brand_Audi', 'Brand_BMW', 'Brand_Chevrolet', 'Brand_Jaguar',\n       'Brand_Land', 'Brand_Mercedes-Benz', 'Brand_Mini', 'Brand_Tata',\n       'Brand_Toyota', 'Model_Xylo'],\n      dtype='object')</pre> <p>New independent train and test sets with the 50 variables selected in the sequential feature selection</p> In\u00a0[138]: Copied! <pre>x_train_final = x_train[x_train.columns[feat_cols]]\nx_test_final = x_test[x_train.columns[feat_cols]]\n</pre> x_train_final = x_train[x_train.columns[feat_cols]] x_test_final = x_test[x_train.columns[feat_cols]] In\u00a0[139]: Copied! <pre>#check shape\nx_train_final.shape\n</pre> #check shape x_train_final.shape Out[139]: <pre>(4213, 20)</pre> In\u00a0[140]: Copied! <pre>#check shape\nx_test_final.shape\n</pre> #check shape x_test_final.shape Out[140]: <pre>(1806, 20)</pre> In\u00a0[141]: Copied! <pre># Fitting linear model\nlin_reg_model2 = LinearRegression()\nlin_reg_model2.fit(x_train_final, y_train)\n\n# let us check the coefficients and intercept of the model\n\ncoef_df = pd.DataFrame(\n    np.append(lin_reg_model2.coef_.flatten(), lin_reg_model2.intercept_),\n    index=x_train_final.columns.tolist() + [\"Intercept\"],\n    columns=[\"Coefficients\"],\n)\ncoef_df\n</pre> # Fitting linear model lin_reg_model2 = LinearRegression() lin_reg_model2.fit(x_train_final, y_train)  # let us check the coefficients and intercept of the model  coef_df = pd.DataFrame(     np.append(lin_reg_model2.coef_.flatten(), lin_reg_model2.intercept_),     index=x_train_final.columns.tolist() + [\"Intercept\"],     columns=[\"Coefficients\"], ) coef_df Out[141]: Coefficients Year 0.115198 Engine 0.000226 Kilometers_Driven_log -0.074748 Power_log 0.778158 Location_Bangalore 0.183211 Location_Coimbatore 0.156587 Location_Hyderabad 0.180627 Location_Kolkata -0.192565 Fuel_Type_Petrol -0.198387 Transmission_Manual -0.107517 Brand_Audi 0.571186 Brand_BMW 0.519360 Brand_Chevrolet -0.278779 Brand_Jaguar 0.607397 Brand_Land 0.887929 Brand_Mercedes-Benz 0.580511 Brand_Mini 0.913336 Brand_Tata -0.406731 Brand_Toyota 0.251947 Model_Xylo -0.500064 Intercept -233.238329 In\u00a0[142]: Copied! <pre># R^2 train set\nlin_reg_model2.score(x_train_final, y_train)\n</pre> # R^2 train set lin_reg_model2.score(x_train_final, y_train) Out[142]: <pre>0.9261017957003274</pre> In\u00a0[143]: Copied! <pre># R^2 test set\nlin_reg_model2.score(x_test_final, y_test)\n</pre> # R^2 test set lin_reg_model2.score(x_test_final, y_test) Out[143]: <pre>0.9318611297475975</pre> In\u00a0[144]: Copied! <pre># Model performance on train set\nmodel_perf(lin_reg_model2, x_train_final, y_train)\n</pre> # Model performance on train set model_perf(lin_reg_model2, x_train_final, y_train) Out[144]: <pre>{'RMSE': 0.2371177477741038,\n 'MAE': 0.17574233711503315,\n 'R^2': 0.9261017957003274,\n 'Adjusted R^2': 0.9257492279317221}</pre> In\u00a0[145]: Copied! <pre># Model performance on train set\nmodel_perf(lin_reg_model2, x_test_final, y_test)\n</pre> # Model performance on train set model_perf(lin_reg_model2, x_test_final, y_test) Out[145]: <pre>{'RMSE': 0.22906136480762884,\n 'MAE': 0.1709946146974173,\n 'R^2': 0.9318611297475975,\n 'Adjusted R^2': 0.9310976690164782}</pre> In\u00a0[146]: Copied! <pre>coef_df[coef_df['Coefficients']&gt;0]\n</pre> coef_df[coef_df['Coefficients']&gt;0]  Out[146]: Coefficients Year 0.115198 Engine 0.000226 Power_log 0.778158 Location_Bangalore 0.183211 Location_Coimbatore 0.156587 Location_Hyderabad 0.180627 Brand_Audi 0.571186 Brand_BMW 0.519360 Brand_Jaguar 0.607397 Brand_Land 0.887929 Brand_Mercedes-Benz 0.580511 Brand_Mini 0.913336 Brand_Toyota 0.251947 In\u00a0[147]: Copied! <pre>coef_df[coef_df['Coefficients']&lt;0]\n</pre> coef_df[coef_df['Coefficients']&lt;0] Out[147]: Coefficients Kilometers_Driven_log -0.074748 Location_Kolkata -0.192565 Fuel_Type_Petrol -0.198387 Transmission_Manual -0.107517 Brand_Chevrolet -0.278779 Brand_Tata -0.406731 Model_Xylo -0.500064 Intercept -233.238329 <p>Cars4U should focus on trade:</p> <ul> <li>The business should focus to negotiate recent owned cars</li> <li>Cars with high power have a positive impact</li> <li>Diesel and Electric cars are more valued than other fuel types</li> <li>Trade cars on specifics locations: Bangalore, Chennai, Coimbatore and Hyderabad</li> <li>If possible, focus on luxury cars and models (Lamborghini, Jaguar, Porsche, etc)</li> </ul> <p>Cars4U should avoid:</p> <ul> <li>Cars with a large number of kilometers driven</li> <li>Trading on Delhi, Kochi, Kolkata and Mumbai</li> <li>LPG and Petrol cars</li> <li>Manual transmission cars</li> <li>Second and above owners cars</li> <li>Economy Brands and Models (Datsun, Renault, Honda, Mahindra)</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"machine_learning/Supervised_Learning_Regression/#supervised-learning-regression","title":"Supervised Learning : Regression\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#1-cars4u-project","title":"1 Cars4U Project\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#11-objective","title":"1.1 Objective\u00b6","text":"<ul> <li>Explore and visualize the dataset.</li> <li>Build a linear regression model to predict the prices of used cars.</li> <li>Generate a set of insights and recommendations that will help the business.</li> </ul>"},{"location":"machine_learning/Supervised_Learning_Regression/#12-data","title":"1.2 Data:\u00b6","text":"<ul> <li>S.No. : Serial Number</li> <li>Name : Name of the car which includes Brand name and Model name</li> <li>Location : The location in which the car is being sold or is available for purchase Cities</li> <li>Year : Manufacturing year of the car</li> <li>Kilometers_driven : The total kilometers driven in the car by the previous owner(s) in KM.</li> <li>Fuel_Type : The type of fuel used by the car. (Petrol, Diesel, Electric, CNG, LPG)</li> <li>Transmission : The type of transmission used by the car. (Automatic / Manual)</li> <li>Owner : Type of ownership</li> <li>Mileage : The standard mileage offered by the car company in kmpl or km/kg</li> <li>Engine : The displacement volume of the engine in CC.</li> <li>Power : The maximum power of the engine in bhp.</li> <li>Seats : The number of seats in the car.</li> <li>New_Price : The price of a new car of the same model in INR Lakhs.(1 Lakh = 100, 000)</li> <li>Price : The price of the used car in INR Lakhs (1 Lakh = 100, 000)</li> </ul>"},{"location":"machine_learning/Supervised_Learning_Regression/#13-problem-definition-and-questions-to-be-answered","title":"1.3 Problem definition and questions to be answered\u00b6","text":"<p>In this project we want to analyze how different characteristics of used cars impact the Price of the car. The questions to be answer are:</p> <ul> <li>Does the brand and model of the car impact the price?</li> <li>Do luxury brands increase the price of the car?</li> <li>How much the total kilometers driven impact the price?</li> <li>Is more profitable to trade cars in some locations than others?</li> <li>What is the impact of the Fuel Type, Transmission, Mileage, Engine, Power and Seats on the price?</li> <li>Is there are relationship between the price of new cars and used cars?</li> </ul>"},{"location":"machine_learning/Supervised_Learning_Regression/#2-import-packages-and-turnoff-warnings","title":"2 Import packages and turnoff warnings\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#3-import-dataset-and-quality-of-data","title":"3 Import dataset and quality of data\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#4-characteristics-of-the-data","title":"4 Characteristics of the data\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#5-processing-columns","title":"5 Processing columns\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#51-mileage","title":"5.1 Mileage\u00b6","text":"<p>This column is the standard mileage offered by the car company in kmpl or km/kg. We are going to split the column between values and units to see if there is a relation between Fuel_Type and Mileage</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#52-engine","title":"5.2 Engine\u00b6","text":"<p>'CC' string is going to be deleted</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#53-power","title":"5.3 Power\u00b6","text":"<p>'bhp' string is going to be deleted</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#54-new_price","title":"5.4 New_Price\u00b6","text":"<p>'Lakh' and 'Cr' strings are going to be deleted.</p> <ul> <li>1 Cr = 100 Lakh</li> </ul>"},{"location":"machine_learning/Supervised_Learning_Regression/#55-featuring-engineering","title":"5.5 Featuring Engineering\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#name","title":"Name\u00b6","text":"<p>The Name column represents Brand, Model and Specs of the car. We are going to split this column in 3 columns to get that information</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#56-category-columns","title":"5.6 Category columns\u00b6","text":"<p>'Brand', 'Model', 'Specs', 'Location', 'Fuel_Type', 'Transmission', and 'Owner_Type' columns are transformed to category</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#57-drop-sno-column","title":"5.7 Drop 'S.No.' column\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#58-duplicate-rows","title":"5.8 Duplicate rows\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#58-check-characteristics-of-data-after-processing","title":"5.8 Check characteristics of data after processing\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#6-exploratory-data-analysis","title":"6 Exploratory data analysis\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#61-pandas-profiling-report","title":"6.1 Pandas profiling report\u00b6","text":"<p>We can get a first statistical and descriptive analysis using pandas_profiling</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#62-pairplot","title":"6.2 Pairplot\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#63-univariate-analysis","title":"6.3 Univariate analysis\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#631-numerical-columns","title":"6.3.1 Numerical columns\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#6311-year","title":"6.3.1.1 Year\u00b6","text":"<p>The Year distribution is slightly skewed to the left. The mean is 2013.36 and the median 2014, and there are not outliers.</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#6312-kilometers_driven","title":"6.3.1.2 Kilometers_Driven\u00b6","text":"<p>The Kilometers_Driven distribution is highly skewed to the right. The mean is 58,699 km, the median 53,416 km, and there are several outliers as we can see in the chart below.</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#6313-mileage","title":"6.3.1.3 Mileage\u00b6","text":"<p>The Mileage distribution is fairly symmetrical. The mean is 18.14 and the median 18.16. However, there are 81 rows with value equal to 0</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#6314-engine","title":"6.3.1.4 Engine|\u00b6","text":"<p>The Engine distribution is skewed to the right. The mean is 1616 and the median 1493</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#6315-power","title":"6.3.1.5 Power\u00b6","text":"<p>The Power distribution is skewed to the right. The mean is 112 and the median 94</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#6316-seats","title":"6.3.1.6 Seats\u00b6","text":"<p>6047 cars (83.4%) have 5 seats. There is one car with 0 seats and 53 with missing values.</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#6317-new_price","title":"6.3.1.7 New_Price\u00b6","text":"<p>The New_Price distribution is skewed to the right. The mean is 22.7 and the median 11.5</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#6318-price","title":"6.3.1.8 Price\u00b6","text":"<p>The Price distribution is skewed to the right. The mean is 9.4 and the median 5.6</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#categorical-columns","title":"Categorical columns\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#6321-location","title":"6.3.2.1 Location\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#6322-transmission","title":"6.3.2.2 Transmission\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#6323-owner-type","title":"6.3.2.3 Owner Type\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#6324-fuel-type","title":"6.3.2.4 Fuel Type\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#6325-brand","title":"6.3.2.5 Brand\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#64-bivariate-analysis","title":"6.4 Bivariate analysis\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#641-engine-power-and-price-relationship","title":"6.4.1 Engine, Power and Price relationship\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#642-power-seats-and-price-relationship","title":"6.4.2 Power, Seats and Price relationship\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#643-price-and-brand","title":"6.4.3 Price and Brand\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#644-price-location-and-fuel-type","title":"6.4.4 Price, Location and Fuel Type\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#7-missing-value-treatment","title":"7 Missing Value Treatment\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#8-log-transformation","title":"8 Log Transformation\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#81-kilometers_driven","title":"8.1 Kilometers_Driven\u00b6","text":"<p>Kilometers_Driven column is very skewed. We are going to use the log transformation to improve the distribution</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#82-power","title":"8.2 Power\u00b6","text":"<p>Power column is skewed. We are going to use the log transformation to improve the distribution</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#83-engine","title":"8.3 Engine\u00b6","text":"<p>Engine column is skewed. However, the log transformation does not improve the distribution</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#84-price","title":"8.4 Price\u00b6","text":"<p>Price column is skewed. We are going to use the log transformation to improve the distribution</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#9-outliers-treatment","title":"9 Outliers Treatment\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#91-kilometers_driven","title":"9.1 Kilometers_Driven\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#92-mileage","title":"9.2 Mileage\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#93-seats","title":"9.3 Seats\u00b6","text":"<p>There is one 1 car with 0 seats. We are going to replace this value with the mean</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#10-model-building","title":"10 Model Building\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#101-define-independent-and-dependent-variables","title":"10.1 Define independent and dependent variables\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#102-creating-dummy-variables","title":"10.2 Creating dummy variables\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#103-split-the-data-into-train-and-test","title":"10.3 Split the data into train and test\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#104-fitting-a-linear-model","title":"10.4 Fitting a linear model\u00b6","text":"<p>Now, we are going to run the linear regression using the train data set</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#105-performance-of-the-model","title":"10.5 Performance of the model\u00b6","text":"<p>First, we are going to calculate the $R^2$ for the train and test sets</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#1051-performance-metrics","title":"10.5.1 Performance metrics\u00b6","text":"<p>User functions to calculate performance metrics</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#1052-residuals-distribution","title":"10.5.2 Residuals distribution\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#train-set-residuals","title":"Train set residuals\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#test-set-residuals","title":"Test set residuals\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#106-coefficients-and-intercept-of-the-model","title":"10.6 Coefficients and Intercept of the model\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#1061-coefficients-interpretation","title":"10.6.1 Coefficients Interpretation\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#positive-impact","title":"Positive impact\u00b6","text":"<p>This is the list of coefficients with positive impact on prices. Among them are Year, Mileage and Power_log. Increase in these will lead to an increase in the price.</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#negative-impact","title":"Negative impact\u00b6","text":"<p>This is the list of coefficients with negative impact on prices. Among them are Kilometers_Drive_log, Engine and Seats. Increase in these will lead to a decrease in the price</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#1062-analysis-of-coefficients","title":"10.6.2 Analysis of coefficients\u00b6","text":"<ul> <li>As expected, Year (most recent) has a positive impact on Price.</li> <li>As expected, Kilometers_Driven has a negative impact on Price.</li> <li>Power has a positive impact on Price.</li> <li>Seats and Engine have a negative impact on Price.</li> <li>There are some locations with positive impact on Price: Bangalore, Chennai, Coimbatore and Hyderabad</li> <li>While, other locations have a negative impact on Price: Delhi, Jaipur, Kochi, Kolkata, Mumbai and Pune</li> <li>Diesel and Electric cars have a positive impact on Price</li> <li>Fuel LPG and Petrol have a negative impact on Price</li> <li>Manual transmission has a negative impact on Price</li> <li>Second, Third, Fourth and above owners have a negative impact on Price.</li> <li>The Brand and Model in luxury cars (Lamborghini, Jaguar, Porsche, etc) have a strong positive impact on Price.</li> <li>Economy Brands and Models (Datsun, Renault, Honda, Mahindra) have a negative impact on Price.</li> </ul>"},{"location":"machine_learning/Supervised_Learning_Regression/#11-forward-feature-selection","title":"11 Forward Feature Selection\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#111-identify-most-important-features","title":"11.1 Identify most important features\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#112-retraining-the-model","title":"11.2 Retraining the model\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#1121-model-performance","title":"11.2.1 Model Performance\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#observations","title":"Observations\u00b6","text":"<ul> <li>The new regression model have 50 features that is 18% on the number of columns of the original regression model</li> <li>The performance of the new model is very close to the original model</li> </ul>"},{"location":"machine_learning/Supervised_Learning_Regression/#113-coefficient-interpretation","title":"11.3 Coefficient Interpretation\u00b6","text":""},{"location":"machine_learning/Supervised_Learning_Regression/#1131-positive-impact","title":"11.3.1 Positive impact\u00b6","text":"<p>This is the list of coefficients with positive impact on prices. Among them are Year, Power and Seats. Increase in these will lead to an increase in the price.</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#1132-negative-impact","title":"11.3.2 Negative impact\u00b6","text":"<p>This is the list of coefficients with negative impact on prices. Among them are Mileage, Engine and Kilometers_Drive_log. Increase in these will lead to a decrease in the price</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#1133-observations","title":"11.3.3 Observations\u00b6","text":"<p>The impact of the different features on Price is similar than the original regression model</p>"},{"location":"machine_learning/Supervised_Learning_Regression/#12-actionable-insights-recommendations","title":"12 Actionable Insights &amp; Recommendations\u00b6","text":""},{"location":"machine_learning/markdown/","title":"Markdown Files","text":"<p>Whether you write your book's content in Jupyter Notebooks (<code>.ipynb</code>) or in regular markdown files (<code>.md</code>), you'll write in the same flavor of markdown called MyST Markdown. This is a simple file to help you get started and show off some syntax.</p>"},{"location":"machine_learning/markdown/#what-is-myst","title":"What is MyST?","text":"<p>MyST stands for \"Markedly Structured Text\". It is a slight variation on a flavor of markdown called \"CommonMark\" markdown, with small syntax extensions to allow you to write roles and directives in the Sphinx ecosystem.</p> <p>For more about MyST, see the MyST Markdown Overview.</p>"},{"location":"machine_learning/markdown/#sample-roles-and-directives","title":"Sample Roles and Directives","text":"<p>Roles and directives are two of the most powerful tools in Jupyter Book. They are like functions, but written in a markup language. They both serve a similar purpose, but roles are written in one line, whereas directives span many lines. They both accept different kinds of inputs, and what they do with those inputs depends on the specific role or directive that is being called.</p> <p>Here is a \"note\" directive:</p> <pre><code>Here is a note\n</code></pre> <p>It will be rendered in a special box when you build your book.</p> <p>Here is an inline directive to refer to a document: {doc}<code>markdown-notebooks</code>.</p>"},{"location":"machine_learning/markdown/#citations","title":"Citations","text":"<p>You can also cite references that are stored in a <code>bibtex</code> file. For example, the following syntax: <code>{cite}`holdgraf_evidence_2014`</code> will render like this: {cite}<code>holdgraf_evidence_2014</code>.</p> <p>Moreover, you can insert a bibliography into your page with this syntax: The <code>{bibliography}</code> directive must be used for all the <code>{cite}</code> roles to render properly. For example, if the references for your book are stored in <code>references.bib</code>, then the bibliography is inserted with:</p> <pre><code>\n</code></pre>"},{"location":"machine_learning/markdown/#learn-more","title":"Learn more","text":"<p>This is just a simple starter to get you started. You can learn a lot more at jupyterbook.org.</p>"},{"location":"machine_learning/notebooks/","title":"Project2","text":"In\u00a0[\u00a0]: Copied! <pre>from matplotlib import rcParams, cycler\nimport matplotlib.pyplot as plt\nimport numpy as np\nplt.ion()\n</pre> from matplotlib import rcParams, cycler import matplotlib.pyplot as plt import numpy as np plt.ion() In\u00a0[\u00a0]: Copied! <pre># Fixing random state for reproducibility\nnp.random.seed(19680801)\n\nN = 10\ndata = [np.logspace(0, 1, 100) + np.random.randn(100) + ii for ii in range(N)]\ndata = np.array(data).T\ncmap = plt.cm.coolwarm\nrcParams['axes.prop_cycle'] = cycler(color=cmap(np.linspace(0, 1, N)))\n\n\nfrom matplotlib.lines import Line2D\ncustom_lines = [Line2D([0], [0], color=cmap(0.), lw=4),\n                Line2D([0], [0], color=cmap(.5), lw=4),\n                Line2D([0], [0], color=cmap(1.), lw=4)]\n\nfig, ax = plt.subplots(figsize=(10, 5))\nlines = ax.plot(data)\nax.legend(custom_lines, ['Cold', 'Medium', 'Hot']);\n</pre> # Fixing random state for reproducibility np.random.seed(19680801)  N = 10 data = [np.logspace(0, 1, 100) + np.random.randn(100) + ii for ii in range(N)] data = np.array(data).T cmap = plt.cm.coolwarm rcParams['axes.prop_cycle'] = cycler(color=cmap(np.linspace(0, 1, N)))   from matplotlib.lines import Line2D custom_lines = [Line2D([0], [0], color=cmap(0.), lw=4),                 Line2D([0], [0], color=cmap(.5), lw=4),                 Line2D([0], [0], color=cmap(1.), lw=4)]  fig, ax = plt.subplots(figsize=(10, 5)) lines = ax.plot(data) ax.legend(custom_lines, ['Cold', 'Medium', 'Hot']); <p>There is a lot more that you can do with outputs (such as including interactive outputs) with your book. For more information about this, see the Jupyter Book documentation</p>"},{"location":"machine_learning/notebooks/#content-with-notebooks","title":"Content with notebooks\u00b6","text":"<p>You can also create content with Jupyter Notebooks. This means that you can include code blocks and their outputs in your book.</p>"},{"location":"machine_learning/notebooks/#markdown-notebooks","title":"Markdown + notebooks\u00b6","text":"<p>As it is markdown, you can embed images, HTML, etc into your posts!</p> <p></p> <p>You can also $add_{math}$ and</p> <p>$$ math^{blocks} $$</p> <p>or</p> <p>$$ \\begin{aligned} \\mbox{mean} la_{tex} \\\\ \\\\ math blocks \\end{aligned} $$</p> <p>But make sure you $Escape $your $dollar signs $you want to keep!</p>"},{"location":"machine_learning/notebooks/#myst-markdown","title":"MyST markdown\u00b6","text":"<p>MyST markdown works in Jupyter Notebooks as well. For more information about MyST markdown, check out the MyST guide in Jupyter Book, or see the MyST markdown documentation.</p>"},{"location":"machine_learning/notebooks/#code-blocks-and-outputs","title":"Code blocks and outputs\u00b6","text":"<p>Jupyter Book will also embed your code blocks and output in your book. For example, here's some sample Matplotlib code:</p>"},{"location":"optimization/","title":"Optimization","text":"<p>This space is all about exploring optimization models and how to solve them using  Mixed-Integer Linear Programming (MILP) techniques. We\u2019ll be using Python and the  Pyomo library to share practical examples that help tackle complex decision-making  problems. Please note, this is a work in progress, new models and solutions will be  added regularly as we continue to expand and refine our content. </p> <p>Whether you're a beginner or experienced, you'll find valuable resources to enhance  your understanding of optimization and its applications.</p>","tags":["Optimization","MILP","Python","Pyomo"]},{"location":"optimization/#models","title":"Models","text":"<ul> <li> <p> Sudoku Solver</p> <p>How to solve Sudoku puzzles using Mixed Integer Linear Programming (MILP)?</p> </li> <li> <p> Traveling Salesperson Problem</p> <p>How to find the most efficient route with the goal of minimizing travel cost</p> </li> <li> <p> Portfolio Optimization</p> <p>How to build a portfolio that gives the best return with the least amount of risk?</p> </li> </ul>","tags":["Optimization","MILP","Python","Pyomo"]},{"location":"optimization/portfolio_optimization/","title":"Portfolio Optimization","text":"Markowitz Portfolio Optimization Efficient Frontier Python In\u00a0[1]: Copied! <pre># Import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport yfinance as yfin\nfrom scipy.optimize import minimize\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Import libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt import yfinance as yfin from scipy.optimize import minimize import warnings warnings.filterwarnings('ignore') In\u00a0[2]: Copied! <pre>stocks = [\n    {\"symbol\": \"AAPL\", \"name\": \"Apple Inc.\"},\n    {\"symbol\": \"MSFT\", \"name\": \"Microsoft Corporation\"},\n    {\"symbol\": \"AMZN\", \"name\": \"Amazon.com Inc.\"},\n    {\"symbol\": \"TSLA\", \"name\": \"Tesla Inc.\"},\n    {\"symbol\": \"JNJ\", \"name\": \"Johnson &amp; Johnson\"},\n    {\"symbol\": \"PFE\", \"name\": \"Pfizer Inc.\"},\n    {\"symbol\": \"JPM\", \"name\": \"JPMorgan Chase &amp; Co.\"},\n    {\"symbol\": \"V\", \"name\": \"Visa Inc.\"},\n    {\"symbol\": \"PG\", \"name\": \"Procter &amp; Gamble Co.\"},\n    {\"symbol\": \"XOM\", \"name\": \"Exxon Mobil Corporation\"}\n]\n</pre> stocks = [     {\"symbol\": \"AAPL\", \"name\": \"Apple Inc.\"},     {\"symbol\": \"MSFT\", \"name\": \"Microsoft Corporation\"},     {\"symbol\": \"AMZN\", \"name\": \"Amazon.com Inc.\"},     {\"symbol\": \"TSLA\", \"name\": \"Tesla Inc.\"},     {\"symbol\": \"JNJ\", \"name\": \"Johnson &amp; Johnson\"},     {\"symbol\": \"PFE\", \"name\": \"Pfizer Inc.\"},     {\"symbol\": \"JPM\", \"name\": \"JPMorgan Chase &amp; Co.\"},     {\"symbol\": \"V\", \"name\": \"Visa Inc.\"},     {\"symbol\": \"PG\", \"name\": \"Procter &amp; Gamble Co.\"},     {\"symbol\": \"XOM\", \"name\": \"Exxon Mobil Corporation\"} ] In\u00a0[3]: Copied! <pre>symbols = [st['symbol'] for st in stocks]\n</pre> symbols = [st['symbol'] for st in stocks] In\u00a0[4]: Copied! <pre># Download the Daily closing price for the last 10 years\nstock_df = yfin.download(symbols, period=\"10y\")['Adj Close']\n</pre> # Download the Daily closing price for the last 10 years stock_df = yfin.download(symbols, period=\"10y\")['Adj Close'] <pre>[*********************100%%**********************]  10 of 10 completed\n</pre> In\u00a0[5]: Copied! <pre>stock_df\n</pre> stock_df Out[5]: Ticker AAPL AMZN JNJ JPM MSFT PFE PG TSLA V XOM Date 2014-11-24 26.418226 16.782000 81.752777 46.244347 40.865200 19.331116 66.516785 16.448000 59.352398 61.911777 2014-11-25 26.188854 16.752001 81.615082 45.743679 40.762161 19.491039 67.037674 16.539333 59.967785 61.303799 2014-11-26 26.500629 16.678499 82.005203 45.774017 41.002590 19.894043 67.098068 16.562668 59.967785 61.109768 2014-11-28 26.485031 16.931999 82.800697 45.637470 41.054115 19.926027 68.268211 16.301332 60.184582 58.561356 2014-12-01 25.625439 16.299999 82.632408 45.516094 41.749657 19.996380 68.003967 15.442667 60.175240 59.732059 ... ... ... ... ... ... ... ... ... ... ... 2024-11-18 228.020004 201.699997 154.770004 245.029999 414.929474 24.860001 170.750000 338.739990 312.160004 120.309998 2024-11-19 228.279999 204.610001 153.000000 243.089996 416.955414 25.100000 170.759995 346.000000 311.850006 118.629997 2024-11-20 229.000000 202.880005 153.110001 240.779999 414.659973 24.940001 170.889999 342.029999 307.390015 120.320000 2024-11-21 228.520004 198.380005 155.500000 244.759995 412.869995 25.129999 172.750000 339.640015 309.899994 121.930000 2024-11-22 230.514999 198.095001 155.020004 248.419998 415.035004 25.605000 177.399994 354.250000 308.684998 121.940002 <p>2517 rows \u00d7 10 columns</p> In\u00a0[6]: Copied! <pre>stock_df.info()\n</pre> stock_df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 2517 entries, 2014-11-24 to 2024-11-22\nData columns (total 10 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   AAPL    2517 non-null   float64\n 1   AMZN    2517 non-null   float64\n 2   JNJ     2517 non-null   float64\n 3   JPM     2517 non-null   float64\n 4   MSFT    2517 non-null   float64\n 5   PFE     2517 non-null   float64\n 6   PG      2517 non-null   float64\n 7   TSLA    2517 non-null   float64\n 8   V       2517 non-null   float64\n 9   XOM     2517 non-null   float64\ndtypes: float64(10)\nmemory usage: 216.3 KB\n</pre> In\u00a0[7]: Copied! <pre># plot stock closing price over time\nstock_df.plot(figsize=(10,5));\n</pre> # plot stock closing price over time stock_df.plot(figsize=(10,5)); In\u00a0[8]: Copied! <pre>daily_returns = stock_df.pct_change(1).dropna()\ndaily_returns.plot(figsize=(10,5));\n</pre> daily_returns = stock_df.pct_change(1).dropna() daily_returns.plot(figsize=(10,5)); In\u00a0[9]: Copied! <pre>daily_returns['AAPL'].plot(kind='hist',bins=100,figsize=(12,3));\n</pre> daily_returns['AAPL'].plot(kind='hist',bins=100,figsize=(12,3)); In\u00a0[10]: Copied! <pre>cumulative_returns = (1 + daily_returns).cumprod() - 1\ncumulative_returns_perc = cumulative_returns*100\ncumulative_returns_perc.plot(figsize=(10,5));\n</pre> cumulative_returns = (1 + daily_returns).cumprod() - 1 cumulative_returns_perc = cumulative_returns*100 cumulative_returns_perc.plot(figsize=(10,5)); In\u00a0[11]: Copied! <pre># Log Returns as Percent of next day\nlog_returns = np.log(stock_df/stock_df.shift(1))\nlog_returns\n</pre> # Log Returns as Percent of next day log_returns = np.log(stock_df/stock_df.shift(1)) log_returns Out[11]: Ticker AAPL AMZN JNJ JPM MSFT PFE PG TSLA V XOM Date 2014-11-24 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2014-11-25 -0.008720 -0.001789 -0.001686 -0.010886 -0.002525 0.008239 0.007800 0.005537 0.010315 -0.009869 2014-11-26 0.011835 -0.004397 0.004769 0.000663 0.005881 0.020466 0.000900 0.001410 0.000000 -0.003170 2014-11-28 -0.000589 0.015085 0.009654 -0.002988 0.001256 0.001606 0.017289 -0.015904 0.003609 -0.042597 2014-12-01 -0.032994 -0.038040 -0.002035 -0.002663 0.016800 0.003524 -0.003878 -0.054113 -0.000155 0.019794 ... ... ... ... ... ... ... ... ... ... ... 2024-11-18 0.013333 -0.004502 0.004988 -0.001142 0.001830 0.002416 0.007112 0.054664 0.008106 0.008347 2024-11-19 0.001140 0.014324 -0.011502 -0.007949 0.004871 0.009608 0.000059 0.021206 -0.000994 -0.014062 2024-11-20 0.003149 -0.008491 0.000719 -0.009548 -0.005520 -0.006395 0.000761 -0.011540 -0.014405 0.014145 2024-11-21 -0.002098 -0.022430 0.015489 0.016394 -0.004326 0.007589 0.010825 -0.007012 0.008132 0.013292 2024-11-22 0.008692 -0.001438 -0.003092 0.014843 0.005230 0.018725 0.026562 0.042117 -0.003928 0.000082 <p>2517 rows \u00d7 10 columns</p> In\u00a0[12]: Copied! <pre># Notice how close this is to our original returns!\nstock_df.pct_change(1)\n</pre> # Notice how close this is to our original returns! stock_df.pct_change(1) Out[12]: Ticker AAPL AMZN JNJ JPM MSFT PFE PG TSLA V XOM Date 2014-11-24 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2014-11-25 -0.008682 -0.001788 -0.001684 -0.010827 -0.002521 0.008273 0.007831 0.005553 0.010368 -0.009820 2014-11-26 0.011905 -0.004388 0.004780 0.000663 0.005898 0.020676 0.000901 0.001411 0.000000 -0.003165 2014-11-28 -0.000589 0.015199 0.009701 -0.002983 0.001257 0.001608 0.017439 -0.015779 0.003615 -0.041702 2014-12-01 -0.032456 -0.037326 -0.002032 -0.002660 0.016942 0.003531 -0.003871 -0.052675 -0.000155 0.019991 ... ... ... ... ... ... ... ... ... ... ... 2024-11-18 0.013422 -0.004491 0.005000 -0.001141 0.001831 0.002419 0.007137 0.056186 0.008138 0.008382 2024-11-19 0.001140 0.014427 -0.011436 -0.007917 0.004883 0.009654 0.000059 0.021432 -0.000993 -0.013964 2024-11-20 0.003154 -0.008455 0.000719 -0.009503 -0.005505 -0.006374 0.000761 -0.011474 -0.014302 0.014246 2024-11-21 -0.002096 -0.022181 0.015610 0.016530 -0.004317 0.007618 0.010884 -0.006988 0.008165 0.013381 2024-11-22 0.008730 -0.001437 -0.003087 0.014953 0.005244 0.018902 0.026917 0.043016 -0.003921 0.000082 <p>2517 rows \u00d7 10 columns</p> In\u00a0[13]: Copied! <pre>def gen_weights(n):\n    source = np.random.random(n)\n    # pos = np.random.randint(0,n)\n    # source[pos] = 10\n    weights = np.random.dirichlet(source,size=1)\n    return np.array(weights[0])\n</pre> def gen_weights(n):     source = np.random.random(n)     # pos = np.random.randint(0,n)     # source[pos] = 10     weights = np.random.dirichlet(source,size=1)     return np.array(weights[0]) In\u00a0[14]: Copied! <pre>def calculate_returns(weights):\n    return np.sum(log_returns.mean()*weights) * 252 #Annualized Returns\n</pre> def calculate_returns(weights):     return np.sum(log_returns.mean()*weights) * 252 #Annualized Returns In\u00a0[15]: Copied! <pre>log_returns_cov = log_returns.cov()  # Covariance\ndef calculate_volatility(weights):\n    annualized_cov = np.dot(log_returns_cov*252,weights)\n    vol = np.dot(weights.transpose(),annualized_cov)\n    return np.sqrt(vol)\n</pre> log_returns_cov = log_returns.cov()  # Covariance def calculate_volatility(weights):     annualized_cov = np.dot(log_returns_cov*252,weights)     vol = np.dot(weights.transpose(),annualized_cov)     return np.sqrt(vol) In\u00a0[16]: Copied! <pre>n = len(stock_df.columns)\nmc_portfolio_returns = []\nmc_portfolio_vol = []\nmc_weights = []\nfor sim in range(100000):\n    # This may take a while!\n    weights = gen_weights(n)\n    mc_weights.append(weights)\n    mc_portfolio_returns.append(calculate_returns(weights))\n    mc_portfolio_vol.append(calculate_volatility(weights))\n</pre> n = len(stock_df.columns) mc_portfolio_returns = [] mc_portfolio_vol = [] mc_weights = [] for sim in range(100000):     # This may take a while!     weights = gen_weights(n)     mc_weights.append(weights)     mc_portfolio_returns.append(calculate_returns(weights))     mc_portfolio_vol.append(calculate_volatility(weights)) In\u00a0[17]: Copied! <pre># Sharpe Ratios\nmc_sharpe_ratios = np.array(mc_portfolio_returns)/np.array(mc_portfolio_vol)\n</pre> # Sharpe Ratios mc_sharpe_ratios = np.array(mc_portfolio_returns)/np.array(mc_portfolio_vol) In\u00a0[18]: Copied! <pre># Plot Results\nplt.figure(dpi=200,figsize=(10,5))\nplt.scatter(mc_portfolio_vol,mc_portfolio_returns,c=mc_sharpe_ratios)\nplt.ylabel('Expected Returns')\nplt.xlabel('Expected Volatility')\nplt.colorbar(label='Sharpe Ratio');\n</pre> # Plot Results plt.figure(dpi=200,figsize=(10,5)) plt.scatter(mc_portfolio_vol,mc_portfolio_returns,c=mc_sharpe_ratios) plt.ylabel('Expected Returns') plt.xlabel('Expected Volatility') plt.colorbar(label='Sharpe Ratio'); In\u00a0[19]: Copied! <pre># Portfolio from the MonteCarlo simulation that maximizes the Sharpe Ratio \nportfolio_mc_sr = mc_weights[np.argmax(mc_sharpe_ratios)]\nportfolio_mc_sr\n</pre> # Portfolio from the MonteCarlo simulation that maximizes the Sharpe Ratio  portfolio_mc_sr = mc_weights[np.argmax(mc_sharpe_ratios)] portfolio_mc_sr Out[19]: <pre>array([4.72203516e-02, 1.36616169e-01, 2.09472211e-02, 1.87584923e-01,\n       3.99647867e-01, 4.05349264e-09, 1.38906542e-01, 6.74778789e-02,\n       1.59904258e-03, 4.00561676e-12])</pre> In\u00a0[20]: Copied! <pre># Objective Function\ndef objective(weight):\n    # Note -1* because we are minimizing. It is the same as maximizing the positive sharpe ratio\n    return -1 * (calculate_returns(weight)/calculate_volatility(weight))\n</pre> # Objective Function def objective(weight):     # Note -1* because we are minimizing. It is the same as maximizing the positive sharpe ratio     return -1 * (calculate_returns(weight)/calculate_volatility(weight)) In\u00a0[21]: Copied! <pre># Bounds of the decision variables (weights)\nbounds = tuple((0,1) for _ in range(n))\n</pre> # Bounds of the decision variables (weights) bounds = tuple((0,1) for _ in range(n)) In\u00a0[22]: Copied! <pre># Constraint - All weights to add up to 1\nsum_constraint = ({'type': 'eq', 'fun': lambda weight: np.sum(weight)-1})\n</pre> # Constraint - All weights to add up to 1 sum_constraint = ({'type': 'eq', 'fun': lambda weight: np.sum(weight)-1}) In\u00a0[23]: Copied! <pre># Starting Guess\nequal_weights = n * [1/n]\n</pre> # Starting Guess equal_weights = n * [1/n]  In\u00a0[24]: Copied! <pre>results = minimize(fun=objective,x0=equal_weights,bounds=bounds,constraints=sum_constraint)\n</pre> results = minimize(fun=objective,x0=equal_weights,bounds=bounds,constraints=sum_constraint) In\u00a0[25]: Copied! <pre>portfolio_optimal_sr = results.x\nportfolio_optimal_sr\n</pre> portfolio_optimal_sr = results.x portfolio_optimal_sr Out[25]: <pre>array([1.13971593e-01, 1.94361952e-01, 0.00000000e+00, 1.71332999e-01,\n       2.55048319e-01, 9.63313630e-17, 2.09921478e-01, 5.53636596e-02,\n       2.03287907e-18, 6.27482007e-18])</pre> In\u00a0[26]: Copied! <pre># Create x-axis positions for groups\nx = np.arange(n)  # Positions for each stock\nwidth = 0.4  # Width of each bar\n\n# Create the bar plot\nplt.figure(figsize=(10, 5))\nplt.bar(x - width/2, portfolio_mc_sr, width, label='MonteCarlo')  # Portfolio from MonteCarlo simulation\nplt.bar(x + width/2, portfolio_optimal_sr, width, label='Minimization')   # Portfolio from minimization solution\n\n# Add labels, title, and legend\nplt.xlabel('Stock', fontsize=12)\nplt.ylabel('Weight', fontsize=12)\nplt.title('Optimal Portfolio based on Sharpe Ratio', fontsize=14)\nplt.xticks(x, symbols, fontsize=10)  # Set x-ticks to the names\nplt.legend(fontsize=10)\n\n# Show the plot\nplt.tight_layout()  # Adjust layout to prevent label overlap\nplt.show()\n</pre> # Create x-axis positions for groups x = np.arange(n)  # Positions for each stock width = 0.4  # Width of each bar  # Create the bar plot plt.figure(figsize=(10, 5)) plt.bar(x - width/2, portfolio_mc_sr, width, label='MonteCarlo')  # Portfolio from MonteCarlo simulation plt.bar(x + width/2, portfolio_optimal_sr, width, label='Minimization')   # Portfolio from minimization solution  # Add labels, title, and legend plt.xlabel('Stock', fontsize=12) plt.ylabel('Weight', fontsize=12) plt.title('Optimal Portfolio based on Sharpe Ratio', fontsize=14) plt.xticks(x, symbols, fontsize=10)  # Set x-ticks to the names plt.legend(fontsize=10)  # Show the plot plt.tight_layout()  # Adjust layout to prevent label overlap plt.show() In\u00a0[27]: Copied! <pre># Historical performance\nmc_sr_returns = np.dot(portfolio_mc_sr,daily_returns.transpose())\noptimal_sr_returns = np.dot(portfolio_optimal_sr,daily_returns.transpose())\n# Cumulative returns\ncumulative_mc_sr_returns = (1 + mc_sr_returns).cumprod() - 1\ncumulative_mc_sr_returns_perc = pd.Series(cumulative_mc_sr_returns*100, index=daily_returns.index)\ncumulative_optimal_sr_returns = (1 + optimal_sr_returns).cumprod() - 1\ncumulative_optimal_sr_returns_perc = pd.Series(cumulative_optimal_sr_returns*100, index=daily_returns.index)\n\ncumulative_mc_sr_returns_perc.plot(figsize=(10,5), label='MonteCarlo')\ncumulative_optimal_sr_returns_perc.plot(figsize=(10,5), label='Optimization')\nplt.title('Historical Performance', fontsize=14)\nplt.legend(fontsize=10);\n</pre> # Historical performance mc_sr_returns = np.dot(portfolio_mc_sr,daily_returns.transpose()) optimal_sr_returns = np.dot(portfolio_optimal_sr,daily_returns.transpose()) # Cumulative returns cumulative_mc_sr_returns = (1 + mc_sr_returns).cumprod() - 1 cumulative_mc_sr_returns_perc = pd.Series(cumulative_mc_sr_returns*100, index=daily_returns.index) cumulative_optimal_sr_returns = (1 + optimal_sr_returns).cumprod() - 1 cumulative_optimal_sr_returns_perc = pd.Series(cumulative_optimal_sr_returns*100, index=daily_returns.index)  cumulative_mc_sr_returns_perc.plot(figsize=(10,5), label='MonteCarlo') cumulative_optimal_sr_returns_perc.plot(figsize=(10,5), label='Optimization') plt.title('Historical Performance', fontsize=14) plt.legend(fontsize=10); In\u00a0[28]: Copied! <pre># Plot the histogram\nplt.figure(figsize=(12, 3))\nplt.hist(mc_sr_returns, bins=100, label='MonteCarlo')\nplt.hist(optimal_sr_returns, bins=100, alpha=0.7, label='Minimization')\nplt.legend(fontsize=10)\n# Show the plot\nplt.tight_layout()\nplt.show()\n</pre> # Plot the histogram plt.figure(figsize=(12, 3)) plt.hist(mc_sr_returns, bins=100, label='MonteCarlo') plt.hist(optimal_sr_returns, bins=100, alpha=0.7, label='Minimization') plt.legend(fontsize=10) # Show the plot plt.tight_layout() plt.show() In\u00a0[29]: Copied! <pre>stats = {\n    'Montecarlo': {\n        'Returns': calculate_returns(portfolio_mc_sr),\n        'Volatility': calculate_volatility(portfolio_mc_sr),\n        'Sharpe Ratio': calculate_returns(portfolio_mc_sr)/calculate_volatility(portfolio_mc_sr)\n    },\n    'Minimization': {\n        'Returns': calculate_returns(portfolio_optimal_sr),\n        'Volatility': calculate_volatility(portfolio_optimal_sr),\n        'Sharpe Ratio': calculate_returns(portfolio_optimal_sr)/calculate_volatility(portfolio_optimal_sr)\n    }\n}\npd.DataFrame(stats)\n</pre> stats = {     'Montecarlo': {         'Returns': calculate_returns(portfolio_mc_sr),         'Volatility': calculate_volatility(portfolio_mc_sr),         'Sharpe Ratio': calculate_returns(portfolio_mc_sr)/calculate_volatility(portfolio_mc_sr)     },     'Minimization': {         'Returns': calculate_returns(portfolio_optimal_sr),         'Volatility': calculate_volatility(portfolio_optimal_sr),         'Sharpe Ratio': calculate_returns(portfolio_optimal_sr)/calculate_volatility(portfolio_optimal_sr)     } } pd.DataFrame(stats) Out[29]: Montecarlo Minimization Returns 0.204399 0.198496 Volatility 0.214235 0.206010 Sharpe Ratio 0.954086 0.963527 In\u00a0[30]: Copied! <pre># Expected Return Range\nexpected_returns_range = np.linspace(0.01,0.35,100)\n</pre> # Expected Return Range expected_returns_range = np.linspace(0.01,0.35,100) In\u00a0[31]: Copied! <pre># Constraints\nconstraints = ({'type':'eq','fun': lambda weight: np.sum(weight)-1},\n               {'type':'eq','fun': lambda weight: calculate_returns(weight) - possible_return})\n</pre> # Constraints constraints = ({'type':'eq','fun': lambda weight: np.sum(weight)-1},                {'type':'eq','fun': lambda weight: calculate_returns(weight) - possible_return}) In\u00a0[32]: Copied! <pre># Now loop for different expected returns:\nfrontier_volatility = []\nweights_opt = pd.DataFrame(columns=range(n))\nfor possible_return in expected_returns_range:\n    # function for return\n    result = minimize(calculate_volatility,equal_weights,bounds=bounds,constraints=constraints)\n    frontier_volatility.append(result['fun'])\n    weights_opt = pd.concat([weights_opt, pd.DataFrame(result['x']).T])\n</pre> # Now loop for different expected returns: frontier_volatility = [] weights_opt = pd.DataFrame(columns=range(n)) for possible_return in expected_returns_range:     # function for return     result = minimize(calculate_volatility,equal_weights,bounds=bounds,constraints=constraints)     frontier_volatility.append(result['fun'])     weights_opt = pd.concat([weights_opt, pd.DataFrame(result['x']).T]) In\u00a0[33]: Copied! <pre>plt.figure(figsize=(12,8))\nplt.scatter(mc_portfolio_vol,mc_portfolio_returns,c=mc_sharpe_ratios)\nplt.ylabel('Expected Returns')\nplt.xlabel('Expected Volatility')\nplt.colorbar(label='Sharpe Ratio')\n# Add frontier line\nplt.plot(frontier_volatility,expected_returns_range,'r-',linewidth=4);\n</pre> plt.figure(figsize=(12,8)) plt.scatter(mc_portfolio_vol,mc_portfolio_returns,c=mc_sharpe_ratios) plt.ylabel('Expected Returns') plt.xlabel('Expected Volatility') plt.colorbar(label='Sharpe Ratio') # Add frontier line plt.plot(frontier_volatility,expected_returns_range,'r-',linewidth=4);"},{"location":"optimization/portfolio_optimization/#markowitz-portfolio-optimization","title":"Markowitz Portfolio Optimization\u00b6","text":"<p>When it comes to investing, one of the most fundamental questions is: How can I build a portfolio that gives me the best return with the least amount of risk? The answer to this question lies in the work of Harry Markowitz, a financial economist whose theories revolutionized the way we approach portfolio management. Markowitz's Portfolio Theory, often referred to as the Markowitz Efficient Frontier, provides a framework for finding that balance using a mathematical approach to optimizing investments.</p>"},{"location":"optimization/portfolio_optimization/#what-is-markowitz-portfolio-optimization","title":"What Is Markowitz Portfolio Optimization?\u00b6","text":"<p>At its core, Markowitz Portfolio Optimization is about finding the right mix of assets that will give you the highest expected return for a given level of risk (or the least risk for a given return). The theory suggests that investors should not look at individual investments in isolation, but rather, how they work together in a portfolio.</p> <p>Markowitz\u2019s main contribution was the realization that combining different types of investments \u2014 stocks, bonds, real estate, etc. \u2014 can reduce the overall risk of a portfolio without necessarily lowering its expected return. This insight came from understanding that assets do not move in perfect harmony. Some investments rise when others fall, and this diversification can smooth out the bumps of the market, creating a more stable, efficient investment strategy.</p>"},{"location":"optimization/portfolio_optimization/#key-concepts","title":"Key Concepts\u00b6","text":""},{"location":"optimization/portfolio_optimization/#risk-and-return","title":"Risk and Return\u00b6","text":"<p>Risk and return are two sides of the same coin. In simple terms:</p> <ul> <li>Return is the gain or profit you expect from your investments.</li> <li>Risk is the uncertainty or potential for loss in an investment. This is often measured by volatility, or how much an investment\u2019s value fluctuates.</li> </ul> <p>The fundamental goal of Markowitz\u2019s theory is to find the optimal combination of assets that maximizes return for a given level of risk.</p>"},{"location":"optimization/portfolio_optimization/#diversification","title":"Diversification\u00b6","text":"<p>Diversification is the cornerstone of Markowitz\u2019s theory. By holding a variety of investments, you reduce the impact of any single asset\u2019s performance on the overall portfolio. The key is to choose assets that don\u2019t move in the same direction. For example, when stock prices are falling, bond prices may be rising. By combining assets with low or negative correlations, you can lower the portfolio's risk.</p>"},{"location":"optimization/portfolio_optimization/#the-efficient-frontier","title":"The Efficient Frontier\u00b6","text":"<p>The Efficient Frontier is a curve that shows the optimal portfolios offering the highest expected return for a given level of risk. Portfolios below this curve are suboptimal, as they either offer lower returns for the same risk or higher risk for the same return.</p> <ul> <li><p>Graph Axes:</p> <ul> <li>The x-axis is Risk</li> <li>The y-axis is Return</li> </ul> </li> <li><p>The Curve</p> <ul> <li>The curve starts at the least risky portfolio and curves upward as you take on more risk for higher returns.</li> <li>Portfolios below the curve are bad deals\u2014they have too much risk for the return they offer.</li> <li>Portfolios on the curve are the best deals\u2014they balance risk and return.</li> </ul> </li> </ul>"},{"location":"optimization/portfolio_optimization/#benefits-and-limitations","title":"Benefits and Limitations\u00b6","text":""},{"location":"optimization/portfolio_optimization/#benefits","title":"Benefits\u00b6","text":"<ul> <li>Encourages diversification, reducing portfolio risk.</li> <li>Provides a systematic, data-driven approach to portfolio construction.</li> <li>Helps visualize trade-offs between risk and return.</li> </ul>"},{"location":"optimization/portfolio_optimization/#limitations","title":"Limitations\u00b6","text":"<ul> <li>Assumes historical data predicts future performance.</li> <li>Relies on estimates of returns and covariances, which can be inaccurate.</li> <li>Treats risk as variance, which penalizes upside volatility (positive surprises).</li> </ul>"},{"location":"optimization/portfolio_optimization/#the-markowitz-optimization-model","title":"The Markowitz Optimization Model\u00b6","text":""},{"location":"optimization/portfolio_optimization/#portfolio-return","title":"Portfolio Return\u00b6","text":"<p>The expected return of a portfolio is the weighted sum of the individual asset returns: $$    E(R_p) = \\sum_{i=1}^n w_i E(R_i)    $$ Here:</p> <ul> <li>$ E(R_p) $: Portfolio\u2019s expected return</li> <li>$ w_i $: Weight (percentage) of the portfolio invested in asset $i$</li> <li>$ E(R_i) $: Expected return of asset $i$</li> </ul>"},{"location":"optimization/portfolio_optimization/#portfolio-risk-variance","title":"Portfolio Risk (Variance)\u00b6","text":"<p>Risk is captured by the variance (or standard deviation) of returns. For a portfolio: $$    \\sigma_p^2 = \\sum_{i=1}^n \\sum_{j=1}^n w_i w_j \\text{Cov}(R_i, R_j)    $$ Where:</p> <ul> <li>$ \\sigma_p^2 $: Portfolio variance</li> <li>$ \\text{Cov}(R_i, R_j) $: Covariance between the returns of assets $i$ and $j$ Covariance measures how the returns of two assets move together. Diversification benefits arise when assets are negatively correlated or have low covariance.</li> </ul>"},{"location":"optimization/portfolio_optimization/#the-optimization-problem","title":"The Optimization Problem\u00b6","text":"<p>The goal is to solve this constrained optimization problem:</p> <p>$$ \\text{Maximize } E(R_p) \\text{ for a given level of } \\sigma_p^2 $$</p> <p>Or equivalently:</p> <p>$$ \\text{Minimize } \\sigma_p^2 \\text{ for a given level of } E(R_p) $$</p> <p>Subject to:</p> <ol> <li>$\\sum_{i=1}^n w_i = 1$ (The weights must sum to 1, i.e., you invest all your money.)</li> <li>$w_i \\geq 0$ (Non-negativity constraint if short-selling is not allowed.)</li> </ol>"},{"location":"optimization/portfolio_optimization/#python-model","title":"Python model\u00b6","text":""},{"location":"optimization/portfolio_optimization/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"optimization/portfolio_optimization/#download-data","title":"Download data\u00b6","text":"<p>We are going to use 10 different stocks in several sectors (Technology, Healthcare, Finance, and Retail)</p>"},{"location":"optimization/portfolio_optimization/#daily-returns","title":"Daily Returns\u00b6","text":"<p>Now, we are going to calculate the daily returns of these stocks</p>"},{"location":"optimization/portfolio_optimization/#cumulative-returns","title":"Cumulative Returns\u00b6","text":""},{"location":"optimization/portfolio_optimization/#log-returns","title":"Log Returns\u00b6","text":""},{"location":"optimization/portfolio_optimization/#montecarlo-simulation-for-optimization-search","title":"MonteCarlo simulation for Optimization Search\u00b6","text":""},{"location":"optimization/portfolio_optimization/#generating-random-weights","title":"Generating Random Weights\u00b6","text":""},{"location":"optimization/portfolio_optimization/#portfolio-returns-function","title":"Portfolio Returns Function\u00b6","text":""},{"location":"optimization/portfolio_optimization/#portfolio-volatility-variance-function","title":"Portfolio Volatility (Variance) Function\u00b6","text":""},{"location":"optimization/portfolio_optimization/#montecarlo","title":"Montecarlo\u00b6","text":"<p>Now we just create many, many random weightings, and we can then plot them on expected return vs. expected volatility (coloring them by Sharpe Ratio):</p>"},{"location":"optimization/portfolio_optimization/#optimal-weighting-through-minimization-search","title":"Optimal Weighting through Minimization Search\u00b6","text":""},{"location":"optimization/portfolio_optimization/#results-comparison","title":"Results comparison\u00b6","text":""},{"location":"optimization/portfolio_optimization/#efficient-frontier","title":"Efficient Frontier\u00b6","text":"<p>The efficient frontier is the set of optimal portfolios that offers the highest expected return for a defined level of risk or the lowest risk for a given level of expected return. Portfolios that lie below the efficient frontier are sub-optimal, because they do not provide enough return for the level of risk. Portfolios that cluster to the right of the efficient frontier are also sub-optimal, because they have a higher level of risk for the defined rate of return</p>"},{"location":"optimization/portfolio_optimization/#plot-efficient-frontier","title":"Plot Efficient Frontier\u00b6","text":""},{"location":"optimization/sudoku/","title":"Sudoku Solver","text":"<p>Using Mixed Integer Linear Programming (MILP) to solve a Sudoku puzzle is an interesting application of optimization techniques.</p>","tags":["Optimization","MILP","Sudoku Solver","Python","Pyomo"]},{"location":"optimization/sudoku/#understanding-the-sudoku-puzzle","title":"Understanding the Sudoku Puzzle","text":"<p>Sudoku involves filling a $n^2 \\times n^2 $ for $n \\in \\mathbb{Z}$ (with $n=3$ being the most common Sudoku Puzzle) grid such that:</p> <ul> <li>Each row contains the numbers 1 to $n^2$ exactly once.</li> <li>Each column contains the numbers 1 to $n^2$ exactly once.</li> <li>Each $n \\times n$ subgrid contains the numbers 1 to $n^2$ exactly once.</li> <li>Some cells are pre-filled with known values.</li> </ul>","tags":["Optimization","MILP","Sudoku Solver","Python","Pyomo"]},{"location":"optimization/sudoku/#formulating-sudoku-as-a-milp-problem","title":"Formulating Sudoku as a MILP Problem","text":"<p>For Sudoku, the goal is to find a feasible solution that satisfies all constraints without needing an explicit optimization objective.</p>","tags":["Optimization","MILP","Sudoku Solver","Python","Pyomo"]},{"location":"optimization/sudoku/#sets","title":"Sets","text":"<ul> <li>$ i $ represents the row index (1 to $n^2$)</li> <li>$ j $ represents the column index (1 to $n^2$)</li> <li>$ k $ represents the digit (1 to $n^2$)</li> <li>$ p $ represents the subgrid index in the horizontal direction (1 to $n$)</li> <li>$ q $ represents the subgrid index in the vertical direction (1 to $n$)</li> </ul> <p>The following table illustrates the indexing for $n=3$.</p> Row $i$ Column $j$ $1$ $2$ $3$ $4$ $5$ $6$ $7$ $8$ $9$ $1$ $p=1$, $q=1$ $p=1$, $q=2$ $p=1$, $q=3$ $2$ $3$ $4$ $p=2$, $q=1$ $p=2$, $q=2$ $p=2$, $q=3$ $5$ $6$ $7$ $p=3$, $q=1$ $p=3$, $q=2$ $p=3$, $q=3$ $8$ $9$","tags":["Optimization","MILP","Sudoku Solver","Python","Pyomo"]},{"location":"optimization/sudoku/#variables","title":"Variables","text":"<p>Define a binary decision variable $ x_{ijk} $, where:</p> <p>The variable $ x_{ijk} = 1 $ if digit $ k $ is placed in cell $ (i, j) $,  otherwise $ x_{ijk} = 0 $.</p>","tags":["Optimization","MILP","Sudoku Solver","Python","Pyomo"]},{"location":"optimization/sudoku/#constraints","title":"Constraints","text":"<p>The problem is modeled by introducing several constraints:</p> <ol> <li> <p>Each cell must contain exactly one digit    $$    \\sum_{k} x_{ijk} = 1 \\quad \\forall i, j    $$</p> </li> <li> <p>Each digit appears exactly once in each row    $$    \\sum_{i} x_{ijk} = 1 \\quad \\forall j, k    $$</p> </li> <li> <p>Each digit appears exactly once in each column    $$    \\sum_{j} x_{ijk} = 1 \\quad \\forall i, k    $$</p> </li> <li> <p>Each digit appears exactly once in each subgrid    $$    \\sum_{i=n \\cdot (p-1) + 1}^{n \\cdot p} \\sum_{j=n \\cdot (q-1) + 1}^{n \\cdot q} x_{ijk} = 1 \\quad \\forall p, q, k    $$</p> </li> <li> <p>Pre-filled cells must match given values    If cell $ (i, j) $ is pre-filled with digit $ k $:    $$    x_{ijk} = 1    $$</p> </li> </ol>","tags":["Optimization","MILP","Sudoku Solver","Python","Pyomo"]},{"location":"optimization/sudoku/#objective-function","title":"Objective Function","text":"<p>There\u2019s no optimization goal in Sudoku (it\u2019s a feasibility problem),  so the objective function can be trivial, like minimizing $ 0 $.</p>","tags":["Optimization","MILP","Sudoku Solver","Python","Pyomo"]},{"location":"optimization/sudoku/#example-milp-code-using-python-and-pyomo","title":"Example MILP Code (Using Python and Pyomo)","text":"<p>Here's a basic Python implementation with Pyomo:</p> <pre><code>import pyomo.environ as pyo\nfrom pyomo.core.util import quicksum\nfrom pyomo.opt import SolverFactory\n\n# Create a model\ndef sudoku_model(n):\n    \"\"\" define the optimization model\n    \"\"\"\n\n    size = n ** 2\n\n    model = pyo.ConcreteModel()\n\n    \"\"\"Sets\"\"\"\n    model.i = pyo.RangeSet(1, size)  # rows - 1 to size\n    model.j = pyo.RangeSet(1, size)  # columns - 1 to size\n    model.k = pyo.RangeSet(1, size)  # digits - 1 to size\n\n    model.p = pyo.RangeSet(1, n)  # row boxes - 1 to base\n    model.q = pyo.RangeSet(1, n)  # column boxes - 1 to base\n\n    \"\"\"Variables\"\"\"\n    model.x = pyo.Var(model.i, model.j, model.k, bounds=(0, 1), domain=pyo.NonNegativeReals)  #domain=pyo.Binary)\n\n    \"\"\"Objective Function\"\"\"\n\n    def obj_expression(m):\n        return 0  \n\n    \"\"\"Constraints\"\"\"\n\n    # Unique digits\n    def c_digits(m, i, j):\n        return quicksum(m.x[i, j, k] for k in m.k) == 1\n\n    # Unique in rows\n    def c_rows(m, j, k):\n        return quicksum(m.x[i, j, k] for i in m.i) == 1\n\n    # Unique in columns\n    def c_columns(m, i, k):\n        return quicksum(m.x[i, j, k] for j in m.j) == 1\n\n    # Unique in boxes\n    def c_boxes(m, k, p, q):\n        return quicksum(m.x[i, j, k] for i in range(n * (p - 1) + 1, n * p + 1)\n                        for j in range(n * (q - 1) + 1, n * q + 1)) == 1\n\n    \"\"\"Define optimization problem\"\"\"\n    model.obj = pyo.Objective(rule=obj_expression, sense=pyo.maximize)\n    \"\"\"Add constraints to optimization model\"\"\"\n    model.c_digits = pyo.Constraint(model.i, model.j, rule=c_digits)\n    model.c_rows = pyo.Constraint(model.j, model.k, rule=c_rows)\n    model.c_columns = pyo.Constraint(model.i, model.k, rule=c_columns)\n    model.c_boxes = pyo.Constraint(model.k, model.p, model.q, rule=c_boxes)\n\n    # return model\n    return model\n</code></pre>","tags":["Optimization","MILP","Sudoku Solver","Python","Pyomo"]},{"location":"optimization/sudoku/#advantages","title":"Advantages","text":"<ul> <li>MILP ensures the solution satisfies all constraints.</li> <li>Can handle more complex Sudoku variants by adding new constraints.</li> </ul>","tags":["Optimization","MILP","Sudoku Solver","Python","Pyomo"]},{"location":"optimization/sudoku/#dash-plotly-app","title":"Dash Plotly app","text":"<p>Here you can find a webapp using Dash Plotly to solve any sudoku, I hope you enjoy it!</p>","tags":["Optimization","MILP","Sudoku Solver","Python","Pyomo"]},{"location":"optimization/travel_sales/","title":"Traveling Salesperson Problem","text":"Optimization MILP Traveling Salesperson Problem Python Pyomo In\u00a0[1]: Copied! <pre># Import libraries\nimport numpy as np\nimport pandas as pd\nimport pyomo.environ as pyo\nfrom pyomo.core.util import quicksum\nimport highspy\n</pre> # Import libraries import numpy as np import pandas as pd import pyomo.environ as pyo from pyomo.core.util import quicksum import highspy In\u00a0[2]: Copied! <pre># Pyomo model\ndef pyomo_model(n: int):\n    \"\"\"\n    \n    :param n: Number of cities\n    :return: algrebaic model\n    \"\"\"\n    \n    tsp = pyo.AbstractModel()\n    \n    # Sets\n    tsp.i = pyo.RangeSet(1, n)\n    tsp.j = pyo.RangeSet(1, n)\n    \n    # Parameters\n    tsp.C_ij = pyo.Param(tsp.i, tsp.j, doc='Cost of traveling from i to j')\n    \n    # Variables\n    tsp.x_ij = pyo.Var(tsp.i, tsp.j, doc='Indicates whether the path from i to j is included', within=pyo.Binary)\n    tsp.u_i = pyo.Var(tsp.i, doc='Represents the order in which i is visited', bounds=(1, n-1), within=pyo.PositiveReals)\n    \n    # Model definition\n    # Objective Function\n    tsp.objective = pyo.Objective(rule=objective, sense=pyo.minimize, doc='Minimize Cost')\n    # Add Constraints\n    tsp.c_visit_once = pyo.Constraint(tsp.j, rule=c_visit_once, doc='Visit Once')\n    tsp.c_leave_once = pyo.Constraint(tsp.i, rule=c_leave_once, doc='Leave Once')\n    tsp.c_subtour_elimination = pyo.Constraint(tsp.i, tsp.j, rule=c_subtour_elimination, doc='Subtour Elimination')\n    \n    return tsp\n    \n# Objective function\ndef objective(m):\n    return quicksum(m.C_ij[i, j] * m.x_ij[i, j] for i in m.i for j in m.j if j!=i)\n\n# Constraints\ndef c_visit_once(m, j):\n    return sum(m.x_ij[i, j] for i in m.i if j!=i) == 1\n\ndef c_leave_once(m, i):\n    return sum(m.x_ij[i, j] for j in m.j if j!=i) == 1\n\ndef c_subtour_elimination(m, i, j):\n    if i != j and i&gt;=2 and j&gt;=2:\n        return m.u_i[i] - m.u_i[j] + len(m.i) * m.x_ij[i, j] &lt;= len(m.i) - 1\n    return pyo.Constraint.Skip\n</pre> # Pyomo model def pyomo_model(n: int):     \"\"\"          :param n: Number of cities     :return: algrebaic model     \"\"\"          tsp = pyo.AbstractModel()          # Sets     tsp.i = pyo.RangeSet(1, n)     tsp.j = pyo.RangeSet(1, n)          # Parameters     tsp.C_ij = pyo.Param(tsp.i, tsp.j, doc='Cost of traveling from i to j')          # Variables     tsp.x_ij = pyo.Var(tsp.i, tsp.j, doc='Indicates whether the path from i to j is included', within=pyo.Binary)     tsp.u_i = pyo.Var(tsp.i, doc='Represents the order in which i is visited', bounds=(1, n-1), within=pyo.PositiveReals)          # Model definition     # Objective Function     tsp.objective = pyo.Objective(rule=objective, sense=pyo.minimize, doc='Minimize Cost')     # Add Constraints     tsp.c_visit_once = pyo.Constraint(tsp.j, rule=c_visit_once, doc='Visit Once')     tsp.c_leave_once = pyo.Constraint(tsp.i, rule=c_leave_once, doc='Leave Once')     tsp.c_subtour_elimination = pyo.Constraint(tsp.i, tsp.j, rule=c_subtour_elimination, doc='Subtour Elimination')          return tsp      # Objective function def objective(m):     return quicksum(m.C_ij[i, j] * m.x_ij[i, j] for i in m.i for j in m.j if j!=i)  # Constraints def c_visit_once(m, j):     return sum(m.x_ij[i, j] for i in m.i if j!=i) == 1  def c_leave_once(m, i):     return sum(m.x_ij[i, j] for j in m.j if j!=i) == 1  def c_subtour_elimination(m, i, j):     if i != j and i&gt;=2 and j&gt;=2:         return m.u_i[i] - m.u_i[j] + len(m.i) * m.x_ij[i, j] &lt;= len(m.i) - 1     return pyo.Constraint.Skip  In\u00a0[3]: Copied! <pre>cost = [[ 0, 10, 15, 20, 12, 24],\n        [10,  0, 35, 25, 17, 28],\n        [15, 35,  0, 30, 26, 13],\n        [20, 25, 30,  0, 23, 13],\n        [12, 17, 26, 23,  0, 27],\n        [24, 28, 13, 13, 27,  0]]\nn = len(cost)\nc_ij = {(i+1, j+1): cost[i][j] for i in range(n) for j in range(n)}\n</pre> cost = [[ 0, 10, 15, 20, 12, 24],         [10,  0, 35, 25, 17, 28],         [15, 35,  0, 30, 26, 13],         [20, 25, 30,  0, 23, 13],         [12, 17, 26, 23,  0, 27],         [24, 28, 13, 13, 27,  0]] n = len(cost) c_ij = {(i+1, j+1): cost[i][j] for i in range(n) for j in range(n)} In\u00a0[4]: Copied! <pre># Data\ndata = {\n    None: {\n        # Parameters\n        'C_ij': c_ij\n    }\n}\n</pre> # Data data = {     None: {         # Parameters         'C_ij': c_ij     } } In\u00a0[5]: Copied! <pre># Create instance model\ntsp = pyomo_model(n)\ntsp_instance = tsp.create_instance(data)\n</pre> # Create instance model tsp = pyomo_model(n) tsp_instance = tsp.create_instance(data) In\u00a0[6]: Copied! <pre># Solve problem with Highspy\nsolver = pyo.SolverFactory('appsi_highs')\nsolver.solve(tsp_instance, tee=True)\n</pre> # Solve problem with Highspy solver = pyo.SolverFactory('appsi_highs') solver.solve(tsp_instance, tee=True) <pre>Coefficient ranges:\n  Matrix [1e+00, 6e+00]\n  Cost   [1e+01, 4e+01]\n  Bound  [1e+00, 5e+00]\n  RHS    [1e+00, 5e+00]\nPresolving model\n32 rows, 35 cols, 120 nonzeros  0s\n32 rows, 35 cols, 120 nonzeros  0s\nObjective function is integral with scale 1\n\nSolving MIP model with:\n   32 rows\n   35 cols (30 binary, 0 integer, 0 implied int., 5 continuous)\n   120 nonzeros\n\n        Nodes      |    B&amp;B Tree     |            Objective Bounds              |  Dynamic Constraints |       Work      \n     Proc. InQueue |  Leaves   Expl. | BestBound       BestSol              Gap |   Cuts   InLp Confl. | LpIters     Time\n\n         0       0         0   0.00%   0               inf                  inf        0      0      0         0     0.0s\n         0       0         0   0.00%   90.4            inf                  inf        0      0      0        17     0.0s\n R       0       0         0   0.00%   90.4            91                 0.66%       26      4      0        23     0.0s\n\nSolving report\n  Status            Optimal\n  Primal bound      91\n  Dual bound        91\n  Gap               0% (tolerance: 0.01%)\n  Solution status   feasible\n                    91 (objective)\n                    0 (bound viol.)\n                    0 (int. viol.)\n                    0 (row viol.)\n  Timing            0.00 (total)\n                    0.00 (presolve)\n                    0.00 (postsolve)\n  Nodes             1\n  LP iterations     23 (total)\n                    0 (strong br.)\n                    6 (separation)\n                    0 (heuristics)\n</pre> Out[6]: <pre>{'Problem': [{'Lower bound': 91.0, 'Upper bound': 91.0, 'Number of objectives': 1, 'Number of constraints': 0, 'Number of variables': 0, 'Sense': 1}], 'Solver': [{'Status': 'ok', 'Termination condition': 'optimal', 'Termination message': 'TerminationCondition.optimal'}], 'Solution': [OrderedDict({'number of solutions': 0, 'number of solutions displayed': 0})]}</pre> In\u00a0[7]: Copied! <pre># Display results\nx_ij = tsp_instance.x_ij.extract_values()\nC_ij = tsp_instance.C_ij.extract_values()\nu_i = tsp_instance.u_i.extract_values()\nu_i[1] = 0  # First city\nu_i = [k for k, v in sorted(u_i.items(), key=lambda item: item[1])]\nu_i.append(1)  # Come back to first city\nfor i in range(len(u_i)-1):\n    print(f\"Travel from {u_i[i]} to {u_i[i+1]} and Cost {C_ij[(u_i[i],u_i[i+1])]}\")\n</pre> # Display results x_ij = tsp_instance.x_ij.extract_values() C_ij = tsp_instance.C_ij.extract_values() u_i = tsp_instance.u_i.extract_values() u_i[1] = 0  # First city u_i = [k for k, v in sorted(u_i.items(), key=lambda item: item[1])] u_i.append(1)  # Come back to first city for i in range(len(u_i)-1):     print(f\"Travel from {u_i[i]} to {u_i[i+1]} and Cost {C_ij[(u_i[i],u_i[i+1])]}\") <pre>Travel from 1 to 3 and Cost 15\nTravel from 3 to 6 and Cost 13\nTravel from 6 to 4 and Cost 13\nTravel from 4 to 5 and Cost 23\nTravel from 5 to 2 and Cost 17\nTravel from 2 to 1 and Cost 10\n</pre>"},{"location":"optimization/travel_sales/#traveling-salesperson-problem","title":"Traveling Salesperson Problem\u00b6","text":"<p>The Traveling Salesperson Problem (TSP) is a classic optimization problem in computer science and operations research. It involves finding the most efficient route for a salesperson to visit a set of cities and return to the starting city, with the goal of minimizing travel distance or cost.</p>"},{"location":"optimization/travel_sales/#problem-definition","title":"Problem Definition\u00b6","text":"<p>Given:</p> <ul> <li>A list of cities (or nodes).</li> <li>The distances or costs between each pair of cities.</li> </ul> <p>Objective:</p> <ul> <li>Find the shortest possible route that visits each city exactly once and returns to the starting city.</li> </ul>"},{"location":"optimization/travel_sales/#example","title":"Example\u00b6","text":"<ul> <li>Suppose there are four cities: A, B, C, and D.</li> <li>The distances between them are:</li> </ul> A B C D E A \u2192 B: 10 A \u2192 C: 15 B \u2192 C: 35 A \u2192 D: 20 B \u2192 D: 25 C \u2192 D: 30 A \u2192 E: 12 B \u2192 E: 17 C \u2192 E: 26 D \u2192 E: 23 A \u2192 F: 24 B \u2192 F: 28 C \u2192 F: 13 D \u2192 F: 13 E \u2192 F: 27 <p>The task is to determine the sequence of cities (e.g., A \u2192 B \u2192 C \u2192 D \u2192 E \u2192 F \u2192 A) that minimizes the total distance traveled.</p>"},{"location":"optimization/travel_sales/#applications","title":"Applications\u00b6","text":"<ol> <li>Logistics and Delivery: Optimizing routes for delivery trucks or couriers.</li> <li>Manufacturing: Designing efficient robotic arms or conveyor belts.</li> <li>Tourism: Creating optimal travel itineraries.</li> </ol>"},{"location":"optimization/travel_sales/#complexity","title":"Complexity\u00b6","text":"<ul> <li>Computationally Hard: TSP is NP-hard, meaning there's no known polynomial-time algorithm to solve it for all cases.</li> <li>For (n) cities, there are ((n-1)!) possible routes.</li> </ul>"},{"location":"optimization/travel_sales/#solution-approaches","title":"Solution Approaches\u00b6","text":"<ol> <li><p>Exact Algorithms:</p> <ul> <li>Brute Force: Explore all possible routes (impractical for large (n)).</li> <li>Dynamic Programming.</li> <li>Mixed Integer Linear Programming (MILP).</li> </ul> </li> <li><p>Heuristic and Approximation Methods:</p> <ul> <li>Greedy Algorithms.</li> <li>Genetic Algorithms.</li> <li>Simulated Annealing.</li> <li>Ant Colony Optimization.</li> </ul> </li> <li><p>Machine Learning</p> </li> </ol>"},{"location":"optimization/travel_sales/#mixed-integer-linear-program-milp-formulation","title":"Mixed-Integer Linear Program (MILP) Formulation\u00b6","text":"<p>The Traveling Salesperson Problem (TSP) can be formulated as a Mixed-Integer Linear Program (MILP). Below is a typical formulation:</p>"},{"location":"optimization/travel_sales/#sets","title":"Sets\u00b6","text":"<ul> <li>$i$, $j$: Cities (nodes).</li> </ul>"},{"location":"optimization/travel_sales/#parameters","title":"Parameters\u00b6","text":"<ul> <li>$ c_{ij} $: Cost or distance of traveling from city $i$ to city $j$.</li> <li>$ n $: Number of cities.</li> </ul>"},{"location":"optimization/travel_sales/#variables","title":"Variables\u00b6","text":"<ol> <li><p>Decision Variables:</p> <ul> <li>$ x_{ij} \\in \\{0, 1\\} $: Binary variable indicating whether the path from city $i$ to city $j$ is included in the tour.<ul> <li>$ x_{ij} = 1 $: Path from city $i$ to $j$ is used.</li> <li>$ x_{ij} = 0 $: Path from city $i$ to $j$ is not used.</li> </ul> </li> </ul> </li> <li><p>Auxiliary Variables (for subtour elimination):</p> <ul> <li>$ u_i $: A continuous variable representing the order in which city $i$ is visited, used to eliminate subtours.</li> </ul> </li> </ol>"},{"location":"optimization/travel_sales/#objective-function","title":"Objective Function:\u00b6","text":"<p>Minimize the total travel cost: $$ \\min_{x_{ij}, u_i} \\quad \\sum_{i=1}^{n} \\sum_{j=1, j \\neq i}^{n} c_{ij} x_{ij} $$</p>"},{"location":"optimization/travel_sales/#constraints","title":"Constraints:\u00b6","text":"<ol> <li><p>Each city is entered exactly once: $$ \\sum_{i=1, i \\neq j}^{n} x_{ij} = 1 \\quad \\forall j = 1, \\dots, n $$</p> </li> <li><p>Each city is exited exactly once: $$ \\sum_{j=1, j \\neq i}^{n} x_{ij} = 1 \\quad \\forall i = 1, \\dots, n $$</p> </li> <li><p>Subtour elimination constraints (Miller-Tucker-Zemlin formulation): $$ u_i - u_j + n \\cdot x_{ij} \\leq n - 1 \\quad \\forall i, j = 2, \\dots, n, \\; i \\neq j $$ $$ 1 \\leq u_i \\leq n - 1 \\quad \\forall i = 2, \\dots, n $$</p> <p>The basic formulation with decision variables $x_{ij}$ ensures each city is visited exactly once but does not inherently prevent subtours (i.e., tours that visit only a subset of cities). These constraints force the auxiliary variable $u_i$ to ensure that a valid sequence of cities is followed. If $x_{ij} = 1$ (i.e., there is a path from $i$ to $j$), then $u_i &lt; u_j$.</p> </li> </ol>"},{"location":"optimization/travel_sales/#pyomo-formulation","title":"Pyomo formulation\u00b6","text":""},{"location":"blog/archive/2024/","title":"November 2024","text":""},{"location":"blog/category/python/","title":"Python","text":""}]}